{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twhool02/ptm-quantization/blob/main/Evaluation_of_original_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11F65poivg_y"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i43SoJPhuk0N"
      },
      "source": [
        "This notebook runs evaluation benchmarks on models that will be quantized during later evaluation runs. This is to allow for accurate comparisons.\n",
        "\n",
        "Evaulation of models is carried out using the [Language Model Evaluation Harness ](https://github.com/EleutherAI/lm-evaluation-harness) from [EleutherAI](https://www.eleuther.ai/)\n",
        "\n",
        "Models are evaluated on:\n",
        "\n",
        "* MMLU (5-shot)\n",
        "* HellaSwag (0-shot)\n",
        "* BoolQ (0-shot)\n",
        "* BBH (3-shot)\n",
        "\n",
        "The number of shots for MMLU, HellaSwag and BBH are the same as those used on the [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCACapbkdGz9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map Google Drive"
      ],
      "metadata": {
        "id": "dHxCFvMN5A0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os, subprocess\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "020dDskO4-h-",
        "outputId": "76db01e9-a83b-42d6-f249-7a064542fbef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew416R-IcVzt"
      },
      "source": [
        "### Log into HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrMW4HL3bcfs",
        "outputId": "c5065b18-c85f-4e2e-e38f-0ea940b24bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/388.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m358.4/388.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hHugging Face Version is: 0.22.2\n"
          ]
        }
      ],
      "source": [
        "# Required when quantizing models/data that are gated on HuggingFace and required for pushing models to HuggingFace\n",
        "!pip install -q --upgrade huggingface_hub\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "print(f\"Hugging Face Version is: {huggingface_hub.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BncmFp4sccnN"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# using the HF_TOKEN secret, this has write permissions to Hugging Face\n",
        "hftoken = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15y1BQaAcdL9",
        "outputId": "88246cf6-8e60-4043-ecae-ffd2361d2efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log into hugging face using the HF_TOKEN secrect\n",
        "login(hftoken, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATvc0b73cfOl"
      },
      "source": [
        "### Install Requried Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkdgJyWhch3N",
        "outputId": "b07de6fa-7895-42f6-ed12-ad55f7b63439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.9/409.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# The Transformers library provides APIs and tools to easily download and train pretrained model.\n",
        "!pip install -q -U transformers -q\n",
        "\n",
        "# Accelerate enables the same PyTorch code to be run across any distributed configuration\n",
        "!pip install -q -U accelerate -q\n",
        "\n",
        "# an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems\n",
        "!pip install sentencepiece -q\n",
        "\n",
        "# 'bitsandbytes' includes quantization primitives for 8-bit & 4-bit operations\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained\n",
        "# models to various downstream applications without fine-tuning all of a model’s parameters\n",
        "!pip install peft -q\n",
        "\n",
        "# trl is short for Transformers Reinforcement Learning, it's used for fine-tuning transformer models using Proximal Policy Optimization.\n",
        "!pip install trl -q\n",
        "\n",
        "# an extension of Transformers that provides a set of performance optimization tools to train and run models\n",
        "!pip install -q -U optimum\n",
        "\n",
        "# used for monitoring the training process.\n",
        "!pip install -q -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGjyoxqYc5PU",
        "outputId": "291141c8-38d3-4091-cfce-0169626dd87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version of transformers: 4.39.3\n",
            "version of accelerate: 0.28.0\n"
          ]
        }
      ],
      "source": [
        "#print the version of transformers\n",
        "import transformers\n",
        "print(f\"version of transformers: {transformers.__version__}\")\n",
        "\n",
        "# print the version of the tensorflow library\n",
        "import accelerate\n",
        "print(f\"version of accelerate: {accelerate.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into Weights and Biases"
      ],
      "metadata": {
        "id": "HKyqqPJt6FWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb_token = userdata.get('wandb_api')\n",
        "wandb.login(key=wandb_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5EUcpT86IbW",
        "outputId": "a11c9f61-83a6-4169-8b80-c8ba668ec001"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create folder for results"
      ],
      "metadata": {
        "id": "sTaU0EC3thAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store results\n",
        "results_dir = f\"/content/drive/MyDrive/Evaluation\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "IBGPPMW7tl5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNNyUvVWjOXM",
        "outputId": "07acbfdf-11c2-45a2-ad5e-fb631078200a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/EleutherAI/lm-evaluation-harness.git@big-refactor\n",
            "  Cloning https://github.com/EleutherAI/lm-evaluation-harness.git (to revision big-refactor) to /tmp/pip-req-build-5ucrhn25\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm-evaluation-harness.git /tmp/pip-req-build-5ucrhn25\n",
            "  Running command git checkout -b big-refactor --track origin/big-refactor\n",
            "  Switched to a new branch 'big-refactor'\n",
            "  Branch 'big-refactor' set up to track remote branch 'big-refactor' from 'origin'.\n",
            "  Resolved https://github.com/EleutherAI/lm-evaluation-harness.git to commit 967eb4fa90b80ba4e8cc7a2fd171f44f0e384808\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (0.28.0)\n",
            "Collecting evaluate (from lm_eval==1.0.0)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (2.18.0)\n",
            "Collecting jsonlines (from lm_eval==1.0.0)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (2.9.0)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (0.10.0)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==1.0.0)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytablewriter (from lm_eval==1.0.0)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm_eval==1.0.0)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==1.0.0)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (1.2.2)\n",
            "Collecting sqlitedict (from lm_eval==1.0.0)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (2.2.1+cu121)\n",
            "Collecting tqdm-multiprocess (from lm_eval==1.0.0)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==1.0.0) (4.39.3)\n",
            "Collecting zstandard (from lm_eval==1.0.0)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.9.3)\n",
            "Collecting responses<0.19 (from evaluate->lm_eval==1.0.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==1.0.0) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==1.0.0) (1.16.0)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==1.0.0)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==1.0.0) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==1.0.0) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==1.0.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==1.0.0) (4.9.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==1.0.0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==1.0.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==1.0.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval==1.0.0) (0.15.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==1.0.0) (23.2.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==1.0.0) (67.7.2)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
            "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm_eval==1.0.0)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==1.0.0) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==1.0.0) (2023.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval==1.0.0) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==1.0.0) (8.1.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.0.0->lm_eval==1.0.0) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->lm_eval==1.0.0) (1.3.0)\n",
            "Building wheels for collected packages: lm_eval, rouge-score, sqlitedict\n",
            "  Building wheel for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm_eval: filename=lm_eval-1.0.0-py3-none-any.whl size=995255 sha256=94faf0bdbd14026b4a00f602a457f9ddcc7203b9bba4a429d9009d7219a72223\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sejp84x_/wheels/17/01/26/599c0779e9858a70a73fa8a306699b5b9a868f820c225457b0\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=7a66cdf22b697903de2708b158d0b455f7dcd7ade6083cf2366b5badc2a39fba\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=6a0470584bd60c373dab56a0f9b3f7f543f009abbc20c5dbd09ca31da6752bee\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built lm_eval rouge-score sqlitedict\n",
            "Installing collected packages: sqlitedict, zstandard, tcolorpy, pybind11, portalocker, pathvalidate, mbstrdecoder, jsonlines, colorama, typepy, tqdm-multiprocess, sacrebleu, rouge-score, responses, DataProperty, tabledata, evaluate, pytablewriter, lm_eval\n",
            "Successfully installed DataProperty-1.0.1 colorama-0.4.6 evaluate-0.4.1 jsonlines-4.0.0 lm_eval-1.0.0 mbstrdecoder-1.1.3 pathvalidate-3.2.0 portalocker-2.8.2 pybind11-2.12.0 pytablewriter-1.2.0 responses-0.18.0 rouge-score-0.1.2 sacrebleu-2.4.1 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "# Install LM-Eval\n",
        "!pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git@big-refactor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHSaptfPcfJ9"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXfUxP_qlcKn",
        "outputId": "55c54c18-b20e-48d3-b185-426b12d2354b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 10:53:04.084573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 10:53:04.084624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 10:53:04.086748: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 10:53:05.124364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB_p4oTTluHG",
        "outputId": "6ab93f7f-29c8-4e55-c1f8-10f222394d9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PhmBepgWc-8W"
      },
      "outputs": [],
      "source": [
        "# os is a standard Python library that provides functions for interacting with the operating system.\n",
        "import os\n",
        "\n",
        "# torch is the main package of PyTorch, an open-source machine learning library for Python.\n",
        "import torch\n",
        "\n",
        "# load_dataset is a function from the datasets library by Hugging Face. It allows you to load and preprocess datasets for machine learning models.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The transformers library is a popular library for Natural Language Processing (NLP). It provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, summarization, translation, and more.\n",
        "from transformers import (\n",
        "    # AutoModelForCausalLM is a class in the transformers library. It represents a model for causal language modeling.\n",
        "    AutoModelForCausalLM,\n",
        "\n",
        "    # AutoTokenizer is a class in the transformers library. It is used for converting input data into a format that can be used by the model.\n",
        "    AutoTokenizer,\n",
        "\n",
        "    # BitsAndBytesConfig is a configuration class in the transformers library. It is used to configure a BitsAndBytes model.\n",
        "    BitsAndBytesConfig,\n",
        "\n",
        "    # HfArgumentParser is a class in the transformers library. It is used for parsing command-line arguments.\n",
        "    HfArgumentParser,\n",
        "\n",
        "    # TrainingArguments is a class in the transformers library. It defines the arguments used during training.\n",
        "    TrainingArguments,\n",
        "\n",
        "    # pipeline is a high-level function in the transformers library. It creates a pipeline that applies a model to some input data.\n",
        "    pipeline,\n",
        "\n",
        "    # logging is a module in the transformers library. It is used for logging events during training and evaluation.\n",
        "    logging,\n",
        "\n",
        "    # A generic model class that will be instantiated as one of the model classes of the library\n",
        "    #(with a question answering head) when created with the from_pretrained() class method or the from_config() class method\n",
        "    AutoModelForQuestionAnswering\n",
        ")\n",
        "\n",
        "# used for Parameter-Efficient Fine-Tuning\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "# install SFTTrainer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# allows addition of progress bars to loops and iterable objects\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVGBFIo1eFGl"
      },
      "source": [
        "### Install lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guVuQvOAeHiu",
        "outputId": "9ac2f50e-2f14-453b-ffa4-3c2e7064ecc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 32695, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 32695 (delta 31), reused 53 (delta 17), pack-reused 32620\u001b[K\n",
            "Receiving objects: 100% (32695/32695), 22.81 MiB | 8.65 MiB/s, done.\n",
            "Resolving deltas: 100% (22840/22840), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t1xCaCKWgJsF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# change directory\n",
        "os.chdir(\"lm-evaluation-harness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-aNRDLYgE8m",
        "outputId": "1ae58937-3fba-4b19-bb64-757871a1c0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current directory is: /content/lm-evaluation-harness\n",
            "\n",
            "Files in current directory:\n",
            "./CITATION.bib: 1712228001.2430425\n",
            "./README.md: 1712228001.2430425\n",
            "./CODEOWNERS: 1712228001.2430425\n",
            "./LICENSE.md: 1712228001.2430425\n",
            "./docs: 1712228001.2460427\n",
            "./examples: 1712228001.2460427\n",
            "./ignore.txt: 1712228001.2460427\n",
            "./lm_eval: 1712228001.3570511\n",
            "./pyproject.toml: 1712228001.3580513\n",
            "./pile_statistics.json: 1712228001.3580513\n",
            "./requirements.txt: 1712228001.3580513\n",
            "./mypy.ini: 1712228001.3580513\n",
            "./setup.py: 1712228001.3590512\n",
            "./scripts: 1712228001.3590512\n",
            "./templates: 1712228001.3590512\n",
            "./tests: 1712228001.3880534\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# get current working dirctory and list files\n",
        "print(f\"current directory is: {os.getcwd()}\\n\")\n",
        "# print(os.listdir('.'))\n",
        "\n",
        "# Get a list of all files and directories in the current directory\n",
        "files = glob.glob('./*')\n",
        "\n",
        "# Create a list of tuples, each containing the name of the file/directory and its last modification time\n",
        "files_with_times = [(file, os.path.getmtime(file)) for file in files]\n",
        "\n",
        "# Sort the list by the modification time (the second element of each tuple)\n",
        "files_with_times.sort(key=lambda x: x[1])\n",
        "\n",
        "# Print the sorted list\n",
        "print(\"Files in current directory:\")\n",
        "for file, mtime in files_with_times:\n",
        "    print(f'{file}: {mtime}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZnEnEQjfx5t",
        "outputId": "a9b6a323-7fdf-4217-be0f-a34a25316d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/lm-evaluation-harness (from -r requirements.txt (line 1))\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.28.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.12.0)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.0.11)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.22.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (0.3.8)\n",
            "Collecting word2number (from lm_eval==0.4.2->-r requirements.txt (line 1))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.2->-r requirements.txt (line 1)) (10.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (4.9.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.1->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.15.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->lm_eval==0.4.2->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: DataProperty<2,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.1.3)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (0.1.4)\n",
            "Requirement already satisfied: typepy[datetime]<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.2->-r requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.2->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.2->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->lm_eval==0.4.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: lm_eval, word2number\n",
            "  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lm_eval: filename=lm_eval-0.4.2-0.editable-py3-none-any.whl size=16122 sha256=d71f34563e05ce0a663d9b69e679be4a626d9c673dae3c2ca73cbbbd61a08bb3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-av5xbxk3/wheels/dc/8d/a0/ce1a137b6a29fcf5007da91566ee423695e01d20703991091d\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=b4e0db9b00188139f80a2f53478f6fc1b31b07a30365d808216c2f134dcd6c47\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "Successfully built lm_eval word2number\n",
            "Installing collected packages: word2number, lm_eval\n",
            "  Attempting uninstall: lm_eval\n",
            "    Found existing installation: lm_eval 1.0.0\n",
            "    Uninstalling lm_eval-1.0.0:\n",
            "      Successfully uninstalled lm_eval-1.0.0\n",
            "Successfully installed lm_eval-0.4.2 word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RfzOzZp2jdvc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3d4d5545110c47578ef83be88fa3f03a",
            "45c67899b62148e9a064209d51d1700e",
            "4806b10419db44b792a304fb2ee6d0ba",
            "ed82baa673714109a091e785f65a39f8",
            "cb502d1982944f6cacc4b8b2bb56925a",
            "039cd8e2213947b7aaf29638d0ffd17b",
            "4f1ccd07713349618d3ccc61e0205f42",
            "791461c7dc2444829eabbf5f5a8b3f8b",
            "663ae81cf4d9405580a5d77c7fa521e9",
            "3369f67dc815474194b95227d1f01592",
            "cda46cef5e1849bbb73cc53812a6f5eb"
          ]
        },
        "outputId": "c1e35a20-4b84-458c-86c0-2e0d4768d0e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.67k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d4d5545110c47578ef83be88fa3f03a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from lm_eval import api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lm_eval Help"
      ],
      "metadata": {
        "id": "knM_tSjvcx8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P5m0FPdQrRs",
        "outputId": "b1b9a604-0dc5-4b8c-d4c5-50d6a74e05a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 10:53:43.449568: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 10:53:43.449619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 10:53:43.451355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 10:53:44.649106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: lm_eval [-h] [--model MODEL] [--tasks task1,task2] [--model_args MODEL_ARGS]\n",
            "               [--num_fewshot N] [--batch_size auto|auto:N|N] [--max_batch_size N]\n",
            "               [--device DEVICE] [--output_path DIR|DIR/file.json] [--limit N|0<N<1]\n",
            "               [--use_cache DIR] [--cache_requests {true,refresh,delete}] [--check_integrity]\n",
            "               [--write_out] [--log_samples] [--show_config] [--include_path DIR]\n",
            "               [--gen_kwargs GEN_KWARGS] [--verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG]\n",
            "               [--wandb_args WANDB_ARGS] [--predict_only] [--seed SEED] [--trust_remote_code]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL, -m MODEL\n",
            "                        Name of model e.g. `hf`\n",
            "  --tasks task1,task2, -t task1,task2\n",
            "                        To get full list of tasks, use the command lm-eval --tasks list\n",
            "  --model_args MODEL_ARGS, -a MODEL_ARGS\n",
            "                        Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\n",
            "  --num_fewshot N, -f N\n",
            "                        Number of examples in few-shot context\n",
            "  --batch_size auto|auto:N|N, -b auto|auto:N|N\n",
            "                        Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\n",
            "  --max_batch_size N    Maximal batch size to try with --batch_size auto.\n",
            "  --device DEVICE       Device to use (e.g. cuda, cuda:0, cpu).\n",
            "  --output_path DIR|DIR/file.json, -o DIR|DIR/file.json\n",
            "                        The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\n",
            "  --limit N|0<N<1, -L N|0<N<1\n",
            "                        Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n",
            "  --use_cache DIR, -c DIR\n",
            "                        A path to a sqlite db file for caching model responses. `None` if not caching.\n",
            "  --cache_requests {true,refresh,delete}\n",
            "                        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\n",
            "  --check_integrity     Whether to run the relevant part of the test suite for the tasks.\n",
            "  --write_out, -w       Prints the prompt for the first few documents.\n",
            "  --log_samples, -s     If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\n",
            "  --show_config         If True, shows the the full config of all tasks at the end of the evaluation.\n",
            "  --include_path DIR    Additional path to include if there are external tasks to include.\n",
            "  --gen_kwargs GEN_KWARGS\n",
            "                        String arguments for model generation on greedy_until tasks, e.g. `temperature=0,top_k=0,top_p=0`.\n",
            "  --verbosity CRITICAL|ERROR|WARNING|INFO|DEBUG, -v CRITICAL|ERROR|WARNING|INFO|DEBUG\n",
            "                        Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\n",
            "  --wandb_args WANDB_ARGS\n",
            "                        Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\n",
            "  --predict_only, -x    Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\n",
            "  --seed SEED           Set seed for python's random, numpy and torch.\n",
            "                        Accepts a comma-separated list of 3 values for python's random, numpy, and torch seeds, respectively, or a single integer to set the same seed for all three.\n",
            "                        The values are either an integer or 'None' to not set the seed. Default is `0,1234,1234` (for backward compatibility).\n",
            "                        E.g. `--seed 0,None,8` sets `random.seed(0)` and `torch.manual_seed(8)`. Here numpy's seed is not set since the second value is `None`.\n",
            "                        E.g, `--seed 42` sets all three seeds to 42.\n",
            "  --trust_remote_code   Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non Quantized Model Evaluation"
      ],
      "metadata": {
        "id": "LlUrYSbbkoN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Llama2-7b-chat-HF"
      ],
      "metadata": {
        "id": "bXZg29J7mxO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MMLU\n",
        "\n",
        "5-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "KtZ2vtxLT1Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# create directory to store results\n",
        "results_dir = f\"/content/drive/MyDrive/Evaluation/{eval_model}\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "jiNjgdI-t5Ss"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True \\\n",
        "    --tasks mmlu_stem,mmlu_social_sciences,mmlu_humanities,mmlu_other \\\n",
        "    --num_fewshot 5 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size 4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_mmlu \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Llama2-7b-chat-HF-MMLU"
      ],
      "metadata": {
        "id": "fq9MbG6Jm6hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22ec3d2-f34c-49c1-ea59-91942576adda"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 15:40:19.874917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 15:40:19.874968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 15:40:19.876911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 15:40:21.108507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_154026-lgvaw8sr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama2-7b-chat-HF-MMLU\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/lgvaw8sr\u001b[0m\n",
            "2024-04-04:15:40:27,498 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:15:40:33,662 INFO     [__main__.py:335] Selected Tasks: ['mmlu_humanities', 'mmlu_other', 'mmlu_social_sciences', 'mmlu_stem']\n",
            "2024-04-04:15:40:33,663 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:15:40:33,663 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-2-7b-chat-hf', 'trust_remote_code': True, 'do_sample': True}\n",
            "2024-04-04:15:40:33,667 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.30s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
            "2024-04-04:15:44:06,966 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
            "2024-04-04:15:44:06,967 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
            "2024-04-04:15:44:06,968 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_management from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
            "2024-04-04:15:44:06,969 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_virology from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
            "2024-04-04:15:44:06,970 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
            "2024-04-04:15:44:06,980 INFO     [task.py:395] Building contexts for mmlu_conceptual_physics on rank 0...\n",
            "100% 235/235 [00:03<00:00, 65.74it/s]\n",
            "2024-04-04:15:44:10,571 INFO     [task.py:395] Building contexts for mmlu_high_school_biology on rank 0...\n",
            "100% 310/310 [00:04<00:00, 66.16it/s]\n",
            "2024-04-04:15:44:15,274 INFO     [task.py:395] Building contexts for mmlu_anatomy on rank 0...\n",
            "100% 135/135 [00:02<00:00, 65.80it/s]\n",
            "2024-04-04:15:44:17,335 INFO     [task.py:395] Building contexts for mmlu_college_chemistry on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.97it/s]\n",
            "2024-04-04:15:44:18,862 INFO     [task.py:395] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
            "100% 378/378 [00:05<00:00, 66.18it/s]\n",
            "2024-04-04:15:44:24,594 INFO     [task.py:395] Building contexts for mmlu_college_biology on rank 0...\n",
            "100% 144/144 [00:02<00:00, 66.15it/s]\n",
            "2024-04-04:15:44:26,782 INFO     [task.py:395] Building contexts for mmlu_electrical_engineering on rank 0...\n",
            "100% 145/145 [00:02<00:00, 65.86it/s]\n",
            "2024-04-04:15:44:28,993 INFO     [task.py:395] Building contexts for mmlu_machine_learning on rank 0...\n",
            "100% 112/112 [00:01<00:00, 65.79it/s]\n",
            "2024-04-04:15:44:30,705 INFO     [task.py:395] Building contexts for mmlu_high_school_statistics on rank 0...\n",
            "100% 216/216 [00:03<00:00, 66.25it/s]\n",
            "2024-04-04:15:44:33,979 INFO     [task.py:395] Building contexts for mmlu_computer_security on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.21it/s]\n",
            "2024-04-04:15:44:35,497 INFO     [task.py:395] Building contexts for mmlu_abstract_algebra on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.00it/s]\n",
            "2024-04-04:15:44:37,020 INFO     [task.py:395] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.79it/s]\n",
            "2024-04-04:15:44:38,547 INFO     [task.py:395] Building contexts for mmlu_college_physics on rank 0...\n",
            "100% 102/102 [00:01<00:00, 65.85it/s]\n",
            "2024-04-04:15:44:40,104 INFO     [task.py:395] Building contexts for mmlu_college_computer_science on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.85it/s]\n",
            "2024-04-04:15:44:41,631 INFO     [task.py:395] Building contexts for mmlu_astronomy on rank 0...\n",
            "100% 152/152 [00:02<00:00, 65.72it/s]\n",
            "2024-04-04:15:44:43,954 INFO     [task.py:395] Building contexts for mmlu_high_school_physics on rank 0...\n",
            "100% 151/151 [00:02<00:00, 65.77it/s]\n",
            "2024-04-04:15:44:46,260 INFO     [task.py:395] Building contexts for mmlu_college_mathematics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.85it/s]\n",
            "2024-04-04:15:44:47,789 INFO     [task.py:395] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
            "100% 270/270 [00:04<00:00, 65.52it/s]\n",
            "2024-04-04:15:44:51,927 INFO     [task.py:395] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
            "100% 203/203 [00:03<00:00, 65.32it/s]\n",
            "2024-04-04:15:44:55,047 INFO     [task.py:395] Building contexts for mmlu_human_sexuality on rank 0...\n",
            "100% 131/131 [00:01<00:00, 66.17it/s]\n",
            "2024-04-04:15:44:57,037 INFO     [task.py:395] Building contexts for mmlu_high_school_geography on rank 0...\n",
            "100% 198/198 [00:02<00:00, 66.01it/s]\n",
            "2024-04-04:15:45:00,048 INFO     [task.py:395] Building contexts for mmlu_econometrics on rank 0...\n",
            "100% 114/114 [00:01<00:00, 65.54it/s]\n",
            "2024-04-04:15:45:01,800 INFO     [task.py:395] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
            "100% 390/390 [00:05<00:00, 65.66it/s]\n",
            "2024-04-04:15:45:07,768 INFO     [task.py:395] Building contexts for mmlu_professional_psychology on rank 0...\n",
            "100% 612/612 [00:09<00:00, 63.45it/s]\n",
            "2024-04-04:15:45:17,447 INFO     [task.py:395] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
            "100% 193/193 [00:02<00:00, 65.66it/s]\n",
            "2024-04-04:15:45:20,405 INFO     [task.py:395] Building contexts for mmlu_high_school_psychology on rank 0...\n",
            "100% 545/545 [00:08<00:00, 65.94it/s]\n",
            "2024-04-04:15:45:28,701 INFO     [task.py:395] Building contexts for mmlu_security_studies on rank 0...\n",
            "100% 245/245 [00:03<00:00, 64.91it/s]\n",
            "2024-04-04:15:45:32,491 INFO     [task.py:395] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.02it/s]\n",
            "2024-04-04:15:45:34,039 INFO     [task.py:395] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
            "100% 238/238 [00:03<00:00, 65.72it/s]\n",
            "2024-04-04:15:45:37,674 INFO     [task.py:395] Building contexts for mmlu_public_relations on rank 0...\n",
            "100% 110/110 [00:01<00:00, 65.43it/s]\n",
            "2024-04-04:15:45:39,365 INFO     [task.py:395] Building contexts for mmlu_sociology on rank 0...\n",
            "100% 201/201 [00:03<00:00, 65.74it/s]\n",
            "2024-04-04:15:45:42,438 INFO     [task.py:395] Building contexts for mmlu_professional_accounting on rank 0...\n",
            "100% 282/282 [00:04<00:00, 65.92it/s]\n",
            "2024-04-04:15:45:46,732 INFO     [task.py:395] Building contexts for mmlu_medical_genetics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.52it/s]\n",
            "2024-04-04:15:45:48,267 INFO     [task.py:395] Building contexts for mmlu_college_medicine on rank 0...\n",
            "100% 173/173 [00:02<00:00, 65.62it/s]\n",
            "2024-04-04:15:45:50,917 INFO     [task.py:395] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
            "100% 265/265 [00:04<00:00, 65.92it/s]\n",
            "2024-04-04:15:45:54,952 INFO     [task.py:395] Building contexts for mmlu_management on rank 0...\n",
            "100% 103/103 [00:01<00:00, 65.17it/s]\n",
            "2024-04-04:15:45:56,542 INFO     [task.py:395] Building contexts for mmlu_human_aging on rank 0...\n",
            "100% 223/223 [00:03<00:00, 66.05it/s]\n",
            "2024-04-04:15:45:59,932 INFO     [task.py:395] Building contexts for mmlu_business_ethics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.78it/s]\n",
            "2024-04-04:15:46:01,459 INFO     [task.py:395] Building contexts for mmlu_global_facts on rank 0...\n",
            "100% 100/100 [00:01<00:00, 65.62it/s]\n",
            "2024-04-04:15:46:03,000 INFO     [task.py:395] Building contexts for mmlu_miscellaneous on rank 0...\n",
            "100% 783/783 [00:11<00:00, 65.48it/s]\n",
            "2024-04-04:15:46:15,000 INFO     [task.py:395] Building contexts for mmlu_nutrition on rank 0...\n",
            "100% 306/306 [00:04<00:00, 65.70it/s]\n",
            "2024-04-04:15:46:19,678 INFO     [task.py:395] Building contexts for mmlu_professional_medicine on rank 0...\n",
            "100% 272/272 [00:04<00:00, 65.42it/s]\n",
            "2024-04-04:15:46:23,853 INFO     [task.py:395] Building contexts for mmlu_marketing on rank 0...\n",
            "100% 234/234 [00:03<00:00, 65.87it/s]\n",
            "2024-04-04:15:46:27,421 INFO     [task.py:395] Building contexts for mmlu_virology on rank 0...\n",
            "100% 166/166 [00:02<00:00, 65.52it/s]\n",
            "2024-04-04:15:46:29,968 INFO     [task.py:395] Building contexts for mmlu_high_school_world_history on rank 0...\n",
            "100% 237/237 [00:03<00:00, 65.72it/s]\n",
            "2024-04-04:15:46:33,591 INFO     [task.py:395] Building contexts for mmlu_international_law on rank 0...\n",
            "100% 121/121 [00:01<00:00, 65.38it/s]\n",
            "2024-04-04:15:46:35,454 INFO     [task.py:395] Building contexts for mmlu_high_school_european_history on rank 0...\n",
            "100% 165/165 [00:02<00:00, 65.79it/s]\n",
            "2024-04-04:15:46:37,977 INFO     [task.py:395] Building contexts for mmlu_moral_disputes on rank 0...\n",
            "100% 346/346 [00:05<00:00, 62.14it/s]\n",
            "2024-04-04:15:46:43,567 INFO     [task.py:395] Building contexts for mmlu_prehistory on rank 0...\n",
            "100% 324/324 [00:04<00:00, 66.11it/s]\n",
            "2024-04-04:15:46:48,491 INFO     [task.py:395] Building contexts for mmlu_high_school_us_history on rank 0...\n",
            "100% 204/204 [00:03<00:00, 65.05it/s]\n",
            "2024-04-04:15:46:51,675 INFO     [task.py:395] Building contexts for mmlu_professional_law on rank 0...\n",
            "100% 1534/1534 [00:23<00:00, 65.78it/s]\n",
            "2024-04-04:15:47:15,082 INFO     [task.py:395] Building contexts for mmlu_moral_scenarios on rank 0...\n",
            "100% 895/895 [00:13<00:00, 65.74it/s]\n",
            "2024-04-04:15:47:28,741 INFO     [task.py:395] Building contexts for mmlu_world_religions on rank 0...\n",
            "100% 171/171 [00:02<00:00, 64.96it/s]\n",
            "2024-04-04:15:47:31,384 INFO     [task.py:395] Building contexts for mmlu_jurisprudence on rank 0...\n",
            "100% 108/108 [00:01<00:00, 65.02it/s]\n",
            "2024-04-04:15:47:33,055 INFO     [task.py:395] Building contexts for mmlu_logical_fallacies on rank 0...\n",
            "100% 163/163 [00:02<00:00, 65.63it/s]\n",
            "2024-04-04:15:47:35,551 INFO     [task.py:395] Building contexts for mmlu_formal_logic on rank 0...\n",
            "100% 126/126 [00:01<00:00, 65.35it/s]\n",
            "2024-04-04:15:47:37,491 INFO     [task.py:395] Building contexts for mmlu_philosophy on rank 0...\n",
            "100% 311/311 [00:04<00:00, 65.39it/s]\n",
            "2024-04-04:15:47:42,262 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 56168/56168 [16:35<00:00, 56.45it/s] \n",
            "hf (pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: 4\n",
            "|                Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|--------------------------------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem                                  |N/A    |none  |     5|acc   |0.3803|±  |0.0085|\n",
            "| - abstract_algebra                   |      0|none  |     5|acc   |0.3000|±  |0.0461|\n",
            "| - anatomy                            |      0|none  |     5|acc   |0.4222|±  |0.0427|\n",
            "| - astronomy                          |      0|none  |     5|acc   |0.4737|±  |0.0406|\n",
            "| - college_biology                    |      0|none  |     5|acc   |0.5139|±  |0.0418|\n",
            "| - college_chemistry                  |      0|none  |     5|acc   |0.2800|±  |0.0451|\n",
            "| - college_computer_science           |      0|none  |     5|acc   |0.3800|±  |0.0488|\n",
            "| - college_mathematics                |      0|none  |     5|acc   |0.3600|±  |0.0482|\n",
            "| - college_physics                    |      0|none  |     5|acc   |0.2255|±  |0.0416|\n",
            "| - computer_security                  |      0|none  |     5|acc   |0.5700|±  |0.0498|\n",
            "| - conceptual_physics                 |      0|none  |     5|acc   |0.4128|±  |0.0322|\n",
            "| - electrical_engineering             |      0|none  |     5|acc   |0.4966|±  |0.0417|\n",
            "| - elementary_mathematics             |      0|none  |     5|acc   |0.2989|±  |0.0236|\n",
            "| - high_school_biology                |      0|none  |     5|acc   |0.5226|±  |0.0284|\n",
            "| - high_school_chemistry              |      0|none  |     5|acc   |0.3842|±  |0.0342|\n",
            "| - high_school_computer_science       |      0|none  |     5|acc   |0.4200|±  |0.0496|\n",
            "| - high_school_mathematics            |      0|none  |     5|acc   |0.2556|±  |0.0266|\n",
            "| - high_school_physics                |      0|none  |     5|acc   |0.2914|±  |0.0371|\n",
            "| - high_school_statistics             |      0|none  |     5|acc   |0.3333|±  |0.0321|\n",
            "| - machine_learning                   |      0|none  |     5|acc   |0.3125|±  |0.0440|\n",
            "|social_sciences                       |N/A    |none  |     5|acc   |0.5466|±  |0.0088|\n",
            "| - econometrics                       |      0|none  |     5|acc   |0.3596|±  |0.0451|\n",
            "| - high_school_geography              |      0|none  |     5|acc   |0.5909|±  |0.0350|\n",
            "| - high_school_government_and_politics|      0|none  |     5|acc   |0.6995|±  |0.0331|\n",
            "| - high_school_macroeconomics         |      0|none  |     5|acc   |0.4154|±  |0.0250|\n",
            "| - high_school_microeconomics         |      0|none  |     5|acc   |0.4244|±  |0.0321|\n",
            "| - high_school_psychology             |      0|none  |     5|acc   |0.6807|±  |0.0200|\n",
            "| - human_sexuality                    |      0|none  |     5|acc   |0.5725|±  |0.0434|\n",
            "| - professional_psychology            |      0|none  |     5|acc   |0.4788|±  |0.0202|\n",
            "| - public_relations                   |      0|none  |     5|acc   |0.5273|±  |0.0478|\n",
            "| - security_studies                   |      0|none  |     5|acc   |0.5143|±  |0.0320|\n",
            "| - sociology                          |      0|none  |     5|acc   |0.6517|±  |0.0337|\n",
            "| - us_foreign_policy                  |      0|none  |     5|acc   |0.7200|±  |0.0451|\n",
            "|other                                 |N/A    |none  |     5|acc   |0.5468|±  |0.0087|\n",
            "| - business_ethics                    |      0|none  |     5|acc   |0.5200|±  |0.0502|\n",
            "| - clinical_knowledge                 |      0|none  |     5|acc   |0.5472|±  |0.0306|\n",
            "| - college_medicine                   |      0|none  |     5|acc   |0.4104|±  |0.0375|\n",
            "| - global_facts                       |      0|none  |     5|acc   |0.3600|±  |0.0482|\n",
            "| - human_aging                        |      0|none  |     5|acc   |0.5785|±  |0.0331|\n",
            "| - management                         |      0|none  |     5|acc   |0.6699|±  |0.0466|\n",
            "| - marketing                          |      0|none  |     5|acc   |0.7094|±  |0.0297|\n",
            "| - medical_genetics                   |      0|none  |     5|acc   |0.4800|±  |0.0502|\n",
            "| - miscellaneous                      |      0|none  |     5|acc   |0.6756|±  |0.0167|\n",
            "| - nutrition                          |      0|none  |     5|acc   |0.5098|±  |0.0286|\n",
            "| - professional_accounting            |      0|none  |     5|acc   |0.3582|±  |0.0286|\n",
            "| - professional_medicine              |      0|none  |     5|acc   |0.4632|±  |0.0303|\n",
            "| - virology                           |      0|none  |     5|acc   |0.4277|±  |0.0385|\n",
            "|humanities                            |N/A    |none  |     5|acc   |0.4402|±  |0.0069|\n",
            "| - formal_logic                       |      0|none  |     5|acc   |0.2540|±  |0.0389|\n",
            "| - high_school_european_history       |      0|none  |     5|acc   |0.5879|±  |0.0384|\n",
            "| - high_school_us_history             |      0|none  |     5|acc   |0.6765|±  |0.0328|\n",
            "| - high_school_world_history          |      0|none  |     5|acc   |0.6709|±  |0.0306|\n",
            "| - international_law                  |      0|none  |     5|acc   |0.6364|±  |0.0439|\n",
            "| - jurisprudence                      |      0|none  |     5|acc   |0.5926|±  |0.0475|\n",
            "| - logical_fallacies                  |      0|none  |     5|acc   |0.5583|±  |0.0390|\n",
            "| - moral_disputes                     |      0|none  |     5|acc   |0.5289|±  |0.0269|\n",
            "| - moral_scenarios                    |      0|none  |     5|acc   |0.2335|±  |0.0141|\n",
            "| - philosophy                         |      0|none  |     5|acc   |0.5563|±  |0.0282|\n",
            "| - prehistory                         |      0|none  |     5|acc   |0.5679|±  |0.0276|\n",
            "| - professional_law                   |      0|none  |     5|acc   |0.3520|±  |0.0122|\n",
            "| - world_religions                    |      0|none  |     5|acc   |0.7251|±  |0.0342|\n",
            "\n",
            "|    Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|---------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem           |N/A    |none  |     5|acc   |0.3803|±  |0.0085|\n",
            "|social_sciences|N/A    |none  |     5|acc   |0.5466|±  |0.0088|\n",
            "|other          |N/A    |none  |     5|acc   |0.5468|±  |0.0087|\n",
            "|humanities     |N/A    |none  |     5|acc   |0.4402|±  |0.0069|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr 0.04606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_abstract_algebra/alias  - abstract_algebra\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc 0.42222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr 0.04267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_anatomy/alias  - anatomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc 0.47368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr 0.04063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_astronomy/alias  - astronomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc 0.52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr 0.05021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_business_ethics/alias  - business_ethics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc 0.54717\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr 0.03064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_clinical_knowledge/alias  - clinical_knowledg...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc 0.51389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr 0.0418\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_biology/alias  - college_biology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc 0.28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr 0.04513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_chemistry/alias  - college_chemistry...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc 0.38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr 0.04878\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_computer_science/alias  - college_computer_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc 0.36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr 0.04824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_college_mathematics/alias  - college_mathemati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc 0.4104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr 0.03751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_college_medicine/alias  - college_medicine\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc 0.22549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr 0.04158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_physics/alias  - college_physics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc 0.57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr 0.04976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_computer_security/alias  - computer_security...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc 0.41277\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr 0.03218\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_conceptual_physics/alias  - conceptual_physic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc 0.35965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr 0.04514\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_econometrics/alias  - econometrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc 0.49655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr 0.04167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_electrical_engineering/alias  - electrical_engine...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc 0.29894\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr 0.02358\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_elementary_mathematics/alias  - elementary_mathem...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc 0.25397\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr 0.03893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_formal_logic/alias  - formal_logic\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc 0.36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr 0.04824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_global_facts/alias  - global_facts\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc 0.52258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr 0.02841\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_biology/alias  - high_school_biolo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc 0.38424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr 0.03422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_chemistry/alias  - high_school_chemi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc 0.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr 0.0496\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_computer_science/alias  - high_school_compu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc 0.58788\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr 0.03844\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_european_history/alias  - high_school_europ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc 0.59091\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr 0.03503\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_geography/alias  - high_school_geogr...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc 0.69948\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr 0.03309\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      mmlu_high_school_government_and_politics/alias  - high_school_gover...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc 0.41538\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr 0.02499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_macroeconomics/alias  - high_school_macro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc 0.25556\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr 0.02659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_mathematics/alias  - high_school_mathe...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc 0.42437\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr 0.0321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_microeconomics/alias  - high_school_micro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc 0.29139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr 0.0371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_physics/alias  - high_school_physi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc 0.68073\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr 0.01999\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_psychology/alias  - high_school_psych...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc 0.33333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr 0.03215\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_statistics/alias  - high_school_stati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc 0.67647\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr 0.03283\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_us_history/alias  - high_school_us_hi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc 0.67089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr 0.03059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                mmlu_high_school_world_history/alias  - high_school_world...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc 0.57848\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr 0.03314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_human_aging/alias  - human_aging\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc 0.57252\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr 0.04339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_human_sexuality/alias  - human_sexuality\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc 0.44017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr 0.00687\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_humanities/alias humanities\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc 0.63636\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr 0.04391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_international_law/alias  - international_law...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc 0.59259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr 0.0475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_jurisprudence/alias  - jurisprudence\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc 0.55828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr 0.03902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_logical_fallacies/alias  - logical_fallacies...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc 0.3125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr 0.04399\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_machine_learning/alias  - machine_learning\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc 0.6699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr 0.04656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_management/alias  - management\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc 0.7094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr 0.02975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_marketing/alias  - marketing\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc 0.48\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr 0.05021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_medical_genetics/alias  - medical_genetics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc 0.67561\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr 0.01674\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_miscellaneous/alias  - miscellaneous\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc 0.5289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr 0.02687\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_moral_disputes/alias  - moral_disputes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc 0.23352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr 0.01415\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_moral_scenarios/alias  - moral_scenarios\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc 0.5098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr 0.02862\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_nutrition/alias  - nutrition\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc 0.54683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr 0.0087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_other/alias other\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc 0.55627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr 0.02822\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_philosophy/alias  - philosophy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc 0.5679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr 0.02756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_prehistory/alias  - prehistory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc 0.35816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr 0.0286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_accounting/alias  - professional_acco...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc 0.35202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr 0.0122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_professional_law/alias  - professional_law\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc 0.46324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr 0.03029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_medicine/alias  - professional_medi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc 0.47876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr 0.02021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_psychology/alias  - professional_psyc...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc 0.52727\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr 0.04782\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_public_relations/alias  - public_relations\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc 0.51429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr 0.032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_security_studies/alias  - security_studies\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc 0.54664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr 0.00877\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_social_sciences/alias social_sciences\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc 0.65174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr 0.03369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_sociology/alias  - sociology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc 0.38027\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr 0.0085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     mmlu_stem/alias stem\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc 0.72\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr 0.04513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_us_foreign_policy/alias  - us_foreign_policy...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc 0.42771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr 0.03852\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_virology/alias  - virology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc 0.72515\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr 0.03424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_world_religions/alias  - world_religions\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLlama2-7b-chat-HF-MMLU\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/lgvaw8sr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 64 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_154026-lgvaw8sr/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HellaSwag\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "BVRuVVUKVWt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True, \\\n",
        "    --tasks hellaswag \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_hellaswag \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Llama2-7b-chat-HF-Hellaswag"
      ],
      "metadata": {
        "id": "6_d_D8Omc7F5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fb0815-b68d-4aac-8ade-7d60a529d301"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 11:07:20.618950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 11:07:20.618997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 11:07:20.620496: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 11:07:21.804917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_110727-f1p9vo3o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama2-7b-chat-HF-Hellaswag\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/f1p9vo3o\u001b[0m\n",
            "2024-04-04:11:07:28,169 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:11:07:34,237 INFO     [__main__.py:335] Selected Tasks: ['hellaswag']\n",
            "2024-04-04:11:07:34,238 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:11:07:34,238 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-2-7b-chat-hf', 'trust_remote_code': True}\n",
            "2024-04-04:11:07:34,242 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.60s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 4.36k/4.36k [00:00<00:00, 11.8MB/s]\n",
            "Downloading metadata: 100% 2.53k/2.53k [00:00<00:00, 8.41MB/s]\n",
            "Downloading readme: 100% 6.84k/6.84k [00:00<00:00, 18.3MB/s]\n",
            "Downloading data: 47.5MB [00:00, 74.6MB/s]\n",
            "Downloading data: 11.8MB [00:00, 64.3MB/s]\n",
            "Downloading data: 12.2MB [00:00, 63.8MB/s]\n",
            "Generating train split: 100% 39905/39905 [00:04<00:00, 9424.83 examples/s]\n",
            "Generating test split: 100% 10003/10003 [00:01<00:00, 9537.15 examples/s]\n",
            "Generating validation split: 100% 10042/10042 [00:01<00:00, 9521.77 examples/s]\n",
            "Map: 100% 39905/39905 [00:06<00:00, 6249.92 examples/s]\n",
            "Map: 100% 10042/10042 [00:01<00:00, 5524.41 examples/s]\n",
            "2024-04-04:11:08:22,006 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2024-04-04:11:08:22,007 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
            "100% 10042/10042 [00:04<00:00, 2155.67it/s]\n",
            "2024-04-04:11:08:27,779 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "Running loglikelihood requests:   0% 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests:  25% 9930/40168 [01:45<04:37, 108.91it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 40168/40168 [04:55<00:00, 135.73it/s]\n",
            "hf (pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4 (64,64,64,64,64)\n",
            "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
            "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
            "|hellaswag|      1|none  |     0|acc     |0.5780|±  |0.0049|\n",
            "|         |       |none  |     0|acc_norm|0.7549|±  |0.0043|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc 0.57797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm 0.75493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr 0.00429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr 0.00493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           hellaswag/alias hellaswag\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLlama2-7b-chat-HF-Hellaswag\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/f1p9vo3o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_110727-f1p9vo3o/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BoolQ\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "PSocie6T5imO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True \\\n",
        "    --tasks boolq \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_boolq \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Llama2-7b-chat-HF-boolq \\\n",
        "    --use_cache results_dir_boolq \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "ypTaxFWs5q8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab1ffa5-1e58-47fc-e246-3419aa0a96ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 11:14:28.760606: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 11:14:28.760660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 11:14:28.762209: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 11:14:29.949854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_111435-wf044pey\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama2-7b-chat-HF-boolq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/wf044pey\u001b[0m\n",
            "2024-04-04:11:14:36,267 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:11:14:42,531 INFO     [__main__.py:335] Selected Tasks: ['boolq']\n",
            "2024-04-04:11:14:42,532 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:11:14:42,532 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-2-7b-chat-hf', 'trust_remote_code': True, 'do_sample': True}\n",
            "2024-04-04:11:14:42,536 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.24s/it]\n",
            "2024-04-04:11:14:48,473 INFO     [evaluator.py:195] Using cache at results_dir_boolq_rank0.db\n",
            "2024-04-04:11:14:48,477 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-04-04:11:14:48,478 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "Downloading data: 100% 3.85M/3.85M [00:00<00:00, 9.47MB/s]\n",
            "Downloading data: 100% 1.31M/1.31M [00:00<00:00, 4.38MB/s]\n",
            "Downloading data: 100% 1.31M/1.31M [00:00<00:00, 4.61MB/s]\n",
            "Generating train split: 100% 9427/9427 [00:00<00:00, 243159.65 examples/s]\n",
            "Generating validation split: 100% 3270/3270 [00:00<00:00, 309853.92 examples/s]\n",
            "Generating test split: 100% 3245/3245 [00:00<00:00, 297920.90 examples/s]\n",
            "2024-04-04:11:14:58,928 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
            "2024-04-04:11:14:58,929 INFO     [task.py:395] Building contexts for boolq on rank 0...\n",
            "100% 3270/3270 [00:01<00:00, 1733.64it/s]\n",
            "2024-04-04:11:15:01,535 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:11:15:01,538 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_boolq_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6540/6540 [00:00<00:00, 7132.43it/s]\n",
            "2024-04-04:11:15:02,455 INFO     [model.py:249] Cached requests: 0, Requests remaining: 6540\n",
            "Running loglikelihood requests:   0% 0/6540 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 16\n",
            "Running loglikelihood requests:  24% 1601/6540 [00:36<00:44, 112.24it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 6540/6540 [01:10<00:00, 93.41it/s] \n",
            "{\n",
            "  \"results\": {\n",
            "    \"boolq\": {\n",
            "      \"acc,none\": 0.7975535168195719,\n",
            "      \"acc_stderr,none\": 0.007027932397259822,\n",
            "      \"alias\": \"boolq\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"boolq\": []\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"boolq\": {\n",
            "      \"task\": \"boolq\",\n",
            "      \"group\": [\n",
            "        \"super-glue-lm-eval-v1\"\n",
            "      ],\n",
            "      \"dataset_path\": \"super_glue\",\n",
            "      \"dataset_name\": \"boolq\",\n",
            "      \"training_split\": \"train\",\n",
            "      \"validation_split\": \"validation\",\n",
            "      \"doc_to_text\": \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\",\n",
            "      \"doc_to_target\": \"label\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"no\",\n",
            "        \"yes\"\n",
            "      ],\n",
            "      \"description\": \"\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\"\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": true,\n",
            "      \"doc_to_decontamination_query\": \"passage\",\n",
            "      \"metadata\": {\n",
            "        \"version\": 2.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"boolq\": 2.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"boolq\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      16,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_boolq\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712229282.5318246,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4 (16,64,64,64,64)\n",
            "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
            "|boolq|      2|none  |     0|acc   |0.7976|±  | 0.007|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc 0.79755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr 0.00703\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      boolq/alias boolq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLlama2-7b-chat-HF-boolq\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/wf044pey\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_111435-wf044pey/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BBH\n",
        "\n",
        "3-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "vbrNwikeQlQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True \\\n",
        "    --tasks bbh_fewshot \\\n",
        "    --num_fewshot 3 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_bbh \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Llama2-7b-chat-HF-bbh \\\n",
        "    --use_cache results_dir_bbh \\\n",
        "    --cache_requests true"
      ],
      "metadata": {
        "id": "1V6NAx24Ql8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9124afcb-53ad-46df-a811-e2e577306bbb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 11:16:46.228178: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 11:16:46.228233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 11:16:46.229720: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 11:16:47.453949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_111652-igecq10y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLlama2-7b-chat-HF-bbh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/igecq10y\u001b[0m\n",
            "2024-04-04:11:16:53,679 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:11:16:59,900 INFO     [__main__.py:335] Selected Tasks: ['bbh_fewshot']\n",
            "2024-04-04:11:16:59,901 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:11:16:59,901 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-2-7b-chat-hf', 'trust_remote_code': True, 'do_sample': True}\n",
            "2024-04-04:11:16:59,905 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.23s/it]\n",
            "2024-04-04:11:17:06,042 INFO     [evaluator.py:195] Using cache at results_dir_bbh_rank0.db\n",
            "Downloading data: 100% 16.7k/16.7k [00:00<00:00, 41.1kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 52282.41 examples/s]\n",
            "2024-04-04:11:17:12,617 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:12,617 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 10.4k/10.4k [00:00<00:00, 33.0kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 113090.60 examples/s]\n",
            "2024-04-04:11:17:18,380 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:18,380 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 10.0k/10.0k [00:00<00:00, 36.6kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 126547.91 examples/s]\n",
            "2024-04-04:11:17:23,852 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:23,852 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 35.8k/35.8k [00:00<00:00, 127kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 81849.66 examples/s]\n",
            "2024-04-04:11:17:29,399 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:29,399 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 49.1k/49.1k [00:00<00:00, 183kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 98698.80 examples/s]\n",
            "2024-04-04:11:17:35,436 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:35,436 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 21.7k/21.7k [00:00<00:00, 79.8kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 120084.29 examples/s]\n",
            "2024-04-04:11:17:39,797 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:39,798 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 25.1k/25.1k [00:00<00:00, 91.9kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 94971.11 examples/s]\n",
            "2024-04-04:11:17:45,699 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:45,699 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 44.0k/44.0k [00:00<00:00, 156kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 94254.02 examples/s]\n",
            "2024-04-04:11:17:50,823 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:50,823 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 33.5k/33.5k [00:00<00:00, 120kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 77494.35 examples/s]\n",
            "2024-04-04:11:17:56,636 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:17:56,636 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 44.6k/44.6k [00:00<00:00, 122kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 116121.37 examples/s]\n",
            "2024-04-04:11:18:02,325 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:02,325 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 16.4k/16.4k [00:00<00:00, 60.4kB/s]\n",
            "Generating test split: 100% 178/178 [00:00<00:00, 77086.85 examples/s]\n",
            "2024-04-04:11:18:07,568 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:07,568 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 15.6k/15.6k [00:00<00:00, 57.6kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 121602.23 examples/s]\n",
            "2024-04-04:11:18:13,031 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:13,031 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 69.5k/69.5k [00:00<00:00, 246kB/s]\n",
            "Generating test split: 100% 187/187 [00:00<00:00, 61477.88 examples/s]\n",
            "2024-04-04:11:18:18,673 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:18,674 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 18.0k/18.0k [00:00<00:00, 65.3kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 101204.13 examples/s]\n",
            "2024-04-04:11:18:24,301 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:24,301 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 56.9k/56.9k [00:00<00:00, 166kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 84162.13 examples/s]\n",
            "2024-04-04:11:18:30,225 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:30,225 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 10.6k/10.6k [00:00<00:00, 38.8kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 113506.82 examples/s]\n",
            "2024-04-04:11:18:35,871 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:35,871 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 35.6k/35.6k [00:00<00:00, 129kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 92044.94 examples/s]\n",
            "2024-04-04:11:18:41,684 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:41,684 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 10.0k/10.0k [00:00<00:00, 36.7kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 96857.20 examples/s]\n",
            "2024-04-04:11:18:47,247 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:47,247 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 20.4k/20.4k [00:00<00:00, 66.5kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 110434.54 examples/s]\n",
            "2024-04-04:11:18:52,908 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:52,908 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 21.6k/21.6k [00:00<00:00, 78.6kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 101194.36 examples/s]\n",
            "2024-04-04:11:18:58,329 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:18:58,329 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 20.2k/20.2k [00:00<00:00, 73.6kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 116676.98 examples/s]\n",
            "2024-04-04:11:19:03,699 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:03,699 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 15.6k/15.6k [00:00<00:00, 56.4kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 101665.31 examples/s]\n",
            "2024-04-04:11:19:08,825 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:08,825 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 10.7k/10.7k [00:00<00:00, 39.1kB/s]\n",
            "Generating test split: 100% 146/146 [00:00<00:00, 57776.05 examples/s]\n",
            "2024-04-04:11:19:13,980 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:13,980 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 8.16k/8.16k [00:00<00:00, 29.8kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 125788.87 examples/s]\n",
            "2024-04-04:11:19:19,755 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:19,756 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 7.55k/7.55k [00:00<00:00, 27.0kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 121419.18 examples/s]\n",
            "2024-04-04:11:19:25,282 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:25,282 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 37.1k/37.1k [00:00<00:00, 135kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 91923.91 examples/s]\n",
            "2024-04-04:11:19:30,501 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:30,501 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 100% 4.70k/4.70k [00:00<00:00, 17.0kB/s]\n",
            "Generating test split: 100% 250/250 [00:00<00:00, 104077.02 examples/s]\n",
            "2024-04-04:11:19:35,979 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:35,979 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:11:19:35,986 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_boolean_expressions in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,986 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_multistep_arithmetic_two in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_sports_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_penguins_in_a_table in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_web_of_lies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_geometric_shapes in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_reasoning_about_colored_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_navigate in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_temporal_sequences in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_object_counting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,987 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_salient_translation_error_detection in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_date_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_causal_judgement in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_ruin_names in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_snarks in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_word_sorting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_movie_recommendation in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_formal_fallacies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,988 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_dyck_languages in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,989 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_hyperbaton in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,989 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_disambiguation_qa in its config. Manual configuration will be ignored.\n",
            "2024-04-04:11:19:35,992 INFO     [task.py:395] Building contexts for bbh_fewshot_boolean_expressions on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1211.44it/s]\n",
            "2024-04-04:11:19:36,235 INFO     [task.py:395] Building contexts for bbh_fewshot_tracking_shuffled_objects_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1026.06it/s]\n",
            "2024-04-04:11:19:36,515 INFO     [task.py:395] Building contexts for bbh_fewshot_multistep_arithmetic_two on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1243.06it/s]\n",
            "2024-04-04:11:19:36,752 INFO     [task.py:395] Building contexts for bbh_fewshot_sports_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1136.99it/s]\n",
            "2024-04-04:11:19:37,009 INFO     [task.py:395] Building contexts for bbh_fewshot_penguins_in_a_table on rank 0...\n",
            "100% 146/146 [00:00<00:00, 1039.03it/s]\n",
            "2024-04-04:11:19:37,172 INFO     [task.py:395] Building contexts for bbh_fewshot_web_of_lies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1153.96it/s]\n",
            "2024-04-04:11:19:37,427 INFO     [task.py:395] Building contexts for bbh_fewshot_geometric_shapes on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1058.39it/s]\n",
            "2024-04-04:11:19:37,702 INFO     [task.py:395] Building contexts for bbh_fewshot_logical_deduction_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1053.69it/s]\n",
            "2024-04-04:11:19:37,977 INFO     [task.py:395] Building contexts for bbh_fewshot_reasoning_about_colored_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1035.02it/s]\n",
            "2024-04-04:11:19:38,260 INFO     [task.py:395] Building contexts for bbh_fewshot_navigate on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1093.45it/s]\n",
            "2024-04-04:11:19:38,532 INFO     [task.py:395] Building contexts for bbh_fewshot_temporal_sequences on rank 0...\n",
            "100% 250/250 [00:00<00:00, 878.73it/s]\n",
            "2024-04-04:11:19:38,855 INFO     [task.py:395] Building contexts for bbh_fewshot_object_counting on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1151.52it/s]\n",
            "2024-04-04:11:19:39,108 INFO     [task.py:395] Building contexts for bbh_fewshot_salient_translation_error_detection on rank 0...\n",
            "100% 250/250 [00:00<00:00, 751.49it/s]\n",
            "2024-04-04:11:19:39,482 INFO     [task.py:395] Building contexts for bbh_fewshot_date_understanding on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1151.27it/s]\n",
            "2024-04-04:11:19:39,735 INFO     [task.py:395] Building contexts for bbh_fewshot_causal_judgement on rank 0...\n",
            "100% 187/187 [00:00<00:00, 948.84it/s]\n",
            "2024-04-04:11:19:39,962 INFO     [task.py:395] Building contexts for bbh_fewshot_ruin_names on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1126.24it/s]\n",
            "2024-04-04:11:19:40,220 INFO     [task.py:395] Building contexts for bbh_fewshot_snarks on rank 0...\n",
            "100% 178/178 [00:00<00:00, 1073.81it/s]\n",
            "2024-04-04:11:19:40,413 INFO     [task.py:395] Building contexts for bbh_fewshot_word_sorting on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1199.06it/s]\n",
            "2024-04-04:11:19:40,658 INFO     [task.py:395] Building contexts for bbh_fewshot_logical_deduction_five_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1043.15it/s]\n",
            "2024-04-04:11:19:40,935 INFO     [task.py:395] Building contexts for bbh_fewshot_logical_deduction_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1046.41it/s]\n",
            "2024-04-04:11:19:41,211 INFO     [task.py:395] Building contexts for bbh_fewshot_tracking_shuffled_objects_three_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1023.64it/s]\n",
            "2024-04-04:11:19:41,492 INFO     [task.py:395] Building contexts for bbh_fewshot_movie_recommendation on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1138.76it/s]\n",
            "2024-04-04:11:19:41,748 INFO     [task.py:395] Building contexts for bbh_fewshot_tracking_shuffled_objects_seven_objects on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1024.60it/s]\n",
            "2024-04-04:11:19:42,029 INFO     [task.py:395] Building contexts for bbh_fewshot_formal_fallacies on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1008.28it/s]\n",
            "2024-04-04:11:19:42,315 INFO     [task.py:395] Building contexts for bbh_fewshot_dyck_languages on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1189.10it/s]\n",
            "2024-04-04:11:19:42,562 INFO     [task.py:395] Building contexts for bbh_fewshot_hyperbaton on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1181.81it/s]\n",
            "2024-04-04:11:19:42,810 INFO     [task.py:395] Building contexts for bbh_fewshot_disambiguation_qa on rank 0...\n",
            "100% 250/250 [00:00<00:00, 1083.40it/s]\n",
            "2024-04-04:11:19:43,077 INFO     [evaluator.py:379] Running generate_until requests\n",
            "2024-04-04:11:19:43,080 INFO     [model.py:225] Loading 'generate_until' responses from cache 'results_dir_bbh_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6511/6511 [00:00<00:00, 6828.57it/s]\n",
            "2024-04-04:11:19:44,034 INFO     [model.py:249] Cached requests: 0, Requests remaining: 6511\n",
            "Running generate_until requests:   0% 0/6511 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
            "Determined Largest batch size: 4\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 6511/6511 [44:53<00:00,  2.42it/s]\n",
            "hf (pretrained=meta-llama/Llama-2-7b-chat-hf,trust_remote_code=True,do_sample=True), gen_kwargs: (None), limit: None, num_fewshot: 3, batch_size: auto:4\n",
            "|                        Tasks                         |Version|Filter|n-shot|  Metric   |Value |   |Stderr|\n",
            "|------------------------------------------------------|-------|------|-----:|-----------|-----:|---|-----:|\n",
            "|bbh_fewshot                                           |N/A    |none  |     3|exact_match|0.3344|±  |0.0054|\n",
            "| - bbh_fewshot_boolean_expressions                    |      1|none  |     3|exact_match|0.7280|±  |0.0282|\n",
            "| - bbh_fewshot_causal_judgement                       |      1|none  |     3|exact_match|0.5455|±  |0.0365|\n",
            "| - bbh_fewshot_date_understanding                     |      1|none  |     3|exact_match|0.3440|±  |0.0301|\n",
            "| - bbh_fewshot_disambiguation_qa                      |      1|none  |     3|exact_match|0.2680|±  |0.0281|\n",
            "| - bbh_fewshot_dyck_languages                         |      1|none  |     3|exact_match|0.0480|±  |0.0135|\n",
            "| - bbh_fewshot_formal_fallacies                       |      1|none  |     3|exact_match|0.4480|±  |0.0315|\n",
            "| - bbh_fewshot_geometric_shapes                       |      1|none  |     3|exact_match|0.3480|±  |0.0302|\n",
            "| - bbh_fewshot_hyperbaton                             |      1|none  |     3|exact_match|0.5080|±  |0.0317|\n",
            "| - bbh_fewshot_logical_deduction_five_objects         |      1|none  |     3|exact_match|0.2800|±  |0.0285|\n",
            "| - bbh_fewshot_logical_deduction_seven_objects        |      1|none  |     3|exact_match|0.2120|±  |0.0259|\n",
            "| - bbh_fewshot_logical_deduction_three_objects        |      1|none  |     3|exact_match|0.3760|±  |0.0307|\n",
            "| - bbh_fewshot_movie_recommendation                   |      1|none  |     3|exact_match|0.5600|±  |0.0315|\n",
            "| - bbh_fewshot_multistep_arithmetic_two               |      1|none  |     3|exact_match|0.0120|±  |0.0069|\n",
            "| - bbh_fewshot_navigate                               |      1|none  |     3|exact_match|0.4840|±  |0.0317|\n",
            "| - bbh_fewshot_object_counting                        |      1|none  |     3|exact_match|0.3200|±  |0.0296|\n",
            "| - bbh_fewshot_penguins_in_a_table                    |      1|none  |     3|exact_match|0.3014|±  |0.0381|\n",
            "| - bbh_fewshot_reasoning_about_colored_objects        |      1|none  |     3|exact_match|0.2120|±  |0.0259|\n",
            "| - bbh_fewshot_ruin_names                             |      1|none  |     3|exact_match|0.2280|±  |0.0266|\n",
            "| - bbh_fewshot_salient_translation_error_detection    |      1|none  |     3|exact_match|0.2840|±  |0.0286|\n",
            "| - bbh_fewshot_snarks                                 |      1|none  |     3|exact_match|0.5281|±  |0.0375|\n",
            "| - bbh_fewshot_sports_understanding                   |      1|none  |     3|exact_match|0.5960|±  |0.0311|\n",
            "| - bbh_fewshot_temporal_sequences                     |      1|none  |     3|exact_match|0.1120|±  |0.0200|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_five_objects |      1|none  |     3|exact_match|0.1600|±  |0.0232|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_seven_objects|      1|none  |     3|exact_match|0.1280|±  |0.0212|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_three_objects|      1|none  |     3|exact_match|0.3520|±  |0.0303|\n",
            "| - bbh_fewshot_web_of_lies                            |      1|none  |     3|exact_match|0.4880|±  |0.0317|\n",
            "| - bbh_fewshot_word_sorting                           |      1|none  |     3|exact_match|0.2520|±  |0.0275|\n",
            "\n",
            "|  Groups   |Version|Filter|n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----------|-------|------|-----:|-----------|-----:|---|-----:|\n",
            "|bbh_fewshot|N/A    |none  |     3|exact_match|0.3344|±  |0.0054|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                      bbh_fewshot/alias bbh_fewshot\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match 0.33436\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr 0.00544\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_boolean_expressions/alias  - bbh_fewshot_boole...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match 0.728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr 0.0282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_causal_judgement/alias  - bbh_fewshot_causa...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match 0.54545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr 0.03651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_date_understanding/alias  - bbh_fewshot_date_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match 0.344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr 0.0301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_disambiguation_qa/alias  - bbh_fewshot_disam...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match 0.268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr 0.02807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_dyck_languages/alias  - bbh_fewshot_dyck_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match 0.048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr 0.01355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_formal_fallacies/alias  - bbh_fewshot_forma...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match 0.448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr 0.03151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_geometric_shapes/alias  - bbh_fewshot_geome...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match 0.348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr 0.03019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_hyperbaton/alias  - bbh_fewshot_hyper...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match 0.508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_logical_deduction_five_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match 0.28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr 0.02845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_seven_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match 0.212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr 0.0259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_three_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match 0.376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr 0.0307\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_movie_recommendation/alias  - bbh_fewshot_movie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match 0.56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr 0.03146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_multistep_arithmetic_two/alias  - bbh_fewshot_multi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match 0.012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr 0.0069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                             bbh_fewshot_navigate/alias  - bbh_fewshot_navig...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match 0.484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr 0.03167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      bbh_fewshot_object_counting/alias  - bbh_fewshot_objec...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match 0.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr 0.02956\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_penguins_in_a_table/alias  - bbh_fewshot_pengu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match 0.30137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr 0.03811\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_reasoning_about_colored_objects/alias  - bbh_fewshot_reaso...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match 0.212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr 0.0259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_ruin_names/alias  - bbh_fewshot_ruin_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match 0.228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr 0.02659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  bbh_fewshot_salient_translation_error_detection/alias  - bbh_fewshot_salie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match 0.284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr 0.02858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                               bbh_fewshot_snarks/alias  - bbh_fewshot_snark...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match 0.52809\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr 0.03752\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_sports_understanding/alias  - bbh_fewshot_sport...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match 0.596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr 0.0311\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_temporal_sequences/alias  - bbh_fewshot_tempo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match 0.112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr 0.01999\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               bbh_fewshot_tracking_shuffled_objects_five_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match 0.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr 0.02323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_seven_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match 0.128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr 0.02117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_three_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match 0.352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr 0.03027\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                          bbh_fewshot_web_of_lies/alias  - bbh_fewshot_web_o...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match 0.488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_word_sorting/alias  - bbh_fewshot_word_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match 0.252\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr 0.02751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mLlama2-7b-chat-HF-bbh\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/igecq10y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 30 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_111652-igecq10y/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Falcon 7B Instruct"
      ],
      "metadata": {
        "id": "IWBcZ09tlCu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# create directory to store results\n",
        "results_dir = f\"/content/drive/MyDrive/Evaluation/{eval_model}\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "hCElwlNdphuW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MMLU\n",
        "\n",
        "5-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n",
        "\n",
        "revision number is used in the code to ensure an newer version of falcon is not downloaded automatically"
      ],
      "metadata": {
        "id": "M0fZ-nauhfLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True \\\n",
        "    --tasks mmlu_stem,mmlu_social_sciences,mmlu_humanities,mmlu_other \\\n",
        "    --num_fewshot 5 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_falcon_mmlu \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Falcon-7B-instruct-MMLU \\\n",
        "    --use_cache results_dir_falcon_mmlu \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "pVtBcH_Mv46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacec3ef-03d7-4b85-fafd-548cba9b8562"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 12:07:08.704294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 12:07:08.704353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 12:07:08.705814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 12:07:09.939893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_120715-yytitm03\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFalcon-7B-instruct-MMLU\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/yytitm03\u001b[0m\n",
            "2024-04-04:12:07:16,191 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:12:07:22,379 INFO     [__main__.py:335] Selected Tasks: ['mmlu_humanities', 'mmlu_other', 'mmlu_social_sciences', 'mmlu_stem']\n",
            "2024-04-04:12:07:22,381 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:12:07:22,381 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'tiiuae/falcon-7b-instruct', 'trust_remote_code': True}\n",
            "2024-04-04:12:07:22,385 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 3.43MB/s]\n",
            "configuration_falcon.py: 100% 7.16k/7.16k [00:00<00:00, 17.1MB/s]\n",
            "2024-04-04:12:07:23,790 WARNING  [logging.py:329] \n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "modeling_falcon.py: 100% 56.9k/56.9k [00:00<00:00, 24.3MB/s]\n",
            "pytorch_model.bin.index.json: 100% 16.9k/16.9k [00:00<00:00, 39.8MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.95G [00:00<00:55, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 52.4M/9.95G [00:00<00:42, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.95G [00:00<00:39, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/9.95G [00:00<00:36, 271MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.95G [00:00<00:35, 279MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.95G [00:00<00:35, 276MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.95G [00:00<00:35, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/9.95G [00:00<00:35, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.95G [00:01<00:35, 273MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/9.95G [00:01<00:36, 264MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.95G [00:01<00:36, 264MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 377M/9.95G [00:01<00:33, 288MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 409M/9.95G [00:01<00:36, 262MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 440M/9.95G [00:01<00:37, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 472M/9.95G [00:01<00:36, 263MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 503M/9.95G [00:01<00:36, 261MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 535M/9.95G [00:02<00:43, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.95G [00:02<00:42, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 598M/9.95G [00:02<00:44, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 629M/9.95G [00:02<00:45, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 661M/9.95G [00:02<00:46, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 692M/9.95G [00:02<00:42, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 724M/9.95G [00:03<00:42, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 755M/9.95G [00:03<00:41, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 786M/9.95G [00:03<00:41, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 818M/9.95G [00:03<00:40, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 849M/9.95G [00:03<00:39, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.95G [00:03<00:40, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 912M/9.95G [00:03<00:40, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.95G [00:03<00:41, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 975M/9.95G [00:04<00:38, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.95G [00:04<00:36, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.04G/9.95G [00:04<00:36, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.95G [00:04<00:36, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.10G/9.95G [00:04<00:37, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.95G [00:04<00:42, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.95G [00:04<00:41, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.95G [00:05<00:45, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.23G/9.95G [00:05<00:43, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.95G [00:05<00:39, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.95G [00:05<00:39, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.95G [00:05<00:39, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.95G [00:05<00:41, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.95G [00:05<00:39, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.95G [00:06<00:40, 210MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.45G/9.95G [00:06<00:39, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.95G [00:06<00:36, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.51G/9.95G [00:06<00:37, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.95G [00:06<00:38, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.57G/9.95G [00:06<00:36, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.95G [00:06<00:36, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.64G/9.95G [00:07<00:34, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.95G [00:07<00:38, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.70G/9.95G [00:07<00:38, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.95G [00:07<00:37, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.95G [00:07<00:36, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.95G [00:07<00:37, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.95G [00:07<00:36, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.95G [00:08<00:38, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.95G [00:08<00:38, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.95G [00:08<00:38, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/9.95G [00:08<00:38, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.96G/9.95G [00:08<00:40, 196MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.99G/9.95G [00:08<00:38, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.95G [00:08<00:41, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.04G/9.95G [00:09<00:39, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.95G [00:09<00:36, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.95G [00:09<00:35, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.14G/9.95G [00:09<00:34, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.95G [00:09<00:33, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.20G/9.95G [00:09<00:33, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.95G [00:09<00:32, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.95G [00:10<00:33, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.95G [00:10<00:50, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.95G [00:10<01:07, 114MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/9.95G [00:11<01:21, 93.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.95G [00:11<01:16, 98.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.38G/9.95G [00:11<01:21, 93.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.40G/9.95G [00:11<01:37, 77.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.95G [00:11<01:20, 93.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.44G/9.95G [00:12<01:33, 80.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.45G/9.95G [00:12<01:34, 79.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.95G [00:12<01:41, 73.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.95G [00:12<01:27, 85.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.50G/9.95G [00:13<01:34, 79.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.95G [00:13<01:40, 74.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/9.95G [00:13<01:40, 73.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.54G/9.95G [00:13<01:31, 81.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.95G [00:13<01:41, 72.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.95G [00:13<01:42, 72.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.95G [00:14<01:18, 93.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.60G/9.95G [00:14<01:03, 117MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/9.95G [00:14<00:57, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.95G [00:14<00:45, 160MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.95G [00:14<00:42, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/9.95G [00:14<00:36, 196MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.73G/9.95G [00:14<00:36, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.75G/9.95G [00:14<00:36, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.77G/9.95G [00:14<00:37, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.95G [00:15<00:40, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.82G/9.95G [00:15<00:39, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.84G/9.95G [00:15<00:42, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.95G [00:15<00:40, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.88G/9.95G [00:15<00:39, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.95G [00:15<00:38, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.95G [00:15<00:38, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.95G/9.95G [00:15<00:40, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.98G/9.95G [00:16<00:37, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.01G/9.95G [00:16<00:34, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.95G [00:16<00:36, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.06G/9.95G [00:16<00:33, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/9.95G [00:16<00:37, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.10G/9.95G [00:16<00:40, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.95G [00:16<00:40, 169MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.95G [00:17<00:39, 174MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.95G [00:17<00:33, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.20G/9.95G [00:17<00:34, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.95G [00:17<00:34, 195MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.95G [00:17<00:33, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.95G [00:17<00:33, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.95G [00:17<00:32, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.32G/9.95G [00:17<00:32, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.36G/9.95G [00:18<00:32, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.39G/9.95G [00:18<00:29, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.42G/9.95G [00:18<00:30, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.95G [00:18<00:29, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.48G/9.95G [00:18<00:29, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.51G/9.95G [00:18<00:30, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.95G [00:18<00:28, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.58G/9.95G [00:19<00:27, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.61G/9.95G [00:19<00:27, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.64G/9.95G [00:19<00:27, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.67G/9.95G [00:19<00:27, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.70G/9.95G [00:19<00:29, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.73G/9.95G [00:19<00:31, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.76G/9.95G [00:19<00:31, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.79G/9.95G [00:20<00:31, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/9.95G [00:20<00:30, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.85G/9.95G [00:20<00:29, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.88G/9.95G [00:20<00:29, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.90G/9.95G [00:20<00:29, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.95G [00:20<00:29, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.95G [00:20<00:28, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.95G [00:21<00:31, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.95G [00:21<00:28, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.95G [00:21<00:31, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.95G [00:21<00:28, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.95G [00:21<00:26, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.95G [00:21<00:25, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.17G/9.95G [00:21<00:24, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.20G/9.95G [00:22<00:24, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.24G/9.95G [00:22<00:23, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.27G/9.95G [00:22<00:23, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.30G/9.95G [00:22<00:23, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.33G/9.95G [00:22<00:22, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.36G/9.95G [00:22<00:21, 260MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.39G/9.95G [00:22<00:21, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.42G/9.95G [00:22<00:24, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.46G/9.95G [00:23<00:22, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.49G/9.95G [00:23<00:22, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.52G/9.95G [00:23<00:26, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.55G/9.95G [00:23<00:29, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.57G/9.95G [00:23<00:28, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.95G [00:23<00:28, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/9.95G [00:23<00:26, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.95G [00:24<00:24, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.69G/9.95G [00:24<00:45, 115MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.71G/9.95G [00:24<00:57, 91.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.73G/9.95G [00:25<00:56, 92.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.75G/9.95G [00:25<01:04, 80.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.95G [00:25<01:02, 83.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.78G/9.95G [00:25<00:55, 92.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.95G [00:26<01:07, 76.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.81G/9.95G [00:26<01:05, 78.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.95G [00:26<01:04, 79.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.83G/9.95G [00:26<01:08, 74.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.84G/9.95G [00:26<01:07, 75.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.95G [00:26<01:15, 67.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.95G [00:27<01:12, 70.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.88G/9.95G [00:27<01:14, 68.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.89G/9.95G [00:27<01:12, 69.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.95G [00:27<01:13, 68.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.93G/9.95G [00:27<00:43, 116MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.95G [00:27<00:36, 137MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.97G/9.95G [00:27<00:33, 147MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.95G [00:27<00:30, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.02G/9.95G [00:28<00:27, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.95G [00:28<00:24, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.95G [00:28<00:22, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.95G [00:28<00:21, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.15G/9.95G [00:28<00:20, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.95G [00:28<00:21, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.95G [00:28<00:19, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.95G [00:29<00:19, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.95G [00:29<00:18, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.32G/9.95G [00:29<00:17, 272MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.35G/9.95G [00:29<00:17, 267MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.38G/9.95G [00:29<00:17, 266MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/9.95G [00:29<00:18, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.44G/9.95G [00:29<00:17, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.95G [00:30<00:25, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/9.95G [00:30<00:21, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.95G [00:30<00:20, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.95G [00:30<00:18, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.95G [00:30<00:18, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.65G/9.95G [00:30<00:16, 265MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.68G/9.95G [00:30<00:16, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.71G/9.95G [00:30<00:17, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.75G/9.95G [00:31<00:17, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.78G/9.95G [00:31<00:17, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.81G/9.95G [00:31<00:16, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.84G/9.95G [00:31<00:16, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.87G/9.95G [00:31<00:16, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.90G/9.95G [00:31<00:16, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.93G/9.95G [00:31<00:17, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.97G/9.95G [00:32<00:18, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.00G/9.95G [00:32<00:17, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.03G/9.95G [00:32<00:16, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.06G/9.95G [00:32<00:15, 256MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.09G/9.95G [00:32<00:14, 263MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.12G/9.95G [00:32<00:14, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.95G [00:32<00:14, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.19G/9.95G [00:32<00:14, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.22G/9.95G [00:33<00:15, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.95G [00:33<00:14, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.28G/9.95G [00:33<00:15, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.95G [00:33<00:14, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.34G/9.95G [00:33<00:14, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.95G [00:33<00:14, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.41G/9.95G [00:33<00:14, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.95G [00:33<00:15, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.95G [00:34<00:14, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.50G/9.95G [00:34<00:14, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.53G/9.95G [00:34<00:14, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.56G/9.95G [00:34<00:18, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.59G/9.95G [00:34<00:18, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.62G/9.95G [00:34<00:17, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.65G/9.95G [00:35<00:15, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.68G/9.95G [00:35<00:15, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.71G/9.95G [00:35<00:16, 195MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.74G/9.95G [00:35<00:16, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.76G/9.95G [00:35<00:17, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.95G [00:35<00:15, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.83G/9.95G [00:35<00:15, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.85G/9.95G [00:36<00:16, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.87G/9.95G [00:36<00:16, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.90G/9.95G [00:36<00:14, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.93G/9.95G [00:36<00:15, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.96G/9.95G [00:36<00:13, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.99G/9.95G [00:36<00:13, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.03G/9.95G [00:36<00:13, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.06G/9.95G [00:36<00:13, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.09G/9.95G [00:37<00:13, 213MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.13G/9.95G [00:37<00:12, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.95G [00:37<00:12, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.19G/9.95G [00:37<00:15, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.21G/9.95G [00:38<00:22, 124MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.95G [00:38<00:27, 99.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.95G [00:38<00:35, 75.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.27G/9.95G [00:39<00:40, 66.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.28G/9.95G [00:39<00:41, 65.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.95G [00:39<00:35, 74.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.31G/9.95G [00:39<00:35, 75.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.32G/9.95G [00:39<00:34, 75.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.34G/9.95G [00:40<00:31, 84.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.95G [00:40<00:32, 80.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.95G [00:40<00:38, 67.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.37G/9.95G [00:40<00:36, 70.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.95G [00:40<00:37, 69.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.40G/9.95G [00:40<00:32, 78.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.95G [00:41<00:35, 70.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.43G/9.95G [00:41<00:27, 92.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.47G/9.95G [00:41<00:18, 132MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.95G [00:41<00:16, 147MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.52G/9.95G [00:41<00:14, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.55G/9.95G [00:41<00:12, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.95G [00:41<00:12, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.60G/9.95G [00:41<00:10, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.63G/9.95G [00:42<00:11, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.65G/9.95G [00:42<00:12, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.69G/9.95G [00:42<00:11, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.72G/9.95G [00:42<00:10, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.75G/9.95G [00:42<00:09, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.78G/9.95G [00:42<00:11, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.81G/9.95G [00:43<00:11, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.95G [00:43<00:11, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.85G/9.95G [00:43<00:11, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.89G/9.95G [00:43<00:10, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.91G/9.95G [00:43<00:10, 188MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.94G/9.95G [00:43<00:09, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.97G/9.95G [00:43<00:10, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.95G [00:43<00:10, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.02G/9.95G [00:44<00:08, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.05G/9.95G [00:44<00:09, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.08G/9.95G [00:44<00:09, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.95G [00:44<00:09, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.13G/9.95G [00:44<00:09, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.95G [00:44<00:09, 199MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.17G/9.95G [00:44<00:09, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.19G/9.95G [00:45<00:09, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.21G/9.95G [00:45<00:09, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.95G [00:45<00:09, 188MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.95G [00:45<00:08, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.95G [00:45<00:08, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.32G/9.95G [00:45<00:10, 150MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.35G/9.95G [00:45<00:09, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.38G/9.95G [00:46<00:08, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.41G/9.95G [00:46<00:07, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.44G/9.95G [00:46<00:06, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.95G [00:46<00:06, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.50G/9.95G [00:46<00:06, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.95G [00:46<00:06, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.95G [00:46<00:08, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.95G [00:47<00:06, 196MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.63G/9.95G [00:47<00:06, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.66G/9.95G [00:47<00:06, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.69G/9.95G [00:47<00:05, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.72G/9.95G [00:47<00:05, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.95G [00:47<00:05, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.79G/9.95G [00:47<00:04, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.82G/9.95G [00:48<00:04, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.85G/9.95G [00:48<00:04, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.95G [00:48<00:04, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.91G/9.95G [00:48<00:04, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.94G/9.95G [00:48<00:04, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.98G/9.95G [00:48<00:04, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.01G/9.95G [00:48<00:04, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.95G [00:49<00:04, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.07G/9.95G [00:49<00:04, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.10G/9.95G [00:49<00:04, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.12G/9.95G [00:49<00:04, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.95G [00:49<00:04, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.95G [00:49<00:03, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.21G/9.95G [00:49<00:03, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.95G [00:50<00:03, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.95G [00:50<00:03, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.95G [00:50<00:02, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.95G [00:50<00:02, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.36G/9.95G [00:50<00:02, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.40G/9.95G [00:50<00:02, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.43G/9.95G [00:50<00:02, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.46G/9.95G [00:50<00:02, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.49G/9.95G [00:51<00:01, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.52G/9.95G [00:51<00:02, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.54G/9.95G [00:51<00:03, 120MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.95G [00:51<00:02, 133MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.58G/9.95G [00:52<00:03, 100MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.60G/9.95G [00:52<00:03, 98.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.63G/9.95G [00:52<00:03, 93.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.65G/9.95G [00:52<00:03, 93.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.67G/9.95G [00:53<00:03, 76.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.68G/9.95G [00:53<00:04, 59.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.69G/9.95G [00:53<00:04, 58.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.70G/9.95G [00:54<00:04, 56.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.71G/9.95G [00:54<00:04, 55.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.72G/9.95G [00:54<00:03, 62.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.75G/9.95G [00:54<00:01, 102MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.95G [00:54<00:01, 118MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.80G/9.95G [00:54<00:00, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.83G/9.95G [00:54<00:00, 163MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.85G/9.95G [00:54<00:00, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.88G/9.95G [00:55<00:00, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.90G/9.95G [00:55<00:00, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.95G [00:55<00:00, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.95G/9.95G [00:55<00:00, 179MB/s]\n",
            "Downloading shards:  50% 1/2 [00:55<00:55, 56.00s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/4.48G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   0% 10.5M/4.48G [00:00<00:55, 79.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 41.9M/4.48G [00:00<00:22, 199MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 83.9M/4.48G [00:00<00:16, 259MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 115M/4.48G [00:00<00:17, 255MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/4.48G [00:00<00:18, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 189M/4.48G [00:00<00:17, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 220M/4.48G [00:00<00:17, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 252M/4.48G [00:01<00:20, 211MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 283M/4.48G [00:01<00:22, 190MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 325M/4.48G [00:01<00:18, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 357M/4.48G [00:01<00:18, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 388M/4.48G [00:01<00:19, 212MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 419M/4.48G [00:01<00:18, 218MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 451M/4.48G [00:02<00:18, 218MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 493M/4.48G [00:02<00:15, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 524M/4.48G [00:02<00:15, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 566M/4.48G [00:02<00:13, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 598M/4.48G [00:02<00:13, 290MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 629M/4.48G [00:02<00:14, 259MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 661M/4.48G [00:02<00:16, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 692M/4.48G [00:02<00:14, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 724M/4.48G [00:03<00:15, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 755M/4.48G [00:03<00:14, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 786M/4.48G [00:03<00:16, 225MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 818M/4.48G [00:03<00:15, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 860M/4.48G [00:03<00:13, 262MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 891M/4.48G [00:03<00:16, 211MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 933M/4.48G [00:03<00:14, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 975M/4.48G [00:04<00:13, 267MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 1.02G/4.48G [00:04<00:13, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 1.06G/4.48G [00:04<00:11, 291MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 1.10G/4.48G [00:04<00:10, 314MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 1.14G/4.48G [00:04<00:10, 305MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 1.18G/4.48G [00:04<00:12, 266MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 1.23G/4.48G [00:04<00:11, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 1.27G/4.48G [00:05<00:10, 317MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.31G/4.48G [00:05<00:09, 333MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.35G/4.48G [00:05<00:09, 339MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.39G/4.48G [00:05<00:09, 319MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.44G/4.48G [00:05<00:10, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.48G/4.48G [00:05<00:09, 311MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.52G/4.48G [00:05<00:09, 319MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.56G/4.48G [00:05<00:09, 317MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.60G/4.48G [00:06<00:08, 330MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.65G/4.48G [00:06<00:08, 325MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.69G/4.48G [00:06<00:09, 310MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.73G/4.48G [00:06<00:08, 331MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.77G/4.48G [00:06<00:07, 344MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.81G/4.48G [00:06<00:08, 324MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.86G/4.48G [00:06<00:08, 298MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.89G/4.48G [00:06<00:08, 300MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.92G/4.48G [00:07<00:08, 303MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.96G/4.48G [00:07<00:07, 328MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 2.00G/4.48G [00:07<00:08, 280MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 2.03G/4.48G [00:07<00:11, 214MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 2.08G/4.48G [00:07<00:09, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 2.11G/4.48G [00:07<00:09, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 2.14G/4.48G [00:07<00:09, 258MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 2.17G/4.48G [00:08<00:08, 257MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 2.20G/4.48G [00:08<00:10, 210MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 2.24G/4.48G [00:08<00:09, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 2.29G/4.48G [00:08<00:08, 274MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 2.32G/4.48G [00:08<00:07, 271MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 2.36G/4.48G [00:08<00:07, 300MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 2.40G/4.48G [00:08<00:07, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 2.43G/4.48G [00:09<00:07, 273MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 2.47G/4.48G [00:09<00:06, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 2.51G/4.48G [00:09<00:10, 180MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.55G/4.48G [00:09<00:08, 215MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.59G/4.48G [00:09<00:07, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.62G/4.48G [00:09<00:08, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.65G/4.48G [00:10<00:08, 218MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.68G/4.48G [00:10<00:08, 220MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.72G/4.48G [00:10<00:07, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.75G/4.48G [00:10<00:08, 216MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.79G/4.48G [00:10<00:07, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.82G/4.48G [00:10<00:06, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.87G/4.48G [00:10<00:05, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.92G/4.48G [00:11<00:05, 302MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.96G/4.48G [00:11<00:04, 309MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 3.00G/4.48G [00:11<00:05, 287MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 3.04G/4.48G [00:11<00:04, 296MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 3.07G/4.48G [00:11<00:04, 285MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 3.10G/4.48G [00:11<00:04, 290MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 3.15G/4.48G [00:11<00:04, 321MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 3.20G/4.48G [00:11<00:03, 336MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 3.24G/4.48G [00:12<00:03, 355MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 3.29G/4.48G [00:12<00:03, 373MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 3.33G/4.48G [00:12<00:03, 375MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 3.38G/4.48G [00:12<00:02, 378MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 3.42G/4.48G [00:12<00:03, 347MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 3.46G/4.48G [00:12<00:03, 331MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 3.50G/4.48G [00:12<00:03, 321MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 3.54G/4.48G [00:13<00:03, 299MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 3.59G/4.48G [00:13<00:02, 323MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 3.63G/4.48G [00:13<00:02, 332MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 3.67G/4.48G [00:13<00:02, 334MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 3.72G/4.48G [00:13<00:02, 362MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 3.76G/4.48G [00:13<00:01, 364MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 3.81G/4.48G [00:13<00:02, 324MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.85G/4.48G [00:13<00:02, 296MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.89G/4.48G [00:14<00:01, 306MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.93G/4.48G [00:14<00:01, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.96G/4.48G [00:14<00:01, 295MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 4.01G/4.48G [00:14<00:01, 292MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 4.05G/4.48G [00:14<00:01, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 4.08G/4.48G [00:14<00:01, 299MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 4.12G/4.48G [00:14<00:01, 329MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 4.16G/4.48G [00:14<00:00, 333MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 4.22G/4.48G [00:15<00:00, 364MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 4.26G/4.48G [00:15<00:00, 365MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 4.30G/4.48G [00:15<00:00, 348MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 4.34G/4.48G [00:15<00:00, 349MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 4.38G/4.48G [00:15<00:00, 348MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 4.42G/4.48G [00:15<00:00, 351MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 4.48G/4.48G [00:15<00:00, 283MB/s]\n",
            "Downloading shards: 100% 2/2 [01:13<00:00, 36.56s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.36s/it]\n",
            "generation_config.json: 100% 117/117 [00:00<00:00, 431kB/s]\n",
            "tokenizer_config.json: 100% 287/287 [00:00<00:00, 957kB/s]\n",
            "tokenizer.json: 100% 2.73M/2.73M [00:00<00:00, 5.54MB/s]\n",
            "special_tokens_map.json: 100% 281/281 [00:00<00:00, 1.02MB/s]\n",
            "2024-04-04:12:08:47,957 INFO     [evaluator.py:195] Using cache at results_dir_falcon_mmlu_rank0.db\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-04-04:12:12:29,448 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
            "2024-04-04:12:12:29,448 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
            "2024-04-04:12:12:29,449 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
            "2024-04-04:12:12:29,450 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
            "2024-04-04:12:12:29,451 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_management from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_virology from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
            "2024-04-04:12:12:29,452 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
            "2024-04-04:12:12:29,453 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
            "2024-04-04:12:12:29,459 INFO     [task.py:395] Building contexts for mmlu_conceptual_physics on rank 0...\n",
            "100% 235/235 [00:03<00:00, 67.06it/s]\n",
            "2024-04-04:12:12:33,052 INFO     [task.py:395] Building contexts for mmlu_high_school_biology on rank 0...\n",
            "100% 310/310 [00:04<00:00, 66.86it/s]\n",
            "2024-04-04:12:12:37,808 INFO     [task.py:395] Building contexts for mmlu_anatomy on rank 0...\n",
            "100% 135/135 [00:02<00:00, 66.46it/s]\n",
            "2024-04-04:12:12:39,890 INFO     [task.py:395] Building contexts for mmlu_college_chemistry on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.92it/s]\n",
            "2024-04-04:12:12:41,423 INFO     [task.py:395] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
            "100% 378/378 [00:05<00:00, 66.56it/s]\n",
            "2024-04-04:12:12:47,244 INFO     [task.py:395] Building contexts for mmlu_college_biology on rank 0...\n",
            "100% 144/144 [00:02<00:00, 67.17it/s]\n",
            "2024-04-04:12:12:49,442 INFO     [task.py:395] Building contexts for mmlu_electrical_engineering on rank 0...\n",
            "100% 145/145 [00:02<00:00, 66.99it/s]\n",
            "2024-04-04:12:12:51,661 INFO     [task.py:395] Building contexts for mmlu_machine_learning on rank 0...\n",
            "100% 112/112 [00:01<00:00, 66.80it/s]\n",
            "2024-04-04:12:12:53,380 INFO     [task.py:395] Building contexts for mmlu_high_school_statistics on rank 0...\n",
            "100% 216/216 [00:03<00:00, 67.48it/s]\n",
            "2024-04-04:12:12:56,663 INFO     [task.py:395] Building contexts for mmlu_computer_security on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.94it/s]\n",
            "2024-04-04:12:12:58,195 INFO     [task.py:395] Building contexts for mmlu_abstract_algebra on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.96it/s]\n",
            "2024-04-04:12:12:59,726 INFO     [task.py:395] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.54it/s]\n",
            "2024-04-04:12:13:01,267 INFO     [task.py:395] Building contexts for mmlu_college_physics on rank 0...\n",
            "100% 102/102 [00:01<00:00, 66.66it/s]\n",
            "2024-04-04:12:13:02,837 INFO     [task.py:395] Building contexts for mmlu_college_computer_science on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.74it/s]\n",
            "2024-04-04:12:13:04,374 INFO     [task.py:395] Building contexts for mmlu_astronomy on rank 0...\n",
            "100% 152/152 [00:02<00:00, 66.93it/s]\n",
            "2024-04-04:12:13:06,702 INFO     [task.py:395] Building contexts for mmlu_high_school_physics on rank 0...\n",
            "100% 151/151 [00:02<00:00, 66.64it/s]\n",
            "2024-04-04:12:13:09,027 INFO     [task.py:395] Building contexts for mmlu_college_mathematics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.98it/s]\n",
            "2024-04-04:12:13:10,558 INFO     [task.py:395] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
            "100% 270/270 [00:04<00:00, 66.64it/s]\n",
            "2024-04-04:12:13:14,711 INFO     [task.py:395] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
            "100% 203/203 [00:03<00:00, 66.82it/s]\n",
            "2024-04-04:12:13:17,826 INFO     [task.py:395] Building contexts for mmlu_human_sexuality on rank 0...\n",
            "100% 131/131 [00:01<00:00, 67.01it/s]\n",
            "2024-04-04:12:13:19,830 INFO     [task.py:395] Building contexts for mmlu_high_school_geography on rank 0...\n",
            "100% 198/198 [00:02<00:00, 67.46it/s]\n",
            "2024-04-04:12:13:22,840 INFO     [task.py:395] Building contexts for mmlu_econometrics on rank 0...\n",
            "100% 114/114 [00:01<00:00, 67.27it/s]\n",
            "2024-04-04:12:13:24,578 INFO     [task.py:395] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
            "100% 390/390 [00:05<00:00, 67.22it/s]\n",
            "2024-04-04:12:13:30,530 INFO     [task.py:395] Building contexts for mmlu_professional_psychology on rank 0...\n",
            "100% 612/612 [00:09<00:00, 66.80it/s]\n",
            "2024-04-04:12:13:39,923 INFO     [task.py:395] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
            "100% 193/193 [00:02<00:00, 67.41it/s]\n",
            "2024-04-04:12:13:42,858 INFO     [task.py:395] Building contexts for mmlu_high_school_psychology on rank 0...\n",
            "100% 545/545 [00:08<00:00, 67.05it/s]\n",
            "2024-04-04:12:13:51,193 INFO     [task.py:395] Building contexts for mmlu_security_studies on rank 0...\n",
            "100% 245/245 [00:03<00:00, 67.17it/s]\n",
            "2024-04-04:12:13:54,935 INFO     [task.py:395] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
            "100% 100/100 [00:01<00:00, 67.26it/s]\n",
            "2024-04-04:12:13:56,460 INFO     [task.py:395] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
            "100% 238/238 [00:03<00:00, 62.30it/s]\n",
            "2024-04-04:12:14:00,371 INFO     [task.py:395] Building contexts for mmlu_public_relations on rank 0...\n",
            "100% 110/110 [00:01<00:00, 66.67it/s]\n",
            "2024-04-04:12:14:02,063 INFO     [task.py:395] Building contexts for mmlu_sociology on rank 0...\n",
            "100% 201/201 [00:03<00:00, 66.98it/s]\n",
            "2024-04-04:12:14:05,140 INFO     [task.py:395] Building contexts for mmlu_professional_accounting on rank 0...\n",
            "100% 282/282 [00:04<00:00, 66.08it/s]\n",
            "2024-04-04:12:14:09,516 INFO     [task.py:395] Building contexts for mmlu_medical_genetics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.94it/s]\n",
            "2024-04-04:12:14:11,048 INFO     [task.py:395] Building contexts for mmlu_college_medicine on rank 0...\n",
            "100% 173/173 [00:02<00:00, 66.45it/s]\n",
            "2024-04-04:12:14:13,718 INFO     [task.py:395] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
            "100% 265/265 [00:03<00:00, 66.98it/s]\n",
            "2024-04-04:12:14:17,773 INFO     [task.py:395] Building contexts for mmlu_management on rank 0...\n",
            "100% 103/103 [00:01<00:00, 67.23it/s]\n",
            "2024-04-04:12:14:19,344 INFO     [task.py:395] Building contexts for mmlu_human_aging on rank 0...\n",
            "100% 223/223 [00:03<00:00, 66.98it/s]\n",
            "2024-04-04:12:14:22,757 INFO     [task.py:395] Building contexts for mmlu_business_ethics on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.91it/s]\n",
            "2024-04-04:12:14:24,290 INFO     [task.py:395] Building contexts for mmlu_global_facts on rank 0...\n",
            "100% 100/100 [00:01<00:00, 66.70it/s]\n",
            "2024-04-04:12:14:25,827 INFO     [task.py:395] Building contexts for mmlu_miscellaneous on rank 0...\n",
            "100% 783/783 [00:11<00:00, 67.23it/s]\n",
            "2024-04-04:12:14:37,769 INFO     [task.py:395] Building contexts for mmlu_nutrition on rank 0...\n",
            "100% 306/306 [00:04<00:00, 66.66it/s]\n",
            "2024-04-04:12:14:42,473 INFO     [task.py:395] Building contexts for mmlu_professional_medicine on rank 0...\n",
            "100% 272/272 [00:04<00:00, 67.00it/s]\n",
            "2024-04-04:12:14:46,638 INFO     [task.py:395] Building contexts for mmlu_marketing on rank 0...\n",
            "100% 234/234 [00:03<00:00, 67.07it/s]\n",
            "2024-04-04:12:14:50,214 INFO     [task.py:395] Building contexts for mmlu_virology on rank 0...\n",
            "100% 166/166 [00:02<00:00, 67.51it/s]\n",
            "2024-04-04:12:14:52,736 INFO     [task.py:395] Building contexts for mmlu_high_school_world_history on rank 0...\n",
            "100% 237/237 [00:03<00:00, 66.75it/s]\n",
            "2024-04-04:12:14:56,381 INFO     [task.py:395] Building contexts for mmlu_international_law on rank 0...\n",
            "100% 121/121 [00:01<00:00, 66.96it/s]\n",
            "2024-04-04:12:14:58,235 INFO     [task.py:395] Building contexts for mmlu_high_school_european_history on rank 0...\n",
            "100% 165/165 [00:02<00:00, 66.85it/s]\n",
            "2024-04-04:12:15:00,776 INFO     [task.py:395] Building contexts for mmlu_moral_disputes on rank 0...\n",
            "100% 346/346 [00:05<00:00, 66.72it/s]\n",
            "2024-04-04:12:15:06,092 INFO     [task.py:395] Building contexts for mmlu_prehistory on rank 0...\n",
            "100% 324/324 [00:04<00:00, 67.20it/s]\n",
            "2024-04-04:12:15:11,035 INFO     [task.py:395] Building contexts for mmlu_high_school_us_history on rank 0...\n",
            "100% 204/204 [00:03<00:00, 67.23it/s]\n",
            "2024-04-04:12:15:14,154 INFO     [task.py:395] Building contexts for mmlu_professional_law on rank 0...\n",
            "100% 1534/1534 [00:22<00:00, 66.95it/s]\n",
            "2024-04-04:12:15:37,691 INFO     [task.py:395] Building contexts for mmlu_moral_scenarios on rank 0...\n",
            "100% 895/895 [00:13<00:00, 65.64it/s]\n",
            "2024-04-04:12:15:51,668 INFO     [task.py:395] Building contexts for mmlu_world_religions on rank 0...\n",
            "100% 171/171 [00:02<00:00, 67.55it/s]\n",
            "2024-04-04:12:15:54,264 INFO     [task.py:395] Building contexts for mmlu_jurisprudence on rank 0...\n",
            "100% 108/108 [00:01<00:00, 67.28it/s]\n",
            "2024-04-04:12:15:55,910 INFO     [task.py:395] Building contexts for mmlu_logical_fallacies on rank 0...\n",
            "100% 163/163 [00:02<00:00, 67.40it/s]\n",
            "2024-04-04:12:15:58,390 INFO     [task.py:395] Building contexts for mmlu_formal_logic on rank 0...\n",
            "100% 126/126 [00:01<00:00, 67.62it/s]\n",
            "2024-04-04:12:16:00,301 INFO     [task.py:395] Building contexts for mmlu_philosophy on rank 0...\n",
            "100% 311/311 [00:04<00:00, 67.24it/s]\n",
            "2024-04-04:12:16:05,044 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:12:16:05,067 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_falcon_mmlu_rank0.db' where possible...\n",
            "Checking cached requests: 100% 56168/56168 [00:08<00:00, 6244.95it/s]\n",
            "2024-04-04:12:16:14,061 INFO     [model.py:249] Cached requests: 0, Requests remaining: 56168\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2745 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Running loglikelihood requests:   0% 0/56168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 8\n",
            "Running loglikelihood requests:  25% 13953/56168 [11:04<12:09, 57.84it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Running loglikelihood requests:  25% 13984/56168 [11:15<12:09, 57.84it/s]Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 56168/56168 [18:57<00:00, 49.37it/s] \n",
            "{\n",
            "  \"results\": {\n",
            "    \"mmlu_stem\": {\n",
            "      \"alias\": \"stem\",\n",
            "      \"acc,none\": 0.2327941642879797,\n",
            "      \"acc_stderr,none\": 0.007506916152472624\n",
            "    },\n",
            "    \"mmlu_abstract_algebra\": {\n",
            "      \"alias\": \" - abstract_algebra\",\n",
            "      \"acc,none\": 0.29,\n",
            "      \"acc_stderr,none\": 0.04560480215720683\n",
            "    },\n",
            "    \"mmlu_anatomy\": {\n",
            "      \"alias\": \" - anatomy\",\n",
            "      \"acc,none\": 0.2,\n",
            "      \"acc_stderr,none\": 0.034554737023254366\n",
            "    },\n",
            "    \"mmlu_astronomy\": {\n",
            "      \"alias\": \" - astronomy\",\n",
            "      \"acc,none\": 0.24342105263157895,\n",
            "      \"acc_stderr,none\": 0.034923496688842384\n",
            "    },\n",
            "    \"mmlu_college_biology\": {\n",
            "      \"alias\": \" - college_biology\",\n",
            "      \"acc,none\": 0.25,\n",
            "      \"acc_stderr,none\": 0.03621034121889507\n",
            "    },\n",
            "    \"mmlu_college_chemistry\": {\n",
            "      \"alias\": \" - college_chemistry\",\n",
            "      \"acc,none\": 0.2,\n",
            "      \"acc_stderr,none\": 0.04020151261036845\n",
            "    },\n",
            "    \"mmlu_college_computer_science\": {\n",
            "      \"alias\": \" - college_computer_science\",\n",
            "      \"acc,none\": 0.29,\n",
            "      \"acc_stderr,none\": 0.04560480215720684\n",
            "    },\n",
            "    \"mmlu_college_mathematics\": {\n",
            "      \"alias\": \" - college_mathematics\",\n",
            "      \"acc,none\": 0.26,\n",
            "      \"acc_stderr,none\": 0.04408440022768077\n",
            "    },\n",
            "    \"mmlu_college_physics\": {\n",
            "      \"alias\": \" - college_physics\",\n",
            "      \"acc,none\": 0.20588235294117646,\n",
            "      \"acc_stderr,none\": 0.040233822736177476\n",
            "    },\n",
            "    \"mmlu_computer_security\": {\n",
            "      \"alias\": \" - computer_security\",\n",
            "      \"acc,none\": 0.32,\n",
            "      \"acc_stderr,none\": 0.046882617226215034\n",
            "    },\n",
            "    \"mmlu_conceptual_physics\": {\n",
            "      \"alias\": \" - conceptual_physics\",\n",
            "      \"acc,none\": 0.2978723404255319,\n",
            "      \"acc_stderr,none\": 0.02989614568209546\n",
            "    },\n",
            "    \"mmlu_electrical_engineering\": {\n",
            "      \"alias\": \" - electrical_engineering\",\n",
            "      \"acc,none\": 0.27586206896551724,\n",
            "      \"acc_stderr,none\": 0.037245636197746325\n",
            "    },\n",
            "    \"mmlu_elementary_mathematics\": {\n",
            "      \"alias\": \" - elementary_mathematics\",\n",
            "      \"acc,none\": 0.2275132275132275,\n",
            "      \"acc_stderr,none\": 0.02159126940782378\n",
            "    },\n",
            "    \"mmlu_high_school_biology\": {\n",
            "      \"alias\": \" - high_school_biology\",\n",
            "      \"acc,none\": 0.2064516129032258,\n",
            "      \"acc_stderr,none\": 0.02302589961718872\n",
            "    },\n",
            "    \"mmlu_high_school_chemistry\": {\n",
            "      \"alias\": \" - high_school_chemistry\",\n",
            "      \"acc,none\": 0.19704433497536947,\n",
            "      \"acc_stderr,none\": 0.027986724666736212\n",
            "    },\n",
            "    \"mmlu_high_school_computer_science\": {\n",
            "      \"alias\": \" - high_school_computer_science\",\n",
            "      \"acc,none\": 0.24,\n",
            "      \"acc_stderr,none\": 0.042923469599092816\n",
            "    },\n",
            "    \"mmlu_high_school_mathematics\": {\n",
            "      \"alias\": \" - high_school_mathematics\",\n",
            "      \"acc,none\": 0.18518518518518517,\n",
            "      \"acc_stderr,none\": 0.02368407558532269\n",
            "    },\n",
            "    \"mmlu_high_school_physics\": {\n",
            "      \"alias\": \" - high_school_physics\",\n",
            "      \"acc,none\": 0.2119205298013245,\n",
            "      \"acc_stderr,none\": 0.03336767086567977\n",
            "    },\n",
            "    \"mmlu_high_school_statistics\": {\n",
            "      \"alias\": \" - high_school_statistics\",\n",
            "      \"acc,none\": 0.16203703703703703,\n",
            "      \"acc_stderr,none\": 0.02513045365226846\n",
            "    },\n",
            "    \"mmlu_machine_learning\": {\n",
            "      \"alias\": \" - machine_learning\",\n",
            "      \"acc,none\": 0.32142857142857145,\n",
            "      \"acc_stderr,none\": 0.04432804055291519\n",
            "    },\n",
            "    \"mmlu_social_sciences\": {\n",
            "      \"alias\": \"social_sciences\",\n",
            "      \"acc,none\": 0.24211894702632433,\n",
            "      \"acc_stderr,none\": 0.007719587269967782\n",
            "    },\n",
            "    \"mmlu_econometrics\": {\n",
            "      \"alias\": \" - econometrics\",\n",
            "      \"acc,none\": 0.2719298245614035,\n",
            "      \"acc_stderr,none\": 0.041857744240220575\n",
            "    },\n",
            "    \"mmlu_high_school_geography\": {\n",
            "      \"alias\": \" - high_school_geography\",\n",
            "      \"acc,none\": 0.2474747474747475,\n",
            "      \"acc_stderr,none\": 0.030746300742124495\n",
            "    },\n",
            "    \"mmlu_high_school_government_and_politics\": {\n",
            "      \"alias\": \" - high_school_government_and_politics\",\n",
            "      \"acc,none\": 0.23834196891191708,\n",
            "      \"acc_stderr,none\": 0.030748905363909902\n",
            "    },\n",
            "    \"mmlu_high_school_macroeconomics\": {\n",
            "      \"alias\": \" - high_school_macroeconomics\",\n",
            "      \"acc,none\": 0.24358974358974358,\n",
            "      \"acc_stderr,none\": 0.02176373368417391\n",
            "    },\n",
            "    \"mmlu_high_school_microeconomics\": {\n",
            "      \"alias\": \" - high_school_microeconomics\",\n",
            "      \"acc,none\": 0.23109243697478993,\n",
            "      \"acc_stderr,none\": 0.027381406927868963\n",
            "    },\n",
            "    \"mmlu_high_school_psychology\": {\n",
            "      \"alias\": \" - high_school_psychology\",\n",
            "      \"acc,none\": 0.24954128440366974,\n",
            "      \"acc_stderr,none\": 0.018553897629501617\n",
            "    },\n",
            "    \"mmlu_human_sexuality\": {\n",
            "      \"alias\": \" - human_sexuality\",\n",
            "      \"acc,none\": 0.2748091603053435,\n",
            "      \"acc_stderr,none\": 0.03915345408847835\n",
            "    },\n",
            "    \"mmlu_professional_psychology\": {\n",
            "      \"alias\": \" - professional_psychology\",\n",
            "      \"acc,none\": 0.2238562091503268,\n",
            "      \"acc_stderr,none\": 0.016863008585416613\n",
            "    },\n",
            "    \"mmlu_public_relations\": {\n",
            "      \"alias\": \" - public_relations\",\n",
            "      \"acc,none\": 0.3181818181818182,\n",
            "      \"acc_stderr,none\": 0.04461272175910508\n",
            "    },\n",
            "    \"mmlu_security_studies\": {\n",
            "      \"alias\": \" - security_studies\",\n",
            "      \"acc,none\": 0.19183673469387755,\n",
            "      \"acc_stderr,none\": 0.025206963154225406\n",
            "    },\n",
            "    \"mmlu_sociology\": {\n",
            "      \"alias\": \" - sociology\",\n",
            "      \"acc,none\": 0.22388059701492538,\n",
            "      \"acc_stderr,none\": 0.029475250236017193\n",
            "    },\n",
            "    \"mmlu_us_foreign_policy\": {\n",
            "      \"alias\": \" - us_foreign_policy\",\n",
            "      \"acc,none\": 0.33,\n",
            "      \"acc_stderr,none\": 0.047258156262526045\n",
            "    },\n",
            "    \"mmlu_other\": {\n",
            "      \"alias\": \"other\",\n",
            "      \"acc,none\": 0.28226585130350823,\n",
            "      \"acc_stderr,none\": 0.008061761310826055\n",
            "    },\n",
            "    \"mmlu_business_ethics\": {\n",
            "      \"alias\": \" - business_ethics\",\n",
            "      \"acc,none\": 0.33,\n",
            "      \"acc_stderr,none\": 0.04725815626252604\n",
            "    },\n",
            "    \"mmlu_clinical_knowledge\": {\n",
            "      \"alias\": \" - clinical_knowledge\",\n",
            "      \"acc,none\": 0.2490566037735849,\n",
            "      \"acc_stderr,none\": 0.0266164829805017\n",
            "    },\n",
            "    \"mmlu_college_medicine\": {\n",
            "      \"alias\": \" - college_medicine\",\n",
            "      \"acc,none\": 0.2658959537572254,\n",
            "      \"acc_stderr,none\": 0.03368762932259431\n",
            "    },\n",
            "    \"mmlu_global_facts\": {\n",
            "      \"alias\": \" - global_facts\",\n",
            "      \"acc,none\": 0.25,\n",
            "      \"acc_stderr,none\": 0.04351941398892446\n",
            "    },\n",
            "    \"mmlu_human_aging\": {\n",
            "      \"alias\": \" - human_aging\",\n",
            "      \"acc,none\": 0.3542600896860987,\n",
            "      \"acc_stderr,none\": 0.032100621541349864\n",
            "    },\n",
            "    \"mmlu_management\": {\n",
            "      \"alias\": \" - management\",\n",
            "      \"acc,none\": 0.27184466019417475,\n",
            "      \"acc_stderr,none\": 0.044052680241409216\n",
            "    },\n",
            "    \"mmlu_marketing\": {\n",
            "      \"alias\": \" - marketing\",\n",
            "      \"acc,none\": 0.2863247863247863,\n",
            "      \"acc_stderr,none\": 0.029614323690456655\n",
            "    },\n",
            "    \"mmlu_medical_genetics\": {\n",
            "      \"alias\": \" - medical_genetics\",\n",
            "      \"acc,none\": 0.31,\n",
            "      \"acc_stderr,none\": 0.04648231987117316\n",
            "    },\n",
            "    \"mmlu_miscellaneous\": {\n",
            "      \"alias\": \" - miscellaneous\",\n",
            "      \"acc,none\": 0.2720306513409962,\n",
            "      \"acc_stderr,none\": 0.015913367447500527\n",
            "    },\n",
            "    \"mmlu_nutrition\": {\n",
            "      \"alias\": \" - nutrition\",\n",
            "      \"acc,none\": 0.23529411764705882,\n",
            "      \"acc_stderr,none\": 0.024288619466046095\n",
            "    },\n",
            "    \"mmlu_professional_accounting\": {\n",
            "      \"alias\": \" - professional_accounting\",\n",
            "      \"acc,none\": 0.24468085106382978,\n",
            "      \"acc_stderr,none\": 0.025645553622266733\n",
            "    },\n",
            "    \"mmlu_professional_medicine\": {\n",
            "      \"alias\": \" - professional_medicine\",\n",
            "      \"acc,none\": 0.35661764705882354,\n",
            "      \"acc_stderr,none\": 0.02909720956841196\n",
            "    },\n",
            "    \"mmlu_virology\": {\n",
            "      \"alias\": \" - virology\",\n",
            "      \"acc,none\": 0.3072289156626506,\n",
            "      \"acc_stderr,none\": 0.03591566797824664\n",
            "    },\n",
            "    \"mmlu_humanities\": {\n",
            "      \"alias\": \"humanities\",\n",
            "      \"acc,none\": 0.24803400637619555,\n",
            "      \"acc_stderr,none\": 0.0062982139880826266\n",
            "    },\n",
            "    \"mmlu_formal_logic\": {\n",
            "      \"alias\": \" - formal_logic\",\n",
            "      \"acc,none\": 0.2698412698412698,\n",
            "      \"acc_stderr,none\": 0.03970158273235172\n",
            "    },\n",
            "    \"mmlu_high_school_european_history\": {\n",
            "      \"alias\": \" - high_school_european_history\",\n",
            "      \"acc,none\": 0.24242424242424243,\n",
            "      \"acc_stderr,none\": 0.03346409881055953\n",
            "    },\n",
            "    \"mmlu_high_school_us_history\": {\n",
            "      \"alias\": \" - high_school_us_history\",\n",
            "      \"acc,none\": 0.22549019607843138,\n",
            "      \"acc_stderr,none\": 0.029331162294251735\n",
            "    },\n",
            "    \"mmlu_high_school_world_history\": {\n",
            "      \"alias\": \" - high_school_world_history\",\n",
            "      \"acc,none\": 0.2742616033755274,\n",
            "      \"acc_stderr,none\": 0.02904133351059804\n",
            "    },\n",
            "    \"mmlu_international_law\": {\n",
            "      \"alias\": \" - international_law\",\n",
            "      \"acc,none\": 0.23140495867768596,\n",
            "      \"acc_stderr,none\": 0.03849856098794089\n",
            "    },\n",
            "    \"mmlu_jurisprudence\": {\n",
            "      \"alias\": \" - jurisprudence\",\n",
            "      \"acc,none\": 0.24074074074074073,\n",
            "      \"acc_stderr,none\": 0.041331194402438376\n",
            "    },\n",
            "    \"mmlu_logical_fallacies\": {\n",
            "      \"alias\": \" - logical_fallacies\",\n",
            "      \"acc,none\": 0.24539877300613497,\n",
            "      \"acc_stderr,none\": 0.03380939813943354\n",
            "    },\n",
            "    \"mmlu_moral_disputes\": {\n",
            "      \"alias\": \" - moral_disputes\",\n",
            "      \"acc,none\": 0.2543352601156069,\n",
            "      \"acc_stderr,none\": 0.023445826276545546\n",
            "    },\n",
            "    \"mmlu_moral_scenarios\": {\n",
            "      \"alias\": \" - moral_scenarios\",\n",
            "      \"acc,none\": 0.24804469273743016,\n",
            "      \"acc_stderr,none\": 0.014444157808261466\n",
            "    },\n",
            "    \"mmlu_philosophy\": {\n",
            "      \"alias\": \" - philosophy\",\n",
            "      \"acc,none\": 0.2315112540192926,\n",
            "      \"acc_stderr,none\": 0.023956532766639133\n",
            "    },\n",
            "    \"mmlu_prehistory\": {\n",
            "      \"alias\": \" - prehistory\",\n",
            "      \"acc,none\": 0.27469135802469136,\n",
            "      \"acc_stderr,none\": 0.02483605786829468\n",
            "    },\n",
            "    \"mmlu_professional_law\": {\n",
            "      \"alias\": \" - professional_law\",\n",
            "      \"acc,none\": 0.23598435462842243,\n",
            "      \"acc_stderr,none\": 0.010844802669662671\n",
            "    },\n",
            "    \"mmlu_world_religions\": {\n",
            "      \"alias\": \" - world_religions\",\n",
            "      \"acc,none\": 0.3216374269005848,\n",
            "      \"acc_stderr,none\": 0.035825294425731215\n",
            "    }\n",
            "  },\n",
            "  \"groups\": {\n",
            "    \"mmlu_stem\": {\n",
            "      \"alias\": \"stem\",\n",
            "      \"acc,none\": 0.2327941642879797,\n",
            "      \"acc_stderr,none\": 0.007506916152472624\n",
            "    },\n",
            "    \"mmlu_social_sciences\": {\n",
            "      \"alias\": \"social_sciences\",\n",
            "      \"acc,none\": 0.24211894702632433,\n",
            "      \"acc_stderr,none\": 0.007719587269967782\n",
            "    },\n",
            "    \"mmlu_other\": {\n",
            "      \"alias\": \"other\",\n",
            "      \"acc,none\": 0.28226585130350823,\n",
            "      \"acc_stderr,none\": 0.008061761310826055\n",
            "    },\n",
            "    \"mmlu_humanities\": {\n",
            "      \"alias\": \"humanities\",\n",
            "      \"acc,none\": 0.24803400637619555,\n",
            "      \"acc_stderr,none\": 0.0062982139880826266\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"mmlu_humanities\": [\n",
            "      \"mmlu_high_school_world_history\",\n",
            "      \"mmlu_international_law\",\n",
            "      \"mmlu_high_school_european_history\",\n",
            "      \"mmlu_moral_disputes\",\n",
            "      \"mmlu_prehistory\",\n",
            "      \"mmlu_high_school_us_history\",\n",
            "      \"mmlu_professional_law\",\n",
            "      \"mmlu_moral_scenarios\",\n",
            "      \"mmlu_world_religions\",\n",
            "      \"mmlu_jurisprudence\",\n",
            "      \"mmlu_logical_fallacies\",\n",
            "      \"mmlu_formal_logic\",\n",
            "      \"mmlu_philosophy\"\n",
            "    ],\n",
            "    \"mmlu_other\": [\n",
            "      \"mmlu_professional_accounting\",\n",
            "      \"mmlu_medical_genetics\",\n",
            "      \"mmlu_college_medicine\",\n",
            "      \"mmlu_clinical_knowledge\",\n",
            "      \"mmlu_management\",\n",
            "      \"mmlu_human_aging\",\n",
            "      \"mmlu_business_ethics\",\n",
            "      \"mmlu_global_facts\",\n",
            "      \"mmlu_miscellaneous\",\n",
            "      \"mmlu_nutrition\",\n",
            "      \"mmlu_professional_medicine\",\n",
            "      \"mmlu_marketing\",\n",
            "      \"mmlu_virology\"\n",
            "    ],\n",
            "    \"mmlu_social_sciences\": [\n",
            "      \"mmlu_human_sexuality\",\n",
            "      \"mmlu_high_school_geography\",\n",
            "      \"mmlu_econometrics\",\n",
            "      \"mmlu_high_school_macroeconomics\",\n",
            "      \"mmlu_professional_psychology\",\n",
            "      \"mmlu_high_school_government_and_politics\",\n",
            "      \"mmlu_high_school_psychology\",\n",
            "      \"mmlu_security_studies\",\n",
            "      \"mmlu_us_foreign_policy\",\n",
            "      \"mmlu_high_school_microeconomics\",\n",
            "      \"mmlu_public_relations\",\n",
            "      \"mmlu_sociology\"\n",
            "    ],\n",
            "    \"mmlu_stem\": [\n",
            "      \"mmlu_conceptual_physics\",\n",
            "      \"mmlu_high_school_biology\",\n",
            "      \"mmlu_anatomy\",\n",
            "      \"mmlu_college_chemistry\",\n",
            "      \"mmlu_elementary_mathematics\",\n",
            "      \"mmlu_college_biology\",\n",
            "      \"mmlu_electrical_engineering\",\n",
            "      \"mmlu_machine_learning\",\n",
            "      \"mmlu_high_school_statistics\",\n",
            "      \"mmlu_computer_security\",\n",
            "      \"mmlu_abstract_algebra\",\n",
            "      \"mmlu_high_school_computer_science\",\n",
            "      \"mmlu_college_physics\",\n",
            "      \"mmlu_college_computer_science\",\n",
            "      \"mmlu_astronomy\",\n",
            "      \"mmlu_high_school_physics\",\n",
            "      \"mmlu_college_mathematics\",\n",
            "      \"mmlu_high_school_mathematics\",\n",
            "      \"mmlu_high_school_chemistry\"\n",
            "    ]\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"mmlu_abstract_algebra\": {\n",
            "      \"task\": \"mmlu_abstract_algebra\",\n",
            "      \"task_alias\": \"abstract_algebra\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"abstract_algebra\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about abstract algebra.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_anatomy\": {\n",
            "      \"task\": \"mmlu_anatomy\",\n",
            "      \"task_alias\": \"anatomy\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"anatomy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about anatomy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_astronomy\": {\n",
            "      \"task\": \"mmlu_astronomy\",\n",
            "      \"task_alias\": \"astronomy\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"astronomy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about astronomy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_business_ethics\": {\n",
            "      \"task\": \"mmlu_business_ethics\",\n",
            "      \"task_alias\": \"business_ethics\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"business_ethics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about business ethics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_clinical_knowledge\": {\n",
            "      \"task\": \"mmlu_clinical_knowledge\",\n",
            "      \"task_alias\": \"clinical_knowledge\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"clinical_knowledge\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about clinical knowledge.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_biology\": {\n",
            "      \"task\": \"mmlu_college_biology\",\n",
            "      \"task_alias\": \"college_biology\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_biology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college biology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_chemistry\": {\n",
            "      \"task\": \"mmlu_college_chemistry\",\n",
            "      \"task_alias\": \"college_chemistry\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_chemistry\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college chemistry.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_computer_science\": {\n",
            "      \"task\": \"mmlu_college_computer_science\",\n",
            "      \"task_alias\": \"college_computer_science\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_computer_science\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college computer science.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_mathematics\": {\n",
            "      \"task\": \"mmlu_college_mathematics\",\n",
            "      \"task_alias\": \"college_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_medicine\": {\n",
            "      \"task\": \"mmlu_college_medicine\",\n",
            "      \"task_alias\": \"college_medicine\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_medicine\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college medicine.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_physics\": {\n",
            "      \"task\": \"mmlu_college_physics\",\n",
            "      \"task_alias\": \"college_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_computer_security\": {\n",
            "      \"task\": \"mmlu_computer_security\",\n",
            "      \"task_alias\": \"computer_security\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"computer_security\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about computer security.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_conceptual_physics\": {\n",
            "      \"task\": \"mmlu_conceptual_physics\",\n",
            "      \"task_alias\": \"conceptual_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"conceptual_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about conceptual physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_econometrics\": {\n",
            "      \"task\": \"mmlu_econometrics\",\n",
            "      \"task_alias\": \"econometrics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"econometrics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about econometrics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_electrical_engineering\": {\n",
            "      \"task\": \"mmlu_electrical_engineering\",\n",
            "      \"task_alias\": \"electrical_engineering\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"electrical_engineering\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about electrical engineering.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_elementary_mathematics\": {\n",
            "      \"task\": \"mmlu_elementary_mathematics\",\n",
            "      \"task_alias\": \"elementary_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"elementary_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about elementary mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_formal_logic\": {\n",
            "      \"task\": \"mmlu_formal_logic\",\n",
            "      \"task_alias\": \"formal_logic\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"formal_logic\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about formal logic.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_global_facts\": {\n",
            "      \"task\": \"mmlu_global_facts\",\n",
            "      \"task_alias\": \"global_facts\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"global_facts\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about global facts.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_biology\": {\n",
            "      \"task\": \"mmlu_high_school_biology\",\n",
            "      \"task_alias\": \"high_school_biology\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_biology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school biology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_chemistry\": {\n",
            "      \"task\": \"mmlu_high_school_chemistry\",\n",
            "      \"task_alias\": \"high_school_chemistry\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_chemistry\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school chemistry.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_computer_science\": {\n",
            "      \"task\": \"mmlu_high_school_computer_science\",\n",
            "      \"task_alias\": \"high_school_computer_science\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_computer_science\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school computer science.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_european_history\": {\n",
            "      \"task\": \"mmlu_high_school_european_history\",\n",
            "      \"task_alias\": \"high_school_european_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_european_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school european history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_geography\": {\n",
            "      \"task\": \"mmlu_high_school_geography\",\n",
            "      \"task_alias\": \"high_school_geography\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_geography\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school geography.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_government_and_politics\": {\n",
            "      \"task\": \"mmlu_high_school_government_and_politics\",\n",
            "      \"task_alias\": \"high_school_government_and_politics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_government_and_politics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school government and politics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_macroeconomics\": {\n",
            "      \"task\": \"mmlu_high_school_macroeconomics\",\n",
            "      \"task_alias\": \"high_school_macroeconomics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_macroeconomics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school macroeconomics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_mathematics\": {\n",
            "      \"task\": \"mmlu_high_school_mathematics\",\n",
            "      \"task_alias\": \"high_school_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_microeconomics\": {\n",
            "      \"task\": \"mmlu_high_school_microeconomics\",\n",
            "      \"task_alias\": \"high_school_microeconomics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_microeconomics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school microeconomics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_physics\": {\n",
            "      \"task\": \"mmlu_high_school_physics\",\n",
            "      \"task_alias\": \"high_school_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_psychology\": {\n",
            "      \"task\": \"mmlu_high_school_psychology\",\n",
            "      \"task_alias\": \"high_school_psychology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_psychology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school psychology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_statistics\": {\n",
            "      \"task\": \"mmlu_high_school_statistics\",\n",
            "      \"task_alias\": \"high_school_statistics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_statistics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school statistics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_us_history\": {\n",
            "      \"task\": \"mmlu_high_school_us_history\",\n",
            "      \"task_alias\": \"high_school_us_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_us_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school us history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_world_history\": {\n",
            "      \"task\": \"mmlu_high_school_world_history\",\n",
            "      \"task_alias\": \"high_school_world_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_world_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school world history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_human_aging\": {\n",
            "      \"task\": \"mmlu_human_aging\",\n",
            "      \"task_alias\": \"human_aging\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"human_aging\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about human aging.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_human_sexuality\": {\n",
            "      \"task\": \"mmlu_human_sexuality\",\n",
            "      \"task_alias\": \"human_sexuality\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"human_sexuality\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about human sexuality.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_international_law\": {\n",
            "      \"task\": \"mmlu_international_law\",\n",
            "      \"task_alias\": \"international_law\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"international_law\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about international law.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_jurisprudence\": {\n",
            "      \"task\": \"mmlu_jurisprudence\",\n",
            "      \"task_alias\": \"jurisprudence\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"jurisprudence\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about jurisprudence.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_logical_fallacies\": {\n",
            "      \"task\": \"mmlu_logical_fallacies\",\n",
            "      \"task_alias\": \"logical_fallacies\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"logical_fallacies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about logical fallacies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_machine_learning\": {\n",
            "      \"task\": \"mmlu_machine_learning\",\n",
            "      \"task_alias\": \"machine_learning\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"machine_learning\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about machine learning.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_management\": {\n",
            "      \"task\": \"mmlu_management\",\n",
            "      \"task_alias\": \"management\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"management\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about management.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_marketing\": {\n",
            "      \"task\": \"mmlu_marketing\",\n",
            "      \"task_alias\": \"marketing\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"marketing\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about marketing.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_medical_genetics\": {\n",
            "      \"task\": \"mmlu_medical_genetics\",\n",
            "      \"task_alias\": \"medical_genetics\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"medical_genetics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about medical genetics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_miscellaneous\": {\n",
            "      \"task\": \"mmlu_miscellaneous\",\n",
            "      \"task_alias\": \"miscellaneous\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"miscellaneous\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about miscellaneous.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_moral_disputes\": {\n",
            "      \"task\": \"mmlu_moral_disputes\",\n",
            "      \"task_alias\": \"moral_disputes\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"moral_disputes\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about moral disputes.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_moral_scenarios\": {\n",
            "      \"task\": \"mmlu_moral_scenarios\",\n",
            "      \"task_alias\": \"moral_scenarios\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"moral_scenarios\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about moral scenarios.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_nutrition\": {\n",
            "      \"task\": \"mmlu_nutrition\",\n",
            "      \"task_alias\": \"nutrition\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"nutrition\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about nutrition.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_philosophy\": {\n",
            "      \"task\": \"mmlu_philosophy\",\n",
            "      \"task_alias\": \"philosophy\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"philosophy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about philosophy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_prehistory\": {\n",
            "      \"task\": \"mmlu_prehistory\",\n",
            "      \"task_alias\": \"prehistory\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"prehistory\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about prehistory.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_accounting\": {\n",
            "      \"task\": \"mmlu_professional_accounting\",\n",
            "      \"task_alias\": \"professional_accounting\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_accounting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional accounting.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_law\": {\n",
            "      \"task\": \"mmlu_professional_law\",\n",
            "      \"task_alias\": \"professional_law\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_law\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional law.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_medicine\": {\n",
            "      \"task\": \"mmlu_professional_medicine\",\n",
            "      \"task_alias\": \"professional_medicine\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_medicine\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional medicine.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_psychology\": {\n",
            "      \"task\": \"mmlu_professional_psychology\",\n",
            "      \"task_alias\": \"professional_psychology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_psychology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional psychology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_public_relations\": {\n",
            "      \"task\": \"mmlu_public_relations\",\n",
            "      \"task_alias\": \"public_relations\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"public_relations\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about public relations.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_security_studies\": {\n",
            "      \"task\": \"mmlu_security_studies\",\n",
            "      \"task_alias\": \"security_studies\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"security_studies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about security studies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_sociology\": {\n",
            "      \"task\": \"mmlu_sociology\",\n",
            "      \"task_alias\": \"sociology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"sociology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about sociology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_us_foreign_policy\": {\n",
            "      \"task\": \"mmlu_us_foreign_policy\",\n",
            "      \"task_alias\": \"us_foreign_policy\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"us_foreign_policy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about us foreign policy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_virology\": {\n",
            "      \"task\": \"mmlu_virology\",\n",
            "      \"task_alias\": \"virology\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"virology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about virology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_world_religions\": {\n",
            "      \"task\": \"mmlu_world_religions\",\n",
            "      \"task_alias\": \"world_religions\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"world_religions\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about world religions.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"mmlu_abstract_algebra\": 0.0,\n",
            "    \"mmlu_anatomy\": 0.0,\n",
            "    \"mmlu_astronomy\": 0.0,\n",
            "    \"mmlu_business_ethics\": 0.0,\n",
            "    \"mmlu_clinical_knowledge\": 0.0,\n",
            "    \"mmlu_college_biology\": 0.0,\n",
            "    \"mmlu_college_chemistry\": 0.0,\n",
            "    \"mmlu_college_computer_science\": 0.0,\n",
            "    \"mmlu_college_mathematics\": 0.0,\n",
            "    \"mmlu_college_medicine\": 0.0,\n",
            "    \"mmlu_college_physics\": 0.0,\n",
            "    \"mmlu_computer_security\": 0.0,\n",
            "    \"mmlu_conceptual_physics\": 0.0,\n",
            "    \"mmlu_econometrics\": 0.0,\n",
            "    \"mmlu_electrical_engineering\": 0.0,\n",
            "    \"mmlu_elementary_mathematics\": 0.0,\n",
            "    \"mmlu_formal_logic\": 0.0,\n",
            "    \"mmlu_global_facts\": 0.0,\n",
            "    \"mmlu_high_school_biology\": 0.0,\n",
            "    \"mmlu_high_school_chemistry\": 0.0,\n",
            "    \"mmlu_high_school_computer_science\": 0.0,\n",
            "    \"mmlu_high_school_european_history\": 0.0,\n",
            "    \"mmlu_high_school_geography\": 0.0,\n",
            "    \"mmlu_high_school_government_and_politics\": 0.0,\n",
            "    \"mmlu_high_school_macroeconomics\": 0.0,\n",
            "    \"mmlu_high_school_mathematics\": 0.0,\n",
            "    \"mmlu_high_school_microeconomics\": 0.0,\n",
            "    \"mmlu_high_school_physics\": 0.0,\n",
            "    \"mmlu_high_school_psychology\": 0.0,\n",
            "    \"mmlu_high_school_statistics\": 0.0,\n",
            "    \"mmlu_high_school_us_history\": 0.0,\n",
            "    \"mmlu_high_school_world_history\": 0.0,\n",
            "    \"mmlu_human_aging\": 0.0,\n",
            "    \"mmlu_human_sexuality\": 0.0,\n",
            "    \"mmlu_international_law\": 0.0,\n",
            "    \"mmlu_jurisprudence\": 0.0,\n",
            "    \"mmlu_logical_fallacies\": 0.0,\n",
            "    \"mmlu_machine_learning\": 0.0,\n",
            "    \"mmlu_management\": 0.0,\n",
            "    \"mmlu_marketing\": 0.0,\n",
            "    \"mmlu_medical_genetics\": 0.0,\n",
            "    \"mmlu_miscellaneous\": 0.0,\n",
            "    \"mmlu_moral_disputes\": 0.0,\n",
            "    \"mmlu_moral_scenarios\": 0.0,\n",
            "    \"mmlu_nutrition\": 0.0,\n",
            "    \"mmlu_philosophy\": 0.0,\n",
            "    \"mmlu_prehistory\": 0.0,\n",
            "    \"mmlu_professional_accounting\": 0.0,\n",
            "    \"mmlu_professional_law\": 0.0,\n",
            "    \"mmlu_professional_medicine\": 0.0,\n",
            "    \"mmlu_professional_psychology\": 0.0,\n",
            "    \"mmlu_public_relations\": 0.0,\n",
            "    \"mmlu_security_studies\": 0.0,\n",
            "    \"mmlu_sociology\": 0.0,\n",
            "    \"mmlu_us_foreign_policy\": 0.0,\n",
            "    \"mmlu_virology\": 0.0,\n",
            "    \"mmlu_world_religions\": 0.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"mmlu_abstract_algebra\": 5,\n",
            "    \"mmlu_anatomy\": 5,\n",
            "    \"mmlu_astronomy\": 5,\n",
            "    \"mmlu_business_ethics\": 5,\n",
            "    \"mmlu_clinical_knowledge\": 5,\n",
            "    \"mmlu_college_biology\": 5,\n",
            "    \"mmlu_college_chemistry\": 5,\n",
            "    \"mmlu_college_computer_science\": 5,\n",
            "    \"mmlu_college_mathematics\": 5,\n",
            "    \"mmlu_college_medicine\": 5,\n",
            "    \"mmlu_college_physics\": 5,\n",
            "    \"mmlu_computer_security\": 5,\n",
            "    \"mmlu_conceptual_physics\": 5,\n",
            "    \"mmlu_econometrics\": 5,\n",
            "    \"mmlu_electrical_engineering\": 5,\n",
            "    \"mmlu_elementary_mathematics\": 5,\n",
            "    \"mmlu_formal_logic\": 5,\n",
            "    \"mmlu_global_facts\": 5,\n",
            "    \"mmlu_high_school_biology\": 5,\n",
            "    \"mmlu_high_school_chemistry\": 5,\n",
            "    \"mmlu_high_school_computer_science\": 5,\n",
            "    \"mmlu_high_school_european_history\": 5,\n",
            "    \"mmlu_high_school_geography\": 5,\n",
            "    \"mmlu_high_school_government_and_politics\": 5,\n",
            "    \"mmlu_high_school_macroeconomics\": 5,\n",
            "    \"mmlu_high_school_mathematics\": 5,\n",
            "    \"mmlu_high_school_microeconomics\": 5,\n",
            "    \"mmlu_high_school_physics\": 5,\n",
            "    \"mmlu_high_school_psychology\": 5,\n",
            "    \"mmlu_high_school_statistics\": 5,\n",
            "    \"mmlu_high_school_us_history\": 5,\n",
            "    \"mmlu_high_school_world_history\": 5,\n",
            "    \"mmlu_human_aging\": 5,\n",
            "    \"mmlu_human_sexuality\": 5,\n",
            "    \"mmlu_humanities\": 5,\n",
            "    \"mmlu_international_law\": 5,\n",
            "    \"mmlu_jurisprudence\": 5,\n",
            "    \"mmlu_logical_fallacies\": 5,\n",
            "    \"mmlu_machine_learning\": 5,\n",
            "    \"mmlu_management\": 5,\n",
            "    \"mmlu_marketing\": 5,\n",
            "    \"mmlu_medical_genetics\": 5,\n",
            "    \"mmlu_miscellaneous\": 5,\n",
            "    \"mmlu_moral_disputes\": 5,\n",
            "    \"mmlu_moral_scenarios\": 5,\n",
            "    \"mmlu_nutrition\": 5,\n",
            "    \"mmlu_other\": 5,\n",
            "    \"mmlu_philosophy\": 5,\n",
            "    \"mmlu_prehistory\": 5,\n",
            "    \"mmlu_professional_accounting\": 5,\n",
            "    \"mmlu_professional_law\": 5,\n",
            "    \"mmlu_professional_medicine\": 5,\n",
            "    \"mmlu_professional_psychology\": 5,\n",
            "    \"mmlu_public_relations\": 5,\n",
            "    \"mmlu_security_studies\": 5,\n",
            "    \"mmlu_social_sciences\": 5,\n",
            "    \"mmlu_sociology\": 5,\n",
            "    \"mmlu_stem\": 5,\n",
            "    \"mmlu_us_foreign_policy\": 5,\n",
            "    \"mmlu_virology\": 5,\n",
            "    \"mmlu_world_religions\": 5\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      8,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_falcon_mmlu\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712232442.3806384,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto:4 (8,64,64,64,64)\n",
            "|                Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|--------------------------------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem                                  |N/A    |none  |     5|acc   |0.2328|±  |0.0075|\n",
            "| - abstract_algebra                   |      0|none  |     5|acc   |0.2900|±  |0.0456|\n",
            "| - anatomy                            |      0|none  |     5|acc   |0.2000|±  |0.0346|\n",
            "| - astronomy                          |      0|none  |     5|acc   |0.2434|±  |0.0349|\n",
            "| - college_biology                    |      0|none  |     5|acc   |0.2500|±  |0.0362|\n",
            "| - college_chemistry                  |      0|none  |     5|acc   |0.2000|±  |0.0402|\n",
            "| - college_computer_science           |      0|none  |     5|acc   |0.2900|±  |0.0456|\n",
            "| - college_mathematics                |      0|none  |     5|acc   |0.2600|±  |0.0441|\n",
            "| - college_physics                    |      0|none  |     5|acc   |0.2059|±  |0.0402|\n",
            "| - computer_security                  |      0|none  |     5|acc   |0.3200|±  |0.0469|\n",
            "| - conceptual_physics                 |      0|none  |     5|acc   |0.2979|±  |0.0299|\n",
            "| - electrical_engineering             |      0|none  |     5|acc   |0.2759|±  |0.0372|\n",
            "| - elementary_mathematics             |      0|none  |     5|acc   |0.2275|±  |0.0216|\n",
            "| - high_school_biology                |      0|none  |     5|acc   |0.2065|±  |0.0230|\n",
            "| - high_school_chemistry              |      0|none  |     5|acc   |0.1970|±  |0.0280|\n",
            "| - high_school_computer_science       |      0|none  |     5|acc   |0.2400|±  |0.0429|\n",
            "| - high_school_mathematics            |      0|none  |     5|acc   |0.1852|±  |0.0237|\n",
            "| - high_school_physics                |      0|none  |     5|acc   |0.2119|±  |0.0334|\n",
            "| - high_school_statistics             |      0|none  |     5|acc   |0.1620|±  |0.0251|\n",
            "| - machine_learning                   |      0|none  |     5|acc   |0.3214|±  |0.0443|\n",
            "|social_sciences                       |N/A    |none  |     5|acc   |0.2421|±  |0.0077|\n",
            "| - econometrics                       |      0|none  |     5|acc   |0.2719|±  |0.0419|\n",
            "| - high_school_geography              |      0|none  |     5|acc   |0.2475|±  |0.0307|\n",
            "| - high_school_government_and_politics|      0|none  |     5|acc   |0.2383|±  |0.0307|\n",
            "| - high_school_macroeconomics         |      0|none  |     5|acc   |0.2436|±  |0.0218|\n",
            "| - high_school_microeconomics         |      0|none  |     5|acc   |0.2311|±  |0.0274|\n",
            "| - high_school_psychology             |      0|none  |     5|acc   |0.2495|±  |0.0186|\n",
            "| - human_sexuality                    |      0|none  |     5|acc   |0.2748|±  |0.0392|\n",
            "| - professional_psychology            |      0|none  |     5|acc   |0.2239|±  |0.0169|\n",
            "| - public_relations                   |      0|none  |     5|acc   |0.3182|±  |0.0446|\n",
            "| - security_studies                   |      0|none  |     5|acc   |0.1918|±  |0.0252|\n",
            "| - sociology                          |      0|none  |     5|acc   |0.2239|±  |0.0295|\n",
            "| - us_foreign_policy                  |      0|none  |     5|acc   |0.3300|±  |0.0473|\n",
            "|other                                 |N/A    |none  |     5|acc   |0.2823|±  |0.0081|\n",
            "| - business_ethics                    |      0|none  |     5|acc   |0.3300|±  |0.0473|\n",
            "| - clinical_knowledge                 |      0|none  |     5|acc   |0.2491|±  |0.0266|\n",
            "| - college_medicine                   |      0|none  |     5|acc   |0.2659|±  |0.0337|\n",
            "| - global_facts                       |      0|none  |     5|acc   |0.2500|±  |0.0435|\n",
            "| - human_aging                        |      0|none  |     5|acc   |0.3543|±  |0.0321|\n",
            "| - management                         |      0|none  |     5|acc   |0.2718|±  |0.0441|\n",
            "| - marketing                          |      0|none  |     5|acc   |0.2863|±  |0.0296|\n",
            "| - medical_genetics                   |      0|none  |     5|acc   |0.3100|±  |0.0465|\n",
            "| - miscellaneous                      |      0|none  |     5|acc   |0.2720|±  |0.0159|\n",
            "| - nutrition                          |      0|none  |     5|acc   |0.2353|±  |0.0243|\n",
            "| - professional_accounting            |      0|none  |     5|acc   |0.2447|±  |0.0256|\n",
            "| - professional_medicine              |      0|none  |     5|acc   |0.3566|±  |0.0291|\n",
            "| - virology                           |      0|none  |     5|acc   |0.3072|±  |0.0359|\n",
            "|humanities                            |N/A    |none  |     5|acc   |0.2480|±  |0.0063|\n",
            "| - formal_logic                       |      0|none  |     5|acc   |0.2698|±  |0.0397|\n",
            "| - high_school_european_history       |      0|none  |     5|acc   |0.2424|±  |0.0335|\n",
            "| - high_school_us_history             |      0|none  |     5|acc   |0.2255|±  |0.0293|\n",
            "| - high_school_world_history          |      0|none  |     5|acc   |0.2743|±  |0.0290|\n",
            "| - international_law                  |      0|none  |     5|acc   |0.2314|±  |0.0385|\n",
            "| - jurisprudence                      |      0|none  |     5|acc   |0.2407|±  |0.0413|\n",
            "| - logical_fallacies                  |      0|none  |     5|acc   |0.2454|±  |0.0338|\n",
            "| - moral_disputes                     |      0|none  |     5|acc   |0.2543|±  |0.0234|\n",
            "| - moral_scenarios                    |      0|none  |     5|acc   |0.2480|±  |0.0144|\n",
            "| - philosophy                         |      0|none  |     5|acc   |0.2315|±  |0.0240|\n",
            "| - prehistory                         |      0|none  |     5|acc   |0.2747|±  |0.0248|\n",
            "| - professional_law                   |      0|none  |     5|acc   |0.2360|±  |0.0108|\n",
            "| - world_religions                    |      0|none  |     5|acc   |0.3216|±  |0.0358|\n",
            "\n",
            "|    Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|---------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem           |N/A    |none  |     5|acc   |0.2328|±  |0.0075|\n",
            "|social_sciences|N/A    |none  |     5|acc   |0.2421|±  |0.0077|\n",
            "|other          |N/A    |none  |     5|acc   |0.2823|±  |0.0081|\n",
            "|humanities     |N/A    |none  |     5|acc   |0.2480|±  |0.0063|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc 0.29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr 0.0456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_abstract_algebra/alias  - abstract_algebra\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr 0.03455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_anatomy/alias  - anatomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc 0.24342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr 0.03492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_astronomy/alias  - astronomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc 0.33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr 0.04726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_business_ethics/alias  - business_ethics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc 0.24906\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr 0.02662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_clinical_knowledge/alias  - clinical_knowledg...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc 0.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr 0.03621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_biology/alias  - college_biology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr 0.0402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_chemistry/alias  - college_chemistry...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc 0.29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr 0.0456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_computer_science/alias  - college_computer_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc 0.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr 0.04408\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_college_mathematics/alias  - college_mathemati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc 0.2659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr 0.03369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_college_medicine/alias  - college_medicine\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc 0.20588\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr 0.04023\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_physics/alias  - college_physics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc 0.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr 0.04688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_computer_security/alias  - computer_security...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc 0.29787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr 0.0299\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_conceptual_physics/alias  - conceptual_physic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc 0.27193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr 0.04186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_econometrics/alias  - econometrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc 0.27586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr 0.03725\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_electrical_engineering/alias  - electrical_engine...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc 0.22751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr 0.02159\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_elementary_mathematics/alias  - elementary_mathem...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc 0.26984\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr 0.0397\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_formal_logic/alias  - formal_logic\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc 0.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr 0.04352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_global_facts/alias  - global_facts\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc 0.20645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr 0.02303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_biology/alias  - high_school_biolo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc 0.19704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr 0.02799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_chemistry/alias  - high_school_chemi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc 0.24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr 0.04292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_computer_science/alias  - high_school_compu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc 0.24242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr 0.03346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_european_history/alias  - high_school_europ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc 0.24747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr 0.03075\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_geography/alias  - high_school_geogr...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc 0.23834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr 0.03075\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      mmlu_high_school_government_and_politics/alias  - high_school_gover...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc 0.24359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr 0.02176\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_macroeconomics/alias  - high_school_macro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc 0.18519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr 0.02368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_mathematics/alias  - high_school_mathe...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc 0.23109\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr 0.02738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_microeconomics/alias  - high_school_micro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc 0.21192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr 0.03337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_physics/alias  - high_school_physi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc 0.24954\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr 0.01855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_psychology/alias  - high_school_psych...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc 0.16204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr 0.02513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_statistics/alias  - high_school_stati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc 0.22549\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr 0.02933\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_us_history/alias  - high_school_us_hi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc 0.27426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr 0.02904\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                mmlu_high_school_world_history/alias  - high_school_world...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc 0.35426\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr 0.0321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_human_aging/alias  - human_aging\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc 0.27481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr 0.03915\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_human_sexuality/alias  - human_sexuality\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc 0.24803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr 0.0063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_humanities/alias humanities\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc 0.2314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr 0.0385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_international_law/alias  - international_law...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc 0.24074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr 0.04133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_jurisprudence/alias  - jurisprudence\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc 0.2454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr 0.03381\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_logical_fallacies/alias  - logical_fallacies...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc 0.32143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr 0.04433\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_machine_learning/alias  - machine_learning\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc 0.27184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr 0.04405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_management/alias  - management\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc 0.28632\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr 0.02961\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_marketing/alias  - marketing\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc 0.31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr 0.04648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_medical_genetics/alias  - medical_genetics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc 0.27203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr 0.01591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_miscellaneous/alias  - miscellaneous\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc 0.25434\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr 0.02345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_moral_disputes/alias  - moral_disputes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc 0.24804\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr 0.01444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_moral_scenarios/alias  - moral_scenarios\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc 0.23529\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr 0.02429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_nutrition/alias  - nutrition\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc 0.28227\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr 0.00806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_other/alias other\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc 0.23151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr 0.02396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_philosophy/alias  - philosophy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc 0.27469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr 0.02484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_prehistory/alias  - prehistory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc 0.24468\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr 0.02565\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_accounting/alias  - professional_acco...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc 0.23598\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr 0.01084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_professional_law/alias  - professional_law\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc 0.35662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr 0.0291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_medicine/alias  - professional_medi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc 0.22386\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr 0.01686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_psychology/alias  - professional_psyc...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc 0.31818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr 0.04461\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_public_relations/alias  - public_relations\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc 0.19184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr 0.02521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_security_studies/alias  - security_studies\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc 0.24212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr 0.00772\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_social_sciences/alias social_sciences\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc 0.22388\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr 0.02948\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_sociology/alias  - sociology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc 0.23279\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr 0.00751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     mmlu_stem/alias stem\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc 0.33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr 0.04726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_us_foreign_policy/alias  - us_foreign_policy...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc 0.30723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr 0.03592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_virology/alias  - virology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc 0.32164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr 0.03583\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_world_religions/alias  - world_religions\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mFalcon-7B-instruct-MMLU\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/yytitm03\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 64 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_120715-yytitm03/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HellaSwag\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "43rhYS7ShpKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True \\\n",
        "    --tasks hellaswag \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_hellaswag \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Falcon-7B-instruct-Hellaswag \\\n",
        "    --use_cache results_dir_hellaswag \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "2Fhq373ahz5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b8e2d2-b5c5-4917-9169-c6ad4d86a8e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 12:45:36.498362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 12:45:36.498410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 12:45:36.499883: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 12:45:37.701871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_124542-h51frpov\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFalcon-7B-instruct-Hellaswag\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/h51frpov\u001b[0m\n",
            "2024-04-04:12:45:43,935 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:12:45:50,100 WARNING  [__main__.py:316] File results_dir_hellaswag/results.json already exists. Results will be overwritten.\n",
            "2024-04-04:12:45:50,100 INFO     [__main__.py:335] Selected Tasks: ['hellaswag']\n",
            "2024-04-04:12:45:50,101 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:12:45:50,101 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'tiiuae/falcon-7b-instruct', 'trust_remote_code': True}\n",
            "2024-04-04:12:45:50,105 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "2024-04-04:12:45:51,351 WARNING  [logging.py:329] \n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.73s/it]\n",
            "2024-04-04:12:45:58,523 INFO     [evaluator.py:195] Using cache at results_dir_hellaswag_rank0.db\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-04-04:12:47:09,682 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2024-04-04:12:47:09,684 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
            "100% 10042/10042 [00:04<00:00, 2200.88it/s]\n",
            "2024-04-04:12:47:19,728 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:12:47:19,738 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_hellaswag_rank0.db' where possible...\n",
            "Checking cached requests: 100% 40168/40168 [00:05<00:00, 7253.45it/s]\n",
            "2024-04-04:12:47:25,277 INFO     [model.py:249] Cached requests: 0, Requests remaining: 40168\n",
            "Running loglikelihood requests:   0% 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests:  25% 9930/40168 [01:30<04:06, 122.63it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 40168/40168 [04:20<00:00, 154.20it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"hellaswag\": {\n",
            "      \"acc,none\": 0.5167297351125274,\n",
            "      \"acc_stderr,none\": 0.004986987508928697,\n",
            "      \"acc_norm,none\": 0.6976697868950408,\n",
            "      \"acc_norm_stderr,none\": 0.004583289072937798,\n",
            "      \"alias\": \"hellaswag\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"hellaswag\": []\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"hellaswag\": {\n",
            "      \"task\": \"hellaswag\",\n",
            "      \"group\": [\n",
            "        \"multiple_choice\"\n",
            "      ],\n",
            "      \"dataset_path\": \"hellaswag\",\n",
            "      \"training_split\": \"train\",\n",
            "      \"validation_split\": \"validation\",\n",
            "      \"process_docs\": \"def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\\n    def _process_doc(doc):\\n        ctx = doc[\\\"ctx_a\\\"] + \\\" \\\" + doc[\\\"ctx_b\\\"].capitalize()\\n        out_doc = {\\n            \\\"query\\\": preprocess(doc[\\\"activity_label\\\"] + \\\": \\\" + ctx),\\n            \\\"choices\\\": [preprocess(ending) for ending in doc[\\\"endings\\\"]],\\n            \\\"gold\\\": int(doc[\\\"label\\\"]),\\n        }\\n        return out_doc\\n\\n    return dataset.map(_process_doc)\\n\",\n",
            "      \"doc_to_text\": \"{{query}}\",\n",
            "      \"doc_to_target\": \"{{label}}\",\n",
            "      \"doc_to_choice\": \"choices\",\n",
            "      \"description\": \"\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        },\n",
            "        {\n",
            "          \"metric\": \"acc_norm\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"hellaswag\": 1.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"hellaswag\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_hellaswag\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712234750.1012921,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4 (64,64,64,64,64)\n",
            "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
            "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
            "|hellaswag|      1|none  |     0|acc     |0.5167|±  |0.0050|\n",
            "|         |       |none  |     0|acc_norm|0.6977|±  |0.0046|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc 0.51673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm 0.69767\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr 0.00458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr 0.00499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           hellaswag/alias hellaswag\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mFalcon-7B-instruct-Hellaswag\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/h51frpov\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_124542-h51frpov/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BoolQ\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "2Sy5-kXCih2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True \\\n",
        "    --tasks boolq \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_boolq \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Falcon-7B-instruct-BoolQ \\\n",
        "    --use_cache results_dir_boolq \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "UiGDg2d-ii8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da6ce05-297c-4b93-9864-519bb74cfb39"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 12:53:01.782631: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 12:53:01.782685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 12:53:01.784153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 12:53:03.010297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_125308-6seeegad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFalcon-7B-instruct-BoolQ\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/6seeegad\u001b[0m\n",
            "2024-04-04:12:53:09,305 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:12:53:15,373 WARNING  [__main__.py:316] File results_dir_boolq/results.json already exists. Results will be overwritten.\n",
            "2024-04-04:12:53:15,374 INFO     [__main__.py:335] Selected Tasks: ['boolq']\n",
            "2024-04-04:12:53:15,374 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:12:53:15,375 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'tiiuae/falcon-7b-instruct', 'trust_remote_code': True}\n",
            "2024-04-04:12:53:15,379 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "2024-04-04:12:53:16,353 WARNING  [logging.py:329] \n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.38s/it]\n",
            "2024-04-04:12:53:22,760 INFO     [evaluator.py:195] Using cache at results_dir_boolq_rank0.db\n",
            "2024-04-04:12:53:22,764 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-04-04:12:53:22,765 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "2024-04-04:12:53:28,215 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
            "2024-04-04:12:53:28,242 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:12:53:28,244 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_boolq_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6540/6540 [00:01<00:00, 3648.71it/s]\n",
            "2024-04-04:12:53:30,037 INFO     [model.py:249] Cached requests: 6540, Requests remaining: 0\n",
            "Running loglikelihood requests: 0it [00:00, ?it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"boolq\": {\n",
            "      \"acc,none\": 0.7975535168195719,\n",
            "      \"acc_stderr,none\": 0.007027932397259822,\n",
            "      \"alias\": \"boolq\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"boolq\": []\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"boolq\": {\n",
            "      \"task\": \"boolq\",\n",
            "      \"group\": [\n",
            "        \"super-glue-lm-eval-v1\"\n",
            "      ],\n",
            "      \"dataset_path\": \"super_glue\",\n",
            "      \"dataset_name\": \"boolq\",\n",
            "      \"training_split\": \"train\",\n",
            "      \"validation_split\": \"validation\",\n",
            "      \"doc_to_text\": \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\",\n",
            "      \"doc_to_target\": \"label\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"no\",\n",
            "        \"yes\"\n",
            "      ],\n",
            "      \"description\": \"\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\"\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": true,\n",
            "      \"doc_to_decontamination_query\": \"passage\",\n",
            "      \"metadata\": {\n",
            "        \"version\": 2.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"boolq\": 2.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"boolq\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_boolq\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712235195.3746045,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4\n",
            "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
            "|boolq|      2|none  |     0|acc   |0.7976|±  | 0.007|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc 0.79755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr 0.00703\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      boolq/alias boolq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mFalcon-7B-instruct-BoolQ\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/6seeegad\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_125308-6seeegad/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BBH - no score returned\n",
        "\n",
        "3-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "Ba8lOe4Dijih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True,do_sample=True \\\n",
        "    --tasks bbh_fewshot \\\n",
        "    --num_fewshot 3 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size 2 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_falcon_bbh \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Falcon-7B-instruct-BBH \\\n",
        "    --use_cache results_dir_falcon_bbh \\\n",
        "    --cache_requests true \\\n",
        "    --write_out \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "leiB-7asikGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e086c33-a881-4c3a-9887-bc3a9fd507f8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 12:53:53.374903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 12:53:53.374955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 12:53:53.376471: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 12:53:54.564977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_125359-gpuiznyr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFalcon-7B-instruct-BBH\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/gpuiznyr\u001b[0m\n",
            "2024-04-04:12:54:00,953 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:12:54:07,121 INFO     [__main__.py:335] Selected Tasks: ['bbh_fewshot']\n",
            "2024-04-04:12:54:07,123 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:12:54:07,123 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'tiiuae/falcon-7b-instruct', 'trust_remote_code': True, 'do_sample': True}\n",
            "2024-04-04:12:54:07,127 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "2024-04-04:12:54:07,974 WARNING  [logging.py:329] \n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.34s/it]\n",
            "2024-04-04:12:54:14,525 INFO     [evaluator.py:195] Using cache at results_dir_falcon_bbh_rank0.db\n",
            "2024-04-04:12:54:18,481 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:18,481 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:21,313 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:21,314 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:24,248 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:24,248 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:27,100 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:27,100 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:30,471 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:30,472 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:33,095 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:33,096 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:35,715 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:54:35,716 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for lukaemon/bbh contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/lukaemon/bbh\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 4.54k/4.54k [00:00<00:00, 10.2MB/s]\n",
            "Downloading readme: 100% 6.77k/6.77k [00:00<00:00, 15.1MB/s]\n",
            "Downloading data: 198kB [00:00, 37.3MB/s]        \n",
            "Generating test split: 100% 250/250 [00:00<00:00, 22028.91 examples/s]\n",
            "2024-04-04:12:55:41,820 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:41,820 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:46,113 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:46,113 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:48,769 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:48,770 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:51,386 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:51,387 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:54,310 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:55:54,311 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:01,411 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:01,411 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:06,722 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:06,722 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:10,592 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:10,592 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:16,725 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:56:16,725 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:19,866 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:19,866 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:22,753 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:22,754 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:25,772 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:57:25,773 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "Downloading data: 112kB [00:00, 76.5MB/s]        \n",
            "Generating test split: 100% 250/250 [00:00<00:00, 27232.91 examples/s]\n",
            "2024-04-04:12:58:30,983 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:30,983 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:34,347 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:34,348 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:37,019 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:37,019 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:40,097 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:40,098 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:43,224 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:43,224 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:58,279 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:58:58,279 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:59:01,250 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:59:01,250 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:59:03,881 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:59:03,882 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:12:59:03,889 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_boolean_expressions in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,889 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,889 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_multistep_arithmetic_two in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,889 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_sports_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_penguins_in_a_table in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_web_of_lies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_geometric_shapes in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_reasoning_about_colored_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,890 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_navigate in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_temporal_sequences in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_object_counting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_salient_translation_error_detection in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_date_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_causal_judgement in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,891 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_ruin_names in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_snarks in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_word_sorting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,892 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_movie_recommendation in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,893 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,893 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_formal_fallacies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,893 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_dyck_languages in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,893 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_hyperbaton in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,893 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_disambiguation_qa in its config. Manual configuration will be ignored.\n",
            "2024-04-04:12:59:03,899 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_boolean_expressions,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Evaluate the result of a random Boolean expression.\n",
            "Q: not ( ( not not True ) ) is\n",
            "A: False\n",
            "\n",
            "Q: True and False and not True and True is\n",
            "A: False\n",
            "\n",
            "Q: not not ( not ( False ) ) is\n",
            "A: True\n",
            "\n",
            "Q: not ( True ) and ( True ) is\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "False\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,899 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'not ( True ) and ( True ) is', 'target': 'False'}, arguments=('Evaluate the result of a random Boolean expression.\\nQ: not ( ( not not True ) ) is\\nA: False\\n\\nQ: True and False and not True and True is\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\nA: True\\n\\nQ: not ( True ) and ( True ) is\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_boolean_expressions', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_boolean_expressions', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,902 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_tracking_shuffled_objects_five_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\n",
            "Options:\n",
            "(A) yellow ball\n",
            "(B) blue ball\n",
            "(C) pink ball\n",
            "A: (A)\n",
            "\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\n",
            "Options:\n",
            "(A) white ball\n",
            "(B) purple ball\n",
            "(C) pink ball\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\n",
            "Throughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\n",
            "Options:\n",
            "(A) Lola\n",
            "(B) Rodrigo\n",
            "(C) Patrick\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\n",
            "Throughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\n",
            "Options:\n",
            "(A) Patrick\n",
            "(B) Sam\n",
            "(C) Jamie\n",
            "(D) Lola\n",
            "(E) Melissa\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,902 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa', 'target': '(A)'}, arguments=('A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_tracking_shuffled_objects_five_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_tracking_shuffled_objects_five_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,904 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_multistep_arithmetic_two,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Solve multi-step arithmetic problems.\n",
            "Q: ((-5 + 9 * -4 - 0) * (4 + -7 + 0 * -5)) =\n",
            "A: 123\n",
            "\n",
            "Q: ((-9 * 7 * 7 * -9) + (4 * -9 - 8 - -4)) =\n",
            "A: 3929\n",
            "\n",
            "Q: ((-3 + 5 * 8 * -4) - (9 - 8 * -7 + -9)) =\n",
            "A: -219\n",
            "\n",
            "Q: ((-1 + 2 + 9 * 5) - (-2 + -4 + -4 * -7)) =\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "24\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,904 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': '((-1 + 2 + 9 * 5) - (-2 + -4 + -4 * -7)) =', 'target': '24'}, arguments=('Solve multi-step arithmetic problems.\\nQ: ((-5 + 9 * -4 - 0) * (4 + -7 + 0 * -5)) =\\nA: 123\\n\\nQ: ((-9 * 7 * 7 * -9) + (4 * -9 - 8 - -4)) =\\nA: 3929\\n\\nQ: ((-3 + 5 * 8 * -4) - (9 - 8 * -7 + -9)) =\\nA: -219\\n\\nQ: ((-1 + 2 + 9 * 5) - (-2 + -4 + -4 * -7)) =\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_multistep_arithmetic_two', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_multistep_arithmetic_two', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,906 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_sports_understanding,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Determine whether an artificially constructed sentence relating to sports is plausible or not.\n",
            "Q: Is the following sentence plausible? \"Bam Adebayo scored a reverse layup in the Western Conference Finals.\"\n",
            "A: yes\n",
            "\n",
            "Q: Is the following sentence plausible? \"Santi Cazorla scored a touchdown.\"\n",
            "A: no\n",
            "\n",
            "Q: Is the following sentence plausible? \"DeMar DeRozan was called for the goal tend.\"\n",
            "A: yes\n",
            "\n",
            "Q: Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "no\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,906 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"', 'target': 'no'}, arguments=('Determine whether an artificially constructed sentence relating to sports is plausible or not.\\nQ: Is the following sentence plausible? \"Bam Adebayo scored a reverse layup in the Western Conference Finals.\"\\nA: yes\\n\\nQ: Is the following sentence plausible? \"Santi Cazorla scored a touchdown.\"\\nA: no\\n\\nQ: Is the following sentence plausible? \"DeMar DeRozan was called for the goal tend.\"\\nA: yes\\n\\nQ: Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_sports_understanding', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_sports_understanding', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,908 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_penguins_in_a_table,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=146); document 0; context prompt (starting on next line):    \n",
            "Answer questions about a table of penguins and their attributes.\n",
            "Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\n",
            "James, 12, 90, 12\n",
            "How many penguins are less than 8 years old?\n",
            "Options:\n",
            "(A) 1\n",
            "(B) 2\n",
            "(C) 3\n",
            "(D) 4\n",
            "(E) 5\n",
            "A: (B)\n",
            "\n",
            "Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the youngest penguin?\n",
            "Options:\n",
            "(A) Louis\n",
            "(B) Bernard\n",
            "(C) Vincent\n",
            "(D) Gwen\n",
            "(E) James\n",
            "A: (B)\n",
            "\n",
            "Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the name of the second penguin sorted by alphabetic order?\n",
            "Options:\n",
            "(A) Louis\n",
            "(B) Bernard\n",
            "(C) Vincent\n",
            "(D) Gwen\n",
            "(E) James\n",
            "A: (D)\n",
            "\n",
            "Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\n",
            "James, 12, 90, 12\n",
            "And here is a similar table, but listing giraffes:\n",
            "name, age, height (cm), weight (kg)\n",
            "Jody, 5, 430, 620\n",
            "Gladys, 10, 420, 590\n",
            "Marian, 2, 310, 410\n",
            "Donna, 9, 440, 650\n",
            "Which is the oldest penguin?\n",
            "Options:\n",
            "(A) Louis\n",
            "(B) Bernard\n",
            "(C) Vincent\n",
            "(D) Gwen\n",
            "(E) James\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(E)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,935 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\\nJames, 12, 90, 12\\nAnd here is a similar table, but listing giraffes:\\nname, age, height (cm), weight (kg)\\nJody, 5, 430, 620\\nGladys, 10, 420, 590\\nMarian, 2, 310, 410\\nDonna, 9, 440, 650\\nWhich is the oldest penguin?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James', 'target': '(E)'}, arguments=('Answer questions about a table of penguins and their attributes.\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\\nJames, 12, 90, 12\\nHow many penguins are less than 8 years old?\\nOptions:\\n(A) 1\\n(B) 2\\n(C) 3\\n(D) 4\\n(E) 5\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the youngest penguin?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the name of the second penguin sorted by alphabetic order?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (D)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\\nJames, 12, 90, 12\\nAnd here is a similar table, but listing giraffes:\\nname, age, height (cm), weight (kg)\\nJody, 5, 430, 620\\nGladys, 10, 420, 590\\nMarian, 2, 310, 410\\nDonna, 9, 440, 650\\nWhich is the oldest penguin?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_penguins_in_a_table', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_penguins_in_a_table', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,938 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_web_of_lies,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Evaluate a random boolean function expressed as a word problem.\n",
            "Q: Question: Fidel tells the truth. Jerry says Fidel tells the truth. Vina says Jerry tells the truth. Millicent says Vina lies. Raymond says Millicent lies. Does Raymond tell the truth?\n",
            "A: Yes\n",
            "\n",
            "Q: Question: Kristian lies. Millie says Kristian lies. Maybelle says Millie tells the truth. Fidel says Maybelle lies. Leda says Fidel lies. Does Leda tell the truth?\n",
            "A: Yes\n",
            "\n",
            "Q: Question: Kristian tells the truth. Michaela says Kristian lies. Raymond says Michaela tells the truth. Osvaldo says Raymond tells the truth. Jamey says Osvaldo tells the truth. Does Jamey tell the truth?\n",
            "A: No\n",
            "\n",
            "Q: Question: Sherrie tells the truth. Vernell says Sherrie tells the truth. Alexis says Vernell lies. Michaela says Alexis tells the truth. Elanor says Michaela tells the truth. Does Elanor tell the truth?\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "No\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,938 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Question: Sherrie tells the truth. Vernell says Sherrie tells the truth. Alexis says Vernell lies. Michaela says Alexis tells the truth. Elanor says Michaela tells the truth. Does Elanor tell the truth?', 'target': 'No'}, arguments=('Evaluate a random boolean function expressed as a word problem.\\nQ: Question: Fidel tells the truth. Jerry says Fidel tells the truth. Vina says Jerry tells the truth. Millicent says Vina lies. Raymond says Millicent lies. Does Raymond tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian lies. Millie says Kristian lies. Maybelle says Millie tells the truth. Fidel says Maybelle lies. Leda says Fidel lies. Does Leda tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian tells the truth. Michaela says Kristian lies. Raymond says Michaela tells the truth. Osvaldo says Raymond tells the truth. Jamey says Osvaldo tells the truth. Does Jamey tell the truth?\\nA: No\\n\\nQ: Question: Sherrie tells the truth. Vernell says Sherrie tells the truth. Alexis says Vernell lies. Michaela says Alexis tells the truth. Elanor says Michaela tells the truth. Does Elanor tell the truth?\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_web_of_lies', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_web_of_lies', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,940 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_geometric_shapes,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Name geometric shapes from their SVG paths.\n",
            "Q: This SVG path element <path d=\"M 31.00,73.00 L 32.00,59.00 L 44.00,50.00 L 49.00,41.00 L 64.00,37.00 L 71.00,55.00 L 64.00,76.00 L 52.00,61.00 L 31.00,73.00\"/> draws a\n",
            "Options:\n",
            "(A) circle\n",
            "(B) heptagon\n",
            "(C) hexagon\n",
            "(D) kite\n",
            "(E) line\n",
            "(F) octagon\n",
            "(G) pentagon\n",
            "(H) rectangle\n",
            "(I) sector\n",
            "(J) triangle\n",
            "A: (F)\n",
            "\n",
            "Q: This SVG path element <path d=\"M 14.19,26.04 L 51.43,39.21 L 58.44,36.69 L 56.63,30.17 L 48.53,26.66 L 14.19,26.04\"/> draws a\n",
            "Options:\n",
            "(A) circle\n",
            "(B) heptagon\n",
            "(C) hexagon\n",
            "(D) kite\n",
            "(E) line\n",
            "(F) octagon\n",
            "(G) pentagon\n",
            "(H) rectangle\n",
            "(I) sector\n",
            "(J) triangle\n",
            "A: (G)\n",
            "\n",
            "Q: This SVG path element <path d=\"M 41.00,43.00 L 37.00,34.00 L 41.00,33.00 L 45.00,34.00 L 41.00,43.00\"/> draws a\n",
            "Options:\n",
            "(A) circle\n",
            "(B) heptagon\n",
            "(C) hexagon\n",
            "(D) kite\n",
            "(E) line\n",
            "(F) octagon\n",
            "(G) pentagon\n",
            "(H) rectangle\n",
            "(I) sector\n",
            "(J) triangle\n",
            "A: (D)\n",
            "\n",
            "Q: This SVG path element <path d=\"M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L 45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69\"/> draws a\n",
            "Options:\n",
            "(A) circle\n",
            "(B) heptagon\n",
            "(C) hexagon\n",
            "(D) kite\n",
            "(E) line\n",
            "(F) octagon\n",
            "(G) pentagon\n",
            "(H) rectangle\n",
            "(I) sector\n",
            "(J) triangle\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(B)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,940 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'This SVG path element <path d=\"M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L 45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle', 'target': '(B)'}, arguments=('Name geometric shapes from their SVG paths.\\nQ: This SVG path element <path d=\"M 31.00,73.00 L 32.00,59.00 L 44.00,50.00 L 49.00,41.00 L 64.00,37.00 L 71.00,55.00 L 64.00,76.00 L 52.00,61.00 L 31.00,73.00\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (F)\\n\\nQ: This SVG path element <path d=\"M 14.19,26.04 L 51.43,39.21 L 58.44,36.69 L 56.63,30.17 L 48.53,26.66 L 14.19,26.04\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (G)\\n\\nQ: This SVG path element <path d=\"M 41.00,43.00 L 37.00,34.00 L 41.00,33.00 L 45.00,34.00 L 41.00,43.00\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (D)\\n\\nQ: This SVG path element <path d=\"M 55.57,80.69 L 57.38,65.80 M 57.38,65.80 L 48.90,57.46 M 48.90,57.46 L 45.58,47.78 M 45.58,47.78 L 53.25,36.07 L 66.29,48.90 L 78.69,61.09 L 55.57,80.69\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_geometric_shapes', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_geometric_shapes', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,942 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_logical_deduction_three_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A logical deduction task which requires deducing the order of a sequence of objects.\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
            "Options:\n",
            "(A) Amy finished last\n",
            "(B) Eli finished last\n",
            "(C) Eve finished last\n",
            "A: (B)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
            "Options:\n",
            "(A) The white book is the leftmost\n",
            "(B) The green book is the leftmost\n",
            "(C) The orange book is the leftmost\n",
            "A: (A)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
            "Options:\n",
            "(A) The red book is the leftmost\n",
            "(B) The gray book is the leftmost\n",
            "(C) The white book is the leftmost\n",
            "A: (C)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a blue jay, a quail, and a falcon. The falcon is to the right of the blue jay. The blue jay is to the right of the quail.\n",
            "Options:\n",
            "(A) The blue jay is the second from the left\n",
            "(B) The quail is the second from the left\n",
            "(C) The falcon is the second from the left\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:03,943 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a blue jay, a quail, and a falcon. The falcon is to the right of the blue jay. The blue jay is to the right of the quail.\\nOptions:\\n(A) The blue jay is the second from the left\\n(B) The quail is the second from the left\\n(C) The falcon is the second from the left', 'target': '(A)'}, arguments=('A logical deduction task which requires deducing the order of a sequence of objects.\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a blue jay, a quail, and a falcon. The falcon is to the right of the blue jay. The blue jay is to the right of the quail.\\nOptions:\\n(A) The blue jay is the second from the left\\n(B) The quail is the second from the left\\n(C) The falcon is the second from the left\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_logical_deduction_three_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_logical_deduction_three_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:03,945 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_reasoning_about_colored_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Answer extremely simple questions about the colors of objects on a surface.\n",
            "Q: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?\n",
            "Options:\n",
            "(A) red\n",
            "(B) orange\n",
            "(C) yellow\n",
            "(D) green\n",
            "(E) blue\n",
            "(F) brown\n",
            "(G) magenta\n",
            "(H) fuchsia\n",
            "(I) mauve\n",
            "(J) teal\n",
            "(K) turquoise\n",
            "(L) burgundy\n",
            "(M) silver\n",
            "(N) gold\n",
            "(O) black\n",
            "(P) grey\n",
            "(Q) purple\n",
            "(R) pink\n",
            "A: (E)\n",
            "\n",
            "Q: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?\n",
            "Options:\n",
            "(A) red\n",
            "(B) orange\n",
            "(C) yellow\n",
            "(D) green\n",
            "(E) blue\n",
            "(F) brown\n",
            "(G) magenta\n",
            "(H) fuchsia\n",
            "(I) mauve\n",
            "(J) teal\n",
            "(K) turquoise\n",
            "(L) burgundy\n",
            "(M) silver\n",
            "(N) gold\n",
            "(O) black\n",
            "(P) grey\n",
            "(Q) purple\n",
            "(R) pink\n",
            "A: (F)\n",
            "\n",
            "Q: On the nightstand, you see the following items arranged in a row: a teal plate, a burgundy keychain, a yellow scrunchiephone charger, an orange mug, a pink notebook, and a grey cup. How many non-orange items do you see to the left of the teal item?\n",
            "Options:\n",
            "(A) zero\n",
            "(B) one\n",
            "(C) two\n",
            "(D) three\n",
            "(E) four\n",
            "(F) five\n",
            "(G) six\n",
            "A: (A)\n",
            "\n",
            "Q: On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\n",
            "Options:\n",
            "(A) zero\n",
            "(B) one\n",
            "(C) two\n",
            "(D) three\n",
            "(E) four\n",
            "(F) five\n",
            "(G) six\n",
            "(H) seven\n",
            "(I) eight\n",
            "(J) nine\n",
            "(K) ten\n",
            "(L) eleven\n",
            "(M) twelve\n",
            "(N) thirteen\n",
            "(O) fourteen\n",
            "(P) fifteen\n",
            "(Q) sixteen\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(D)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,037 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\n(H) seven\\n(I) eight\\n(J) nine\\n(K) ten\\n(L) eleven\\n(M) twelve\\n(N) thirteen\\n(O) fourteen\\n(P) fifteen\\n(Q) sixteen', 'target': '(D)'}, arguments=('Answer extremely simple questions about the colors of objects on a surface.\\nQ: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (E)\\n\\nQ: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (F)\\n\\nQ: On the nightstand, you see the following items arranged in a row: a teal plate, a burgundy keychain, a yellow scrunchiephone charger, an orange mug, a pink notebook, and a grey cup. How many non-orange items do you see to the left of the teal item?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\nA: (A)\\n\\nQ: On the floor, there is one mauve cat toy, two purple cat toys, three grey cat toys, two mauve notebooks, three grey notebooks, three burgundy cat toys, and one purple notebook. If I remove all the notebooks from the floor, how many grey objects remain on it?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\n(H) seven\\n(I) eight\\n(J) nine\\n(K) ten\\n(L) eleven\\n(M) twelve\\n(N) thirteen\\n(O) fourteen\\n(P) fifteen\\n(Q) sixteen\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_reasoning_about_colored_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_reasoning_about_colored_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,041 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_navigate,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Given a series of navigation instructions, determine whether one would end up back at the starting point.\n",
            "Q: If you follow these instructions, do you return to the starting point? Turn left. Turn around. Turn left. Take 7 steps. Take 2 steps. Take 4 steps. Take 8 steps.\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: No\n",
            "\n",
            "Q: If you follow these instructions, do you return to the starting point? Turn around. Take 1 step. Take 6 steps. Turn around. Take 6 steps. Take 9 steps. Take 1 step.\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: No\n",
            "\n",
            "Q: If you follow these instructions, do you return to the starting point? Always face forward. Take 2 steps right. Take 9 steps left. Take 7 steps right.\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: Yes\n",
            "\n",
            "Q: If you follow these instructions, do you return to the starting point? Always face forward. Take 1 step backward. Take 9 steps left. Take 2 steps backward. Take 6 steps forward. Take 4 steps forward. Take 4 steps backward. Take 3 steps right.\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "No\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,041 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'If you follow these instructions, do you return to the starting point? Always face forward. Take 1 step backward. Take 9 steps left. Take 2 steps backward. Take 6 steps forward. Take 4 steps forward. Take 4 steps backward. Take 3 steps right.\\nOptions:\\n- Yes\\n- No', 'target': 'No'}, arguments=('Given a series of navigation instructions, determine whether one would end up back at the starting point.\\nQ: If you follow these instructions, do you return to the starting point? Turn left. Turn around. Turn left. Take 7 steps. Take 2 steps. Take 4 steps. Take 8 steps.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Turn around. Take 1 step. Take 6 steps. Turn around. Take 6 steps. Take 9 steps. Take 1 step.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Always face forward. Take 2 steps right. Take 9 steps left. Take 7 steps right.\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: If you follow these instructions, do you return to the starting point? Always face forward. Take 1 step backward. Take 9 steps left. Take 2 steps backward. Take 6 steps forward. Take 4 steps forward. Take 4 steps backward. Take 3 steps right.\\nOptions:\\n- Yes\\n- No\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_navigate', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_navigate', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,043 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_temporal_sequences,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Task description: Answer questions about which times certain events could have occurred.\n",
            "Q: Today, Emily went to the museum. Between what times could they have gone?\n",
            "We know that:\n",
            "Emily woke up at 1pm.\n",
            "Elizabeth saw Emily reading at the library from 2pm to 4pm.\n",
            "Jessica saw Emily watching a movie at the theater from 4pm to 5pm.\n",
            "Leslie saw Emily waiting at the airport from 5pm to 6pm.\n",
            "William saw Emily buying clothes at the mall from 6pm to 7pm.\n",
            "The museum was closed after 7pm.\n",
            "Between what times could Emily have gone to the museum?\n",
            "Options:\n",
            "(A) 1pm to 2pm\n",
            "(B) 6pm to 7pm\n",
            "(C) 5pm to 6pm\n",
            "(D) 2pm to 4pm\n",
            "A: (A)\n",
            "\n",
            "Q: Today, Elizabeth went to the amusement park. Between what times could they have gone?\n",
            "We know that:\n",
            "Elizabeth woke up at 7am.\n",
            "David saw Elizabeth fixing their computer at the electronic store from 1pm to 2pm.\n",
            "Sarah saw Elizabeth playing tennis at the tennis court from 2pm to 3pm.\n",
            "Susan saw Elizabeth walking towards the Statue of Liberty from 3pm to 6pm.\n",
            "Andrew saw Elizabeth taking photos near the Eiffel Tower from 6pm to 9pm.\n",
            "Emily saw Elizabeth getting a coffee at the cafe from 9pm to 10pm.\n",
            "The amusement park was closed after 10pm.\n",
            "Between what times could Elizabeth have gone to the amusement park?\n",
            "Options:\n",
            "(A) 7am to 1pm\n",
            "(B) 9pm to 10pm\n",
            "(C) 1pm to 2pm\n",
            "(D) 3pm to 6pm\n",
            "A: (A)\n",
            "\n",
            "Q: Today, Tiffany went to the beach. Between what times could they have gone?\n",
            "We know that:\n",
            "Tiffany woke up at 5am.\n",
            "Betty saw Tiffany getting a coffee at the cafe from 5am to 6am.\n",
            "Jessica saw Tiffany working at the office from 6am to 9am.\n",
            "John saw Tiffany stretching at a yoga studio from 9am to 12pm.\n",
            "Sean saw Tiffany sitting on a rooftop from 12pm to 2pm.\n",
            "Sarah saw Tiffany playing tennis at the tennis court from 2pm to 3pm.\n",
            "The beach was closed after 4pm.\n",
            "Between what times could Tiffany have gone to the beach?\n",
            "Options:\n",
            "(A) 9am to 12pm\n",
            "(B) 12pm to 2pm\n",
            "(C) 5am to 6am\n",
            "(D) 3pm to 4pm\n",
            "A: (D)\n",
            "\n",
            "Q: Today, Susan went to the coffee shop. Between what times could they have gone?\n",
            "We know that:\n",
            "Susan woke up at 7am.\n",
            "Linda saw Susan driving to the water park from 7am to 11am.\n",
            "John saw Susan buying clothes at the mall from 11am to 12pm.\n",
            "Jessica saw Susan taking photos near the Eiffel Tower from 12pm to 1pm.\n",
            "Steven saw Susan buying lunch at the deli from 1pm to 2pm.\n",
            "Thomas saw Susan reading at the library from 2pm to 6pm.\n",
            "The coffee shop was closed after 9pm.\n",
            "Between what times could Susan have gone to the coffee shop?\n",
            "Options:\n",
            "(A) 6pm to 9pm\n",
            "(B) 7am to 11am\n",
            "(C) 1pm to 2pm\n",
            "(D) 2pm to 6pm\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,044 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Today, Susan went to the coffee shop. Between what times could they have gone?\\nWe know that:\\nSusan woke up at 7am.\\nLinda saw Susan driving to the water park from 7am to 11am.\\nJohn saw Susan buying clothes at the mall from 11am to 12pm.\\nJessica saw Susan taking photos near the Eiffel Tower from 12pm to 1pm.\\nSteven saw Susan buying lunch at the deli from 1pm to 2pm.\\nThomas saw Susan reading at the library from 2pm to 6pm.\\nThe coffee shop was closed after 9pm.\\nBetween what times could Susan have gone to the coffee shop?\\nOptions:\\n(A) 6pm to 9pm\\n(B) 7am to 11am\\n(C) 1pm to 2pm\\n(D) 2pm to 6pm', 'target': '(A)'}, arguments=('Task description: Answer questions about which times certain events could have occurred.\\nQ: Today, Emily went to the museum. Between what times could they have gone?\\nWe know that:\\nEmily woke up at 1pm.\\nElizabeth saw Emily reading at the library from 2pm to 4pm.\\nJessica saw Emily watching a movie at the theater from 4pm to 5pm.\\nLeslie saw Emily waiting at the airport from 5pm to 6pm.\\nWilliam saw Emily buying clothes at the mall from 6pm to 7pm.\\nThe museum was closed after 7pm.\\nBetween what times could Emily have gone to the museum?\\nOptions:\\n(A) 1pm to 2pm\\n(B) 6pm to 7pm\\n(C) 5pm to 6pm\\n(D) 2pm to 4pm\\nA: (A)\\n\\nQ: Today, Elizabeth went to the amusement park. Between what times could they have gone?\\nWe know that:\\nElizabeth woke up at 7am.\\nDavid saw Elizabeth fixing their computer at the electronic store from 1pm to 2pm.\\nSarah saw Elizabeth playing tennis at the tennis court from 2pm to 3pm.\\nSusan saw Elizabeth walking towards the Statue of Liberty from 3pm to 6pm.\\nAndrew saw Elizabeth taking photos near the Eiffel Tower from 6pm to 9pm.\\nEmily saw Elizabeth getting a coffee at the cafe from 9pm to 10pm.\\nThe amusement park was closed after 10pm.\\nBetween what times could Elizabeth have gone to the amusement park?\\nOptions:\\n(A) 7am to 1pm\\n(B) 9pm to 10pm\\n(C) 1pm to 2pm\\n(D) 3pm to 6pm\\nA: (A)\\n\\nQ: Today, Tiffany went to the beach. Between what times could they have gone?\\nWe know that:\\nTiffany woke up at 5am.\\nBetty saw Tiffany getting a coffee at the cafe from 5am to 6am.\\nJessica saw Tiffany working at the office from 6am to 9am.\\nJohn saw Tiffany stretching at a yoga studio from 9am to 12pm.\\nSean saw Tiffany sitting on a rooftop from 12pm to 2pm.\\nSarah saw Tiffany playing tennis at the tennis court from 2pm to 3pm.\\nThe beach was closed after 4pm.\\nBetween what times could Tiffany have gone to the beach?\\nOptions:\\n(A) 9am to 12pm\\n(B) 12pm to 2pm\\n(C) 5am to 6am\\n(D) 3pm to 4pm\\nA: (D)\\n\\nQ: Today, Susan went to the coffee shop. Between what times could they have gone?\\nWe know that:\\nSusan woke up at 7am.\\nLinda saw Susan driving to the water park from 7am to 11am.\\nJohn saw Susan buying clothes at the mall from 11am to 12pm.\\nJessica saw Susan taking photos near the Eiffel Tower from 12pm to 1pm.\\nSteven saw Susan buying lunch at the deli from 1pm to 2pm.\\nThomas saw Susan reading at the library from 2pm to 6pm.\\nThe coffee shop was closed after 9pm.\\nBetween what times could Susan have gone to the coffee shop?\\nOptions:\\n(A) 6pm to 9pm\\n(B) 7am to 11am\\n(C) 1pm to 2pm\\n(D) 2pm to 6pm\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_temporal_sequences', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_temporal_sequences', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,046 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_object_counting,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Questions that involve enumerating objects and asking the model to count them.\n",
            "Q: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\n",
            "A: 6\n",
            "\n",
            "Q: I have an orange, a raspberry, two peaches, a blackberry, an apple, a grape, a nectarine, and three plums. How many fruits do I have?\n",
            "A: 11\n",
            "\n",
            "Q: I have a lettuce head, a head of broccoli, an onion, a stalk of celery, two carrots, a garlic, and a yam. How many vegetables do I have?\n",
            "A: 8\n",
            "\n",
            "Q: I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet. How many musical instruments do I have?\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "8\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,046 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet. How many musical instruments do I have?', 'target': '8'}, arguments=('Questions that involve enumerating objects and asking the model to count them.\\nQ: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\\nA: 6\\n\\nQ: I have an orange, a raspberry, two peaches, a blackberry, an apple, a grape, a nectarine, and three plums. How many fruits do I have?\\nA: 11\\n\\nQ: I have a lettuce head, a head of broccoli, an onion, a stalk of celery, two carrots, a garlic, and a yam. How many vegetables do I have?\\nA: 8\\n\\nQ: I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet. How many musical instruments do I have?\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_object_counting', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_object_counting', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,050 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_salient_translation_error_detection,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Detect the type of error in an English translation of a German source sentence.\n",
            "Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.\n",
            "Translation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.\n",
            "The translation contains an error pertaining to\n",
            "Options:\n",
            "(A) Modifiers or Adjectives\n",
            "(B) Numerical Values\n",
            "(C) Negation or Antonyms\n",
            "(D) Named Entities\n",
            "(E) Dropped Content\n",
            "(F) Facts\n",
            "A: (D)\n",
            "\n",
            "Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Auf dieser Seite sind die Baudenkmäler der oberbayerischen Großen Kreisstadt Landsberg am Lech zusammengestellt.\n",
            "Translation: On this page are compiled the architectural monuments of the town of Landsberg am Lech.\n",
            "The translation contains an error pertaining to\n",
            "Options:\n",
            "(A) Modifiers or Adjectives\n",
            "(B) Numerical Values\n",
            "(C) Negation or Antonyms\n",
            "(D) Named Entities\n",
            "(E) Dropped Content\n",
            "(F) Facts\n",
            "A: (E)\n",
            "\n",
            "Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Łeba ist eine Kleinstadt und ein Badeort im Powiat Lęborski der polnischen Woiwodschaft Pommern.\n",
            "Translation: Eba is not a small town and seaside resort in the Powiat Léborski county of the Pomeranian Voivodeship of Poland.\n",
            "The translation contains an error pertaining to\n",
            "Options:\n",
            "(A) Modifiers or Adjectives\n",
            "(B) Numerical Values\n",
            "(C) Negation or Antonyms\n",
            "(D) Named Entities\n",
            "(E) Dropped Content\n",
            "(F) Facts\n",
            "A: (C)\n",
            "\n",
            "Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Karl Borromäus Joseph Fürst von Liechtenstein war ein kaiserlicher Feldmarschall.\n",
            "Translation: Charles Borromeo Joseph Prince of Liechtenstein was an judicial field marshal.\n",
            "The translation contains an error pertaining to\n",
            "Options:\n",
            "(A) Modifiers or Adjectives\n",
            "(B) Numerical Values\n",
            "(C) Negation or Antonyms\n",
            "(D) Named Entities\n",
            "(E) Dropped Content\n",
            "(F) Facts\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,050 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Karl Borromäus Joseph Fürst von Liechtenstein war ein kaiserlicher Feldmarschall.\\nTranslation: Charles Borromeo Joseph Prince of Liechtenstein was an judicial field marshal.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts', 'target': '(A)'}, arguments=('Detect the type of error in an English translation of a German source sentence.\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.\\nTranslation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (D)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Auf dieser Seite sind die Baudenkmäler der oberbayerischen Großen Kreisstadt Landsberg am Lech zusammengestellt.\\nTranslation: On this page are compiled the architectural monuments of the town of Landsberg am Lech.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (E)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Łeba ist eine Kleinstadt und ein Badeort im Powiat Lęborski der polnischen Woiwodschaft Pommern.\\nTranslation: Eba is not a small town and seaside resort in the Powiat Léborski county of the Pomeranian Voivodeship of Poland.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (C)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Karl Borromäus Joseph Fürst von Liechtenstein war ein kaiserlicher Feldmarschall.\\nTranslation: Charles Borromeo Joseph Prince of Liechtenstein was an judicial field marshal.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_salient_translation_error_detection', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_salient_translation_error_detection', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,141 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_date_understanding,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Infer the date from context.\n",
            "Q: Today is Christmas Eve of 1937. What is the date 10 days ago in MM/DD/YYYY?\n",
            "Options:\n",
            "(A) 12/14/2026\n",
            "(B) 12/14/1950\n",
            "(C) 12/14/2007\n",
            "(D) 12/14/1937\n",
            "(E) 07/14/1938\n",
            "(F) 12/14/1988\n",
            "A: (D)\n",
            "\n",
            "Q: Tomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY?\n",
            "Options:\n",
            "(A) 09/04/2018\n",
            "(B) 11/11/2018\n",
            "(C) 08/25/2018\n",
            "(D) 11/02/2018\n",
            "(E) 11/04/2018\n",
            "A: (B)\n",
            "\n",
            "Q: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date tomorrow in MM/DD/YYYY?\n",
            "Options:\n",
            "(A) 01/11/1961\n",
            "(B) 01/03/1963\n",
            "(C) 01/18/1961\n",
            "(D) 10/14/1960\n",
            "(E) 01/03/1982\n",
            "(F) 12/03/1960\n",
            "A: (B)\n",
            "\n",
            "Q: Today is Christmas Eve of 1937. What is the date tomorrow in MM/DD/YYYY?\n",
            "Options:\n",
            "(A) 12/11/1937\n",
            "(B) 12/25/1937\n",
            "(C) 01/04/1938\n",
            "(D) 12/04/1937\n",
            "(E) 12/25/2006\n",
            "(F) 07/25/1937\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(B)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,141 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Today is Christmas Eve of 1937. What is the date tomorrow in MM/DD/YYYY?\\nOptions:\\n(A) 12/11/1937\\n(B) 12/25/1937\\n(C) 01/04/1938\\n(D) 12/04/1937\\n(E) 12/25/2006\\n(F) 07/25/1937', 'target': '(B)'}, arguments=('Infer the date from context.\\nQ: Today is Christmas Eve of 1937. What is the date 10 days ago in MM/DD/YYYY?\\nOptions:\\n(A) 12/14/2026\\n(B) 12/14/1950\\n(C) 12/14/2007\\n(D) 12/14/1937\\n(E) 07/14/1938\\n(F) 12/14/1988\\nA: (D)\\n\\nQ: Tomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY?\\nOptions:\\n(A) 09/04/2018\\n(B) 11/11/2018\\n(C) 08/25/2018\\n(D) 11/02/2018\\n(E) 11/04/2018\\nA: (B)\\n\\nQ: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date tomorrow in MM/DD/YYYY?\\nOptions:\\n(A) 01/11/1961\\n(B) 01/03/1963\\n(C) 01/18/1961\\n(D) 10/14/1960\\n(E) 01/03/1982\\n(F) 12/03/1960\\nA: (B)\\n\\nQ: Today is Christmas Eve of 1937. What is the date tomorrow in MM/DD/YYYY?\\nOptions:\\n(A) 12/11/1937\\n(B) 12/25/1937\\n(C) 01/04/1938\\n(D) 12/04/1937\\n(E) 12/25/2006\\n(F) 07/25/1937\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_date_understanding', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_date_understanding', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,143 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_causal_judgement,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=187); document 0; context prompt (starting on next line):    \n",
            "Answer questions about causal attribution.\n",
            "Q: How would a typical person answer each of the following questions about causation?\n",
            "Frank T., had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor's body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: No\n",
            "\n",
            "Q: How would a typical person answer each of the following questions about causation?\n",
            "Suzy and Billy are working on a project that is very important for our nation's security. The boss tells them both: \"Be sure that you are here at exactly 9 am. It is absolutely essential that you arrive at that time.\" Both Billy and Suzy arrive at 9 am. As it happens, there was a motion detector installed in the room where they arrived. The motion detector was set up to be triggered if at least one person appeared in the room at the same time. So the motion detector went off. Did Billy cause the motion detector to go off?\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: Yes\n",
            "\n",
            "Q: How would a typical person answer each of the following questions about causation?\n",
            "George and his sister Lena reunite at their parents' house for Thanksgiving. Whereas George just got into medical school, Lena is unhappy in her marriage and recently lost her job. Over the course of the day, George and Lena get into a number of heated arguments. Later in the afternoon they play a game of darts. They split the first two games, and the third game is close until the end. Who will win comes down to George's last shot. If he hits a high point region, he wins; if he hits a low point region, Lena wins. George thinks of the difficult time Lena is having, and he really wants to let her win. He aims the dart at the low point region. He sets up his shot and the dart lands in the low point region. After his shot, Lena wins the game and is very happy. Did George hit the low point region intentionally?\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A: Yes\n",
            "\n",
            "Q: How would a typical person answer each of the following questions about causation?\n",
            "A machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\n",
            "Options:\n",
            "- Yes\n",
            "- No\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "No\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,144 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No', 'target': 'No'}, arguments=('Answer questions about causal attribution.\\nQ: How would a typical person answer each of the following questions about causation?\\nFrank T., had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor\\'s body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nSuzy and Billy are working on a project that is very important for our nation\\'s security. The boss tells them both: \"Be sure that you are here at exactly 9 am. It is absolutely essential that you arrive at that time.\" Both Billy and Suzy arrive at 9 am. As it happens, there was a motion detector installed in the room where they arrived. The motion detector was set up to be triggered if at least one person appeared in the room at the same time. So the motion detector went off. Did Billy cause the motion detector to go off?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nGeorge and his sister Lena reunite at their parents\\' house for Thanksgiving. Whereas George just got into medical school, Lena is unhappy in her marriage and recently lost her job. Over the course of the day, George and Lena get into a number of heated arguments. Later in the afternoon they play a game of darts. They split the first two games, and the third game is close until the end. Who will win comes down to George\\'s last shot. If he hits a high point region, he wins; if he hits a low point region, Lena wins. George thinks of the difficult time Lena is having, and he really wants to let her win. He aims the dart at the low point region. He sets up his shot and the dart lands in the low point region. After his shot, Lena wins the game and is very happy. Did George hit the low point region intentionally?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_causal_judgement', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_causal_judgement', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,146 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_ruin_names,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Select the humorous edit that 'ruins' the input movie or musical artist name.\n",
            "Q: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?\n",
            "Options:\n",
            "(A) whitesnape\n",
            "(B) whitesnapke\n",
            "(C) whitesnuake\n",
            "(D) mwhitesnake\n",
            "A: (A)\n",
            "\n",
            "Q: Which of the following is a humorous edit of this artist or movie name: 'one of our dinosaurs is missing'?\n",
            "Options:\n",
            "(A) ofne of our dinosaurs is missing\n",
            "(B) one af our dinosaurs is missing\n",
            "(C) one of our dinosaurs is pissing\n",
            "(D) one of our dinosaur is missing\n",
            "A: (C)\n",
            "\n",
            "Q: Which of the following is a humorous edit of this artist or movie name: 'counting crows'?\n",
            "Options:\n",
            "(A) countingy crows\n",
            "(B) counting cows\n",
            "(C) courting crows\n",
            "(D) coutnting crows\n",
            "A: (B)\n",
            "\n",
            "Q: Which of the following is a humorous edit of this artist or movie name: 'rain man'?\n",
            "Options:\n",
            "(A) ruin man\n",
            "(B) rains man\n",
            "(C) rain men\n",
            "(D) rainmman\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,146 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': \"Which of the following is a humorous edit of this artist or movie name: 'rain man'?\\nOptions:\\n(A) ruin man\\n(B) rains man\\n(C) rain men\\n(D) rainmman\", 'target': '(A)'}, arguments=(\"Select the humorous edit that 'ruins' the input movie or musical artist name.\\nQ: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?\\nOptions:\\n(A) whitesnape\\n(B) whitesnapke\\n(C) whitesnuake\\n(D) mwhitesnake\\nA: (A)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'one of our dinosaurs is missing'?\\nOptions:\\n(A) ofne of our dinosaurs is missing\\n(B) one af our dinosaurs is missing\\n(C) one of our dinosaurs is pissing\\n(D) one of our dinosaur is missing\\nA: (C)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'counting crows'?\\nOptions:\\n(A) countingy crows\\n(B) counting cows\\n(C) courting crows\\n(D) coutnting crows\\nA: (B)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'rain man'?\\nOptions:\\n(A) ruin man\\n(B) rains man\\n(C) rain men\\n(D) rainmman\\nA:\", {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_ruin_names', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_ruin_names', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,148 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_snarks,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=178); document 0; context prompt (starting on next line):    \n",
            "Determine which of two sentences is sarcastic.\n",
            "\n",
            "According to Cambridge University Dictionary, sarcasm is \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\n",
            "Q: Which statement is sarcastic?\n",
            "Options:\n",
            "(A) Yes, because having interests and actively researching them is a huge waste\n",
            "(B) Yes, because having interests and actively researching them is a huge deal\n",
            "A: (A)\n",
            "\n",
            "Q: Which statement is sarcastic?\n",
            "Options:\n",
            "(A) No one is going to disagree with you on this. Avoiding ad hominem attacks really help your case\n",
            "(B) No one is going to disagree with you on this. Ad hominem attacks really help your case\n",
            "A: (B)\n",
            "\n",
            "Q: Which statement is sarcastic?\n",
            "Options:\n",
            "(A) Consistency in the league's punishments? What do you think this is supposed to be, politics?\n",
            "(B) Consistency in the league's punishments? What do you think this is supposed to be, moral?\n",
            "A: (A)\n",
            "\n",
            "Q: Which statement is sarcastic?\n",
            "Options:\n",
            "(A) He's a generous person, trying to promote a charity stream that has raised millions to help kids in need\n",
            "(B) He's a terrible person, trying to promote a charity stream that has raised millions to help kids in need\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(B)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,148 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': \"Which statement is sarcastic?\\nOptions:\\n(A) He's a generous person, trying to promote a charity stream that has raised millions to help kids in need\\n(B) He's a terrible person, trying to promote a charity stream that has raised millions to help kids in need\", 'target': '(B)'}, arguments=('Determine which of two sentences is sarcastic.\\n\\nAccording to Cambridge University Dictionary, sarcasm is \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone\\'s feelings or to criticize something in a humorous way.\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) Yes, because having interests and actively researching them is a huge waste\\n(B) Yes, because having interests and actively researching them is a huge deal\\nA: (A)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) No one is going to disagree with you on this. Avoiding ad hominem attacks really help your case\\n(B) No one is going to disagree with you on this. Ad hominem attacks really help your case\\nA: (B)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) Consistency in the league\\'s punishments? What do you think this is supposed to be, politics?\\n(B) Consistency in the league\\'s punishments? What do you think this is supposed to be, moral?\\nA: (A)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) He\\'s a generous person, trying to promote a charity stream that has raised millions to help kids in need\\n(B) He\\'s a terrible person, trying to promote a charity stream that has raised millions to help kids in need\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_snarks', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_snarks', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,244 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_word_sorting,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Sort a list of words.\n",
            "Q: Sort the following words alphabetically: List: oven costume counterpart\n",
            "A: costume counterpart oven\n",
            "\n",
            "Q: Sort the following words alphabetically: List: hypochlorite ponderosa phone credulity\n",
            "A: credulity hypochlorite phone ponderosa\n",
            "\n",
            "Q: Sort the following words alphabetically: List: newt arson parthia seismography mugho aspect census\n",
            "A: arson aspect census mugho newt parthia seismography\n",
            "\n",
            "Q: Sort the following words alphabetically: List: syndrome therefrom\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "syndrome therefrom\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,244 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Sort the following words alphabetically: List: syndrome therefrom', 'target': 'syndrome therefrom'}, arguments=('Sort a list of words.\\nQ: Sort the following words alphabetically: List: oven costume counterpart\\nA: costume counterpart oven\\n\\nQ: Sort the following words alphabetically: List: hypochlorite ponderosa phone credulity\\nA: credulity hypochlorite phone ponderosa\\n\\nQ: Sort the following words alphabetically: List: newt arson parthia seismography mugho aspect census\\nA: arson aspect census mugho newt parthia seismography\\n\\nQ: Sort the following words alphabetically: List: syndrome therefrom\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_word_sorting', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_word_sorting', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,246 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_logical_deduction_five_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A logical deduction task which requires deducing the order of a sequence of objects.\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
            "Options:\n",
            "(A) Amy finished last\n",
            "(B) Eli finished last\n",
            "(C) Eve finished last\n",
            "A: (B)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
            "Options:\n",
            "(A) The white book is the leftmost\n",
            "(B) The green book is the leftmost\n",
            "(C) The orange book is the leftmost\n",
            "A: (A)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
            "Options:\n",
            "(A) The red book is the leftmost\n",
            "(B) The gray book is the leftmost\n",
            "(C) The white book is the leftmost\n",
            "A: (C)\n",
            "\n",
            "Q: The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\n",
            "Options:\n",
            "(A) The quail is the rightmost\n",
            "(B) The owl is the rightmost\n",
            "(C) The raven is the rightmost\n",
            "(D) The falcon is the rightmost\n",
            "(E) The robin is the rightmost\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,247 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\\nOptions:\\n(A) The quail is the rightmost\\n(B) The owl is the rightmost\\n(C) The raven is the rightmost\\n(D) The falcon is the rightmost\\n(E) The robin is the rightmost', 'target': '(A)'}, arguments=('A logical deduction task which requires deducing the order of a sequence of objects.\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\\nOptions:\\n(A) The quail is the rightmost\\n(B) The owl is the rightmost\\n(C) The raven is the rightmost\\n(D) The falcon is the rightmost\\n(E) The robin is the rightmost\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_logical_deduction_five_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_logical_deduction_five_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,249 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_logical_deduction_seven_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A logical deduction task which requires deducing the order of a sequence of objects.\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\n",
            "Options:\n",
            "(A) Amy finished last\n",
            "(B) Eli finished last\n",
            "(C) Eve finished last\n",
            "A: (B)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\n",
            "Options:\n",
            "(A) The white book is the leftmost\n",
            "(B) The green book is the leftmost\n",
            "(C) The orange book is the leftmost\n",
            "A: (A)\n",
            "\n",
            "Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\n",
            "Options:\n",
            "(A) The red book is the leftmost\n",
            "(B) The gray book is the leftmost\n",
            "(C) The white book is the leftmost\n",
            "A: (C)\n",
            "\n",
            "Q: The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\n",
            "Options:\n",
            "(A) Ana finished third\n",
            "(B) Eve finished third\n",
            "(C) Ada finished third\n",
            "(D) Dan finished third\n",
            "(E) Rob finished third\n",
            "(F) Amy finished third\n",
            "(G) Joe finished third\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(D)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,249 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\\nOptions:\\n(A) Ana finished third\\n(B) Eve finished third\\n(C) Ada finished third\\n(D) Dan finished third\\n(E) Rob finished third\\n(F) Amy finished third\\n(G) Joe finished third', 'target': '(D)'}, arguments=('A logical deduction task which requires deducing the order of a sequence of objects.\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\\nOptions:\\n(A) Ana finished third\\n(B) Eve finished third\\n(C) Ada finished third\\n(D) Dan finished third\\n(E) Rob finished third\\n(F) Amy finished third\\n(G) Joe finished third\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_logical_deduction_seven_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_logical_deduction_seven_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,251 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_tracking_shuffled_objects_three_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\n",
            "Options:\n",
            "(A) yellow ball\n",
            "(B) blue ball\n",
            "(C) pink ball\n",
            "A: (A)\n",
            "\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\n",
            "Options:\n",
            "(A) white ball\n",
            "(B) purple ball\n",
            "(C) pink ball\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\n",
            "Throughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\n",
            "Options:\n",
            "(A) Lola\n",
            "(B) Rodrigo\n",
            "(C) Patrick\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets Frankenstein, and Claire gets Lolita.\n",
            "As the semester proceeds, they start trading around the new books. First, Claire and Bob swap books. Then, Bob and Alice swap books. Finally, Claire and Bob swap books. At the end of the semester, Bob has\n",
            "Options:\n",
            "(A) Ulysses\n",
            "(B) Frankenstein\n",
            "(C) Lolita\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(B)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,343 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets Frankenstein, and Claire gets Lolita.\\nAs the semester proceeds, they start trading around the new books. First, Claire and Bob swap books. Then, Bob and Alice swap books. Finally, Claire and Bob swap books. At the end of the semester, Bob has\\nOptions:\\n(A) Ulysses\\n(B) Frankenstein\\n(C) Lolita', 'target': '(B)'}, arguments=('A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets Frankenstein, and Claire gets Lolita.\\nAs the semester proceeds, they start trading around the new books. First, Claire and Bob swap books. Then, Bob and Alice swap books. Finally, Claire and Bob swap books. At the end of the semester, Bob has\\nOptions:\\n(A) Ulysses\\n(B) Frankenstein\\n(C) Lolita\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_tracking_shuffled_objects_three_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_tracking_shuffled_objects_three_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,345 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_movie_recommendation,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Recommend movies similar to the given list of movies.\n",
            "Q: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\n",
            "Options:\n",
            "(A) Tetsuo\n",
            "(B) the Ironman\n",
            "(C) The Princess Bride\n",
            "(D) The Barkley Marathons The Race That Eats Its Young\n",
            "(E) Bug\n",
            "A: (C)\n",
            "\n",
            "Q: Find a movie similar to Twister, The Silence of the Lambs, Independence Day, Braveheart:\n",
            "Options:\n",
            "(A) They Shoot Horses\n",
            "(B) Don't They\n",
            "(C) Forrest Gump\n",
            "(D) The Salton Sea\n",
            "(E) Extreme Days\n",
            "A: (C)\n",
            "\n",
            "Q: Find a movie similar to Minority Report, Total Recall, Inside Out, Forrest Gump:\n",
            "Options:\n",
            "(A) Phenomena\n",
            "(B) Lilting\n",
            "(C) Catwoman\n",
            "(D) Edge of Tomorrow\n",
            "A: (D)\n",
            "\n",
            "Q: Find a movie similar to Batman, The Mask, The Fugitive, Pretty Woman:\n",
            "Options:\n",
            "(A) The Front Page\n",
            "(B) Maelstrom\n",
            "(C) The Lion King\n",
            "(D) Lamerica\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(C)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,345 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Find a movie similar to Batman, The Mask, The Fugitive, Pretty Woman:\\nOptions:\\n(A) The Front Page\\n(B) Maelstrom\\n(C) The Lion King\\n(D) Lamerica', 'target': '(C)'}, arguments=(\"Recommend movies similar to the given list of movies.\\nQ: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\\nOptions:\\n(A) Tetsuo\\n(B) the Ironman\\n(C) The Princess Bride\\n(D) The Barkley Marathons The Race That Eats Its Young\\n(E) Bug\\nA: (C)\\n\\nQ: Find a movie similar to Twister, The Silence of the Lambs, Independence Day, Braveheart:\\nOptions:\\n(A) They Shoot Horses\\n(B) Don't They\\n(C) Forrest Gump\\n(D) The Salton Sea\\n(E) Extreme Days\\nA: (C)\\n\\nQ: Find a movie similar to Minority Report, Total Recall, Inside Out, Forrest Gump:\\nOptions:\\n(A) Phenomena\\n(B) Lilting\\n(C) Catwoman\\n(D) Edge of Tomorrow\\nA: (D)\\n\\nQ: Find a movie similar to Batman, The Mask, The Fugitive, Pretty Woman:\\nOptions:\\n(A) The Front Page\\n(B) Maelstrom\\n(C) The Lion King\\n(D) Lamerica\\nA:\", {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_movie_recommendation', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_movie_recommendation', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,347 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_tracking_shuffled_objects_seven_objects,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\n",
            "Options:\n",
            "(A) yellow ball\n",
            "(B) blue ball\n",
            "(C) pink ball\n",
            "A: (A)\n",
            "\n",
            "Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\n",
            "As the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\n",
            "Options:\n",
            "(A) white ball\n",
            "(B) purple ball\n",
            "(C) pink ball\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\n",
            "Throughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\n",
            "Options:\n",
            "(A) Lola\n",
            "(B) Rodrigo\n",
            "(C) Patrick\n",
            "A: (C)\n",
            "\n",
            "Q: Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing striker, Bob is playing right winger, Claire is playing left winger, Dave is playing benchwarmer, Eve is playing goalkeeper, Fred is playing center midfielder, and Gertrude is playing cheerleader.\n",
            "As the game progresses, pairs of players occasionally swap positions. First, Eve and Claire trade positions. Then, Gertrude and Alice trade positions. Then, Fred and Bob trade positions. Then, Dave and Fred trade positions. Then, Fred and Bob trade positions. Then, Bob and Eve trade positions. Finally, Claire and Alice trade positions. At the end of the match, Gertrude is playing\n",
            "Options:\n",
            "(A) striker\n",
            "(B) right winger\n",
            "(C) left winger\n",
            "(D) benchwarmer\n",
            "(E) goalkeeper\n",
            "(F) center midfielder\n",
            "(G) cheerleader\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,348 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing striker, Bob is playing right winger, Claire is playing left winger, Dave is playing benchwarmer, Eve is playing goalkeeper, Fred is playing center midfielder, and Gertrude is playing cheerleader.\\nAs the game progresses, pairs of players occasionally swap positions. First, Eve and Claire trade positions. Then, Gertrude and Alice trade positions. Then, Fred and Bob trade positions. Then, Dave and Fred trade positions. Then, Fred and Bob trade positions. Then, Bob and Eve trade positions. Finally, Claire and Alice trade positions. At the end of the match, Gertrude is playing\\nOptions:\\n(A) striker\\n(B) right winger\\n(C) left winger\\n(D) benchwarmer\\n(E) goalkeeper\\n(F) center midfielder\\n(G) cheerleader', 'target': '(A)'}, arguments=('A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing striker, Bob is playing right winger, Claire is playing left winger, Dave is playing benchwarmer, Eve is playing goalkeeper, Fred is playing center midfielder, and Gertrude is playing cheerleader.\\nAs the game progresses, pairs of players occasionally swap positions. First, Eve and Claire trade positions. Then, Gertrude and Alice trade positions. Then, Fred and Bob trade positions. Then, Dave and Fred trade positions. Then, Fred and Bob trade positions. Then, Bob and Eve trade positions. Finally, Claire and Alice trade positions. At the end of the match, Gertrude is playing\\nOptions:\\n(A) striker\\n(B) right winger\\n(C) left winger\\n(D) benchwarmer\\n(E) goalkeeper\\n(F) center midfielder\\n(G) cheerleader\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_tracking_shuffled_objects_seven_objects', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_tracking_shuffled_objects_seven_objects', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,350 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_formal_fallacies,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Distinguish deductively valid arguments from formal fallacies.\n",
            "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy.\"\n",
            "Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
            "Options:\n",
            "- valid\n",
            "- invalid\n",
            "A: valid\n",
            "\n",
            "Q: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: Whoever is not a great-grandfather of Clyde is a stepbrother of Brian. Being an ancestor of Dana is sufficient for not being a great-grandfather of Clyde. We may conclude: Everyone who is an ancestor of Dana is a stepbrother of Brian, too.\"\n",
            "Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
            "Options:\n",
            "- valid\n",
            "- invalid\n",
            "A: valid\n",
            "\n",
            "Q: \"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: Every infrequent user of Paul Mitchell shampoo is either a rare consumer of Nioxin shampoo or a loyal buyer of Caress soap, or both. No regular consumer of Lush soap is a rare consumer of Nioxin shampoo and, in the same time, a loyal buyer of Caress soap. It follows that whoever is an infrequent user of Paul Mitchell shampoo is not a regular consumer of Lush soap.\"\n",
            "Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
            "Options:\n",
            "- valid\n",
            "- invalid\n",
            "A: invalid\n",
            "\n",
            "Q: \"Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla. In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra.\"\n",
            "Is the argument, given the explicitly stated premises, deductively valid or invalid?\n",
            "Options:\n",
            "- valid \n",
            "- invalid\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "invalid\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,350 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': '\"Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla. In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra.\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid \\n- invalid', 'target': 'invalid'}, arguments=('Distinguish deductively valid arguments from formal fallacies.\\nQ: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy.\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: Whoever is not a great-grandfather of Clyde is a stepbrother of Brian. Being an ancestor of Dana is sufficient for not being a great-grandfather of Clyde. We may conclude: Everyone who is an ancestor of Dana is a stepbrother of Brian, too.\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: Every infrequent user of Paul Mitchell shampoo is either a rare consumer of Nioxin shampoo or a loyal buyer of Caress soap, or both. No regular consumer of Lush soap is a rare consumer of Nioxin shampoo and, in the same time, a loyal buyer of Caress soap. It follows that whoever is an infrequent user of Paul Mitchell shampoo is not a regular consumer of Lush soap.\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: invalid\\n\\nQ: \"Here comes a perfectly valid argument: First of all, whoever is a schoolmate of Sondra is not a stepsister of Pricilla. In consequence, whoever is not a stepsister of Pricilla is a schoolmate of Sondra.\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid \\n- invalid\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_formal_fallacies', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_formal_fallacies', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,353 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_dyck_languages,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Correctly close a Dyck-n word.\n",
            "Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ { [\n",
            "A: ] } ]\n",
            "\n",
            "Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < > ( ( [ [ ( { } ) [ < > ] ]\n",
            "A: ] ) )\n",
            "\n",
            "Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ < [ { < [ ] < { } > > } ] > { { ( ) } { < [ < > ] > }\n",
            "A: } ] >\n",
            "\n",
            "Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ [\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "] ]\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,353 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ [', 'target': '] ]'}, arguments=('Correctly close a Dyck-n word.\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ { [\\nA: ] } ]\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < > ( ( [ [ ( { } ) [ < > ] ]\\nA: ] ) )\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ < [ { < [ ] < { } > > } ] > { { ( ) } { < [ < > ] > }\\nA: } ] >\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ [\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_dyck_languages', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_dyck_languages', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,447 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_hyperbaton,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Order adjectives correctly in English sentences.\n",
            "Q: Which sentence has the correct adjective order:\n",
            "Options:\n",
            "(A) rubber terrible ship\n",
            "(B) terrible rubber ship\n",
            "A: (B)\n",
            "\n",
            "Q: Which sentence has the correct adjective order:\n",
            "Options:\n",
            "(A) repulsive small Brazilian exercise ship\n",
            "(B) Brazilian repulsive exercise small ship\n",
            "A: (A)\n",
            "\n",
            "Q: Which sentence has the correct adjective order:\n",
            "Options:\n",
            "(A) blue gold wonderful square shoe\n",
            "(B) wonderful square blue gold shoe\n",
            "A: (B)\n",
            "\n",
            "Q: Which sentence has the correct adjective order:\n",
            "Options:\n",
            "(A) midsize old grey Brazilian sweater\n",
            "(B) midsize grey Brazilian old sweater\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,447 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'Which sentence has the correct adjective order:\\nOptions:\\n(A) midsize old grey Brazilian sweater\\n(B) midsize grey Brazilian old sweater', 'target': '(A)'}, arguments=('Order adjectives correctly in English sentences.\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) rubber terrible ship\\n(B) terrible rubber ship\\nA: (B)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) repulsive small Brazilian exercise ship\\n(B) Brazilian repulsive exercise small ship\\nA: (A)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) blue gold wonderful square shoe\\n(B) wonderful square blue gold shoe\\nA: (B)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) midsize old grey Brazilian sweater\\n(B) midsize grey Brazilian old sweater\\nA:', {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_hyperbaton', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_hyperbaton', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,449 INFO     [evaluator_utils.py:138] Task: ConfigurableTask(task_name=bbh_fewshot_disambiguation_qa,group_name=bbh_fewshot,output_type=generate_until,num_fewshot=0,num_samples=250); document 0; context prompt (starting on next line):    \n",
            "Clarify the meaning of sentences with ambiguous pronouns.\n",
            "Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\n",
            "Sentence: The chief told the counselor that they took the day off.\n",
            "Options:\n",
            "(A) The chief took the day off\n",
            "(B) The counselor took the day off\n",
            "(C) Ambiguous\n",
            "A: (A)\n",
            "\n",
            "Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\n",
            "Sentence: The manager sent a message to the secretary, but he didn't reply yet.\n",
            "Options:\n",
            "(A) The secretary didn't reply yet\n",
            "(B) The manager didn't reply yet\n",
            "(C) Ambiguous\n",
            "A: (A)\n",
            "\n",
            "Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\n",
            "Sentence: Bailey will plan to meet the director at his office\n",
            "Options:\n",
            "(A) It will be Bailey's office\n",
            "(B) It will be the director's office\n",
            "(C) Ambiguous\n",
            "A: (C)\n",
            "\n",
            "Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\n",
            "Sentence: The patient was referred to the specialist because he had a rare skin condition.\n",
            "Options:\n",
            "(A) The patient had a skin condition\n",
            "(B) The specialist had a skin condition\n",
            "(C) Ambiguous\n",
            "A:\n",
            "(end of prompt on previous line)\n",
            "target string or answer choice index (starting on next line):\n",
            "(A)\n",
            "(end of target on previous line)\n",
            "2024-04-04:12:59:04,449 INFO     [evaluator_utils.py:142] Request: Instance(request_type='generate_until', doc={'input': 'In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The patient was referred to the specialist because he had a rare skin condition.\\nOptions:\\n(A) The patient had a skin condition\\n(B) The specialist had a skin condition\\n(C) Ambiguous', 'target': '(A)'}, arguments=(\"Clarify the meaning of sentences with ambiguous pronouns.\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The chief told the counselor that they took the day off.\\nOptions:\\n(A) The chief took the day off\\n(B) The counselor took the day off\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The manager sent a message to the secretary, but he didn't reply yet.\\nOptions:\\n(A) The secretary didn't reply yet\\n(B) The manager didn't reply yet\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: Bailey will plan to meet the director at his office\\nOptions:\\n(A) It will be Bailey's office\\n(B) It will be the director's office\\n(C) Ambiguous\\nA: (C)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The patient was referred to the specialist because he had a rare skin condition.\\nOptions:\\n(A) The patient had a skin condition\\n(B) The specialist had a skin condition\\n(C) Ambiguous\\nA:\", {'until': ['</s>', 'Q', '\\n\\n'], 'do_sample': False, 'temperature': 0.0}), idx=0, metadata=('bbh_fewshot_disambiguation_qa', 0, 1), resps=[], filtered_resps={}, task_name='bbh_fewshot_disambiguation_qa', doc_id=0, repeats=1)\n",
            "2024-04-04:12:59:04,450 INFO     [evaluator.py:379] Running generate_until requests\n",
            "2024-04-04:12:59:04,452 INFO     [model.py:225] Loading 'generate_until' responses from cache 'results_dir_falcon_bbh_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6511/6511 [00:00<00:00, 6849.72it/s]\n",
            "2024-04-04:12:59:05,403 INFO     [model.py:249] Cached requests: 0, Requests remaining: 6511\n",
            "Running generate_until requests:   0% 0/6511 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 6511/6511 [1:14:55<00:00,  1.45it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"bbh_fewshot\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \"bbh_fewshot\"\n",
            "    },\n",
            "    \"bbh_fewshot_boolean_expressions\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_boolean_expressions\"\n",
            "    },\n",
            "    \"bbh_fewshot_causal_judgement\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_causal_judgement\"\n",
            "    },\n",
            "    \"bbh_fewshot_date_understanding\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_date_understanding\"\n",
            "    },\n",
            "    \"bbh_fewshot_disambiguation_qa\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_disambiguation_qa\"\n",
            "    },\n",
            "    \"bbh_fewshot_dyck_languages\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_dyck_languages\"\n",
            "    },\n",
            "    \"bbh_fewshot_formal_fallacies\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_formal_fallacies\"\n",
            "    },\n",
            "    \"bbh_fewshot_geometric_shapes\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_geometric_shapes\"\n",
            "    },\n",
            "    \"bbh_fewshot_hyperbaton\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_hyperbaton\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_five_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_seven_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_three_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_movie_recommendation\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_movie_recommendation\"\n",
            "    },\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_multistep_arithmetic_two\"\n",
            "    },\n",
            "    \"bbh_fewshot_navigate\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_navigate\"\n",
            "    },\n",
            "    \"bbh_fewshot_object_counting\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_object_counting\"\n",
            "    },\n",
            "    \"bbh_fewshot_penguins_in_a_table\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_penguins_in_a_table\"\n",
            "    },\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_reasoning_about_colored_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_ruin_names\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_ruin_names\"\n",
            "    },\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_salient_translation_error_detection\"\n",
            "    },\n",
            "    \"bbh_fewshot_snarks\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_snarks\"\n",
            "    },\n",
            "    \"bbh_fewshot_sports_understanding\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_sports_understanding\"\n",
            "    },\n",
            "    \"bbh_fewshot_temporal_sequences\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_temporal_sequences\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_five_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_seven_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_three_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_web_of_lies\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_web_of_lies\"\n",
            "    },\n",
            "    \"bbh_fewshot_word_sorting\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \" - bbh_fewshot_word_sorting\"\n",
            "    }\n",
            "  },\n",
            "  \"groups\": {\n",
            "    \"bbh_fewshot\": {\n",
            "      \"exact_match,none\": 0.0,\n",
            "      \"exact_match_stderr,none\": 0.0,\n",
            "      \"alias\": \"bbh_fewshot\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"bbh_fewshot\": [\n",
            "      \"bbh_fewshot_boolean_expressions\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"bbh_fewshot_multistep_arithmetic_two\",\n",
            "      \"bbh_fewshot_sports_understanding\",\n",
            "      \"bbh_fewshot_penguins_in_a_table\",\n",
            "      \"bbh_fewshot_web_of_lies\",\n",
            "      \"bbh_fewshot_geometric_shapes\",\n",
            "      \"bbh_fewshot_logical_deduction_three_objects\",\n",
            "      \"bbh_fewshot_reasoning_about_colored_objects\",\n",
            "      \"bbh_fewshot_navigate\",\n",
            "      \"bbh_fewshot_temporal_sequences\",\n",
            "      \"bbh_fewshot_object_counting\",\n",
            "      \"bbh_fewshot_salient_translation_error_detection\",\n",
            "      \"bbh_fewshot_date_understanding\",\n",
            "      \"bbh_fewshot_causal_judgement\",\n",
            "      \"bbh_fewshot_ruin_names\",\n",
            "      \"bbh_fewshot_snarks\",\n",
            "      \"bbh_fewshot_word_sorting\",\n",
            "      \"bbh_fewshot_logical_deduction_five_objects\",\n",
            "      \"bbh_fewshot_logical_deduction_seven_objects\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_three_objects\",\n",
            "      \"bbh_fewshot_movie_recommendation\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_seven_objects\",\n",
            "      \"bbh_fewshot_formal_fallacies\",\n",
            "      \"bbh_fewshot_dyck_languages\",\n",
            "      \"bbh_fewshot_hyperbaton\",\n",
            "      \"bbh_fewshot_disambiguation_qa\"\n",
            "    ]\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"bbh_fewshot_boolean_expressions\": {\n",
            "      \"task\": \"bbh_fewshot_boolean_expressions\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"boolean_expressions\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: not ( ( not not True ) ) is\\nA: False\\n\\nQ: True and False and not True and True is\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\nA: True\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Evaluate the result of a random Boolean expression.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_causal_judgement\": {\n",
            "      \"task\": \"bbh_fewshot_causal_judgement\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"causal_judgement\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: How would a typical person answer each of the following questions about causation?\\nFrank T., had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor's body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nSuzy and Billy are working on a project that is very important for our nation's security. The boss tells them both: \\\"Be sure that you are here at exactly 9 am. It is absolutely essential that you arrive at that time.\\\" Both Billy and Suzy arrive at 9 am. As it happens, there was a motion detector installed in the room where they arrived. The motion detector was set up to be triggered if at least one person appeared in the room at the same time. So the motion detector went off. Did Billy cause the motion detector to go off?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nGeorge and his sister Lena reunite at their parents' house for Thanksgiving. Whereas George just got into medical school, Lena is unhappy in her marriage and recently lost her job. Over the course of the day, George and Lena get into a number of heated arguments. Later in the afternoon they play a game of darts. They split the first two games, and the third game is close until the end. Who will win comes down to George's last shot. If he hits a high point region, he wins; if he hits a low point region, Lena wins. George thinks of the difficult time Lena is having, and he really wants to let her win. He aims the dart at the low point region. He sets up his shot and the dart lands in the low point region. After his shot, Lena wins the game and is very happy. Did George hit the low point region intentionally?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer questions about causal attribution.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_date_understanding\": {\n",
            "      \"task\": \"bbh_fewshot_date_understanding\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"date_understanding\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Today is Christmas Eve of 1937. What is the date 10 days ago in MM/DD/YYYY?\\nOptions:\\n(A) 12/14/2026\\n(B) 12/14/1950\\n(C) 12/14/2007\\n(D) 12/14/1937\\n(E) 07/14/1938\\n(F) 12/14/1988\\nA: (D)\\n\\nQ: Tomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY?\\nOptions:\\n(A) 09/04/2018\\n(B) 11/11/2018\\n(C) 08/25/2018\\n(D) 11/02/2018\\n(E) 11/04/2018\\nA: (B)\\n\\nQ: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date tomorrow in MM/DD/YYYY?\\nOptions:\\n(A) 01/11/1961\\n(B) 01/03/1963\\n(C) 01/18/1961\\n(D) 10/14/1960\\n(E) 01/03/1982\\n(F) 12/03/1960\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Infer the date from context.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_disambiguation_qa\": {\n",
            "      \"task\": \"bbh_fewshot_disambiguation_qa\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"disambiguation_qa\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The chief told the counselor that they took the day off.\\nOptions:\\n(A) The chief took the day off\\n(B) The counselor took the day off\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The manager sent a message to the secretary, but he didn't reply yet.\\nOptions:\\n(A) The secretary didn't reply yet\\n(B) The manager didn't reply yet\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: Bailey will plan to meet the director at his office\\nOptions:\\n(A) It will be Bailey's office\\n(B) It will be the director's office\\n(C) Ambiguous\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Clarify the meaning of sentences with ambiguous pronouns.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_dyck_languages\": {\n",
            "      \"task\": \"bbh_fewshot_dyck_languages\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"dyck_languages\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ { [\\nA: ] } ]\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < > ( ( [ [ ( { } ) [ < > ] ]\\nA: ] ) )\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ < [ { < [ ] < { } > > } ] > { { ( ) } { < [ < > ] > }\\nA: } ] >\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Correctly close a Dyck-n word.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_formal_fallacies\": {\n",
            "      \"task\": \"bbh_fewshot_formal_fallacies\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"formal_fallacies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: \\\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \\\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: Whoever is not a great-grandfather of Clyde is a stepbrother of Brian. Being an ancestor of Dana is sufficient for not being a great-grandfather of Clyde. We may conclude: Everyone who is an ancestor of Dana is a stepbrother of Brian, too.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \\\"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: Every infrequent user of Paul Mitchell shampoo is either a rare consumer of Nioxin shampoo or a loyal buyer of Caress soap, or both. No regular consumer of Lush soap is a rare consumer of Nioxin shampoo and, in the same time, a loyal buyer of Caress soap. It follows that whoever is an infrequent user of Paul Mitchell shampoo is not a regular consumer of Lush soap.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: invalid\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Distinguish deductively valid arguments from formal fallacies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_geometric_shapes\": {\n",
            "      \"task\": \"bbh_fewshot_geometric_shapes\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"geometric_shapes\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: This SVG path element <path d=\\\"M 31.00,73.00 L 32.00,59.00 L 44.00,50.00 L 49.00,41.00 L 64.00,37.00 L 71.00,55.00 L 64.00,76.00 L 52.00,61.00 L 31.00,73.00\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (F)\\n\\nQ: This SVG path element <path d=\\\"M 14.19,26.04 L 51.43,39.21 L 58.44,36.69 L 56.63,30.17 L 48.53,26.66 L 14.19,26.04\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (G)\\n\\nQ: This SVG path element <path d=\\\"M 41.00,43.00 L 37.00,34.00 L 41.00,33.00 L 45.00,34.00 L 41.00,43.00\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Name geometric shapes from their SVG paths.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_hyperbaton\": {\n",
            "      \"task\": \"bbh_fewshot_hyperbaton\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"hyperbaton\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which sentence has the correct adjective order:\\nOptions:\\n(A) rubber terrible ship\\n(B) terrible rubber ship\\nA: (B)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) repulsive small Brazilian exercise ship\\n(B) Brazilian repulsive exercise small ship\\nA: (A)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) blue gold wonderful square shoe\\n(B) wonderful square blue gold shoe\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Order adjectives correctly in English sentences.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_five_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_five_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_seven_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_seven_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_three_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_three_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_movie_recommendation\": {\n",
            "      \"task\": \"bbh_fewshot_movie_recommendation\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"movie_recommendation\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\\nOptions:\\n(A) Tetsuo\\n(B) the Ironman\\n(C) The Princess Bride\\n(D) The Barkley Marathons The Race That Eats Its Young\\n(E) Bug\\nA: (C)\\n\\nQ: Find a movie similar to Twister, The Silence of the Lambs, Independence Day, Braveheart:\\nOptions:\\n(A) They Shoot Horses\\n(B) Don't They\\n(C) Forrest Gump\\n(D) The Salton Sea\\n(E) Extreme Days\\nA: (C)\\n\\nQ: Find a movie similar to Minority Report, Total Recall, Inside Out, Forrest Gump:\\nOptions:\\n(A) Phenomena\\n(B) Lilting\\n(C) Catwoman\\n(D) Edge of Tomorrow\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Recommend movies similar to the given list of movies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": {\n",
            "      \"task\": \"bbh_fewshot_multistep_arithmetic_two\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"multistep_arithmetic_two\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: ((-5 + 9 * -4 - 0) * (4 + -7 + 0 * -5)) =\\nA: 123\\n\\nQ: ((-9 * 7 * 7 * -9) + (4 * -9 - 8 - -4)) =\\nA: 3929\\n\\nQ: ((-3 + 5 * 8 * -4) - (9 - 8 * -7 + -9)) =\\nA: -219\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Solve multi-step arithmetic problems.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_navigate\": {\n",
            "      \"task\": \"bbh_fewshot_navigate\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"navigate\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: If you follow these instructions, do you return to the starting point? Turn left. Turn around. Turn left. Take 7 steps. Take 2 steps. Take 4 steps. Take 8 steps.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Turn around. Take 1 step. Take 6 steps. Turn around. Take 6 steps. Take 9 steps. Take 1 step.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Always face forward. Take 2 steps right. Take 9 steps left. Take 7 steps right.\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Given a series of navigation instructions, determine whether one would end up back at the starting point.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_object_counting\": {\n",
            "      \"task\": \"bbh_fewshot_object_counting\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"object_counting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\\nA: 6\\n\\nQ: I have an orange, a raspberry, two peaches, a blackberry, an apple, a grape, a nectarine, and three plums. How many fruits do I have?\\nA: 11\\n\\nQ: I have a lettuce head, a head of broccoli, an onion, a stalk of celery, two carrots, a garlic, and a yam. How many vegetables do I have?\\nA: 8\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Questions that involve enumerating objects and asking the model to count them.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_penguins_in_a_table\": {\n",
            "      \"task\": \"bbh_fewshot_penguins_in_a_table\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"penguins_in_a_table\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\\nJames, 12, 90, 12\\nHow many penguins are less than 8 years old?\\nOptions:\\n(A) 1\\n(B) 2\\n(C) 3\\n(D) 4\\n(E) 5\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the youngest penguin?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the name of the second penguin sorted by alphabetic order?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer questions about a table of penguins and their attributes.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": {\n",
            "      \"task\": \"bbh_fewshot_reasoning_about_colored_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"reasoning_about_colored_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (E)\\n\\nQ: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (F)\\n\\nQ: On the nightstand, you see the following items arranged in a row: a teal plate, a burgundy keychain, a yellow scrunchiephone charger, an orange mug, a pink notebook, and a grey cup. How many non-orange items do you see to the left of the teal item?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\nA: (A)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer extremely simple questions about the colors of objects on a surface.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_ruin_names\": {\n",
            "      \"task\": \"bbh_fewshot_ruin_names\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"ruin_names\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?\\nOptions:\\n(A) whitesnape\\n(B) whitesnapke\\n(C) whitesnuake\\n(D) mwhitesnake\\nA: (A)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'one of our dinosaurs is missing'?\\nOptions:\\n(A) ofne of our dinosaurs is missing\\n(B) one af our dinosaurs is missing\\n(C) one of our dinosaurs is pissing\\n(D) one of our dinosaur is missing\\nA: (C)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'counting crows'?\\nOptions:\\n(A) countingy crows\\n(B) counting cows\\n(C) courting crows\\n(D) coutnting crows\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Select the humorous edit that 'ruins' the input movie or musical artist name.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": {\n",
            "      \"task\": \"bbh_fewshot_salient_translation_error_detection\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"salient_translation_error_detection\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.\\nTranslation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (D)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Auf dieser Seite sind die Baudenkmäler der oberbayerischen Großen Kreisstadt Landsberg am Lech zusammengestellt.\\nTranslation: On this page are compiled the architectural monuments of the town of Landsberg am Lech.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (E)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Łeba ist eine Kleinstadt und ein Badeort im Powiat Lęborski der polnischen Woiwodschaft Pommern.\\nTranslation: Eba is not a small town and seaside resort in the Powiat Léborski county of the Pomeranian Voivodeship of Poland.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Detect the type of error in an English translation of a German source sentence.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_snarks\": {\n",
            "      \"task\": \"bbh_fewshot_snarks\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"snarks\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which statement is sarcastic?\\nOptions:\\n(A) Yes, because having interests and actively researching them is a huge waste\\n(B) Yes, because having interests and actively researching them is a huge deal\\nA: (A)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) No one is going to disagree with you on this. Avoiding ad hominem attacks really help your case\\n(B) No one is going to disagree with you on this. Ad hominem attacks really help your case\\nA: (B)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) Consistency in the league's punishments? What do you think this is supposed to be, politics?\\n(B) Consistency in the league's punishments? What do you think this is supposed to be, moral?\\nA: (A)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Determine which of two sentences is sarcastic.\\n\\nAccording to Cambridge University Dictionary, sarcasm is \\\"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\\\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_sports_understanding\": {\n",
            "      \"task\": \"bbh_fewshot_sports_understanding\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"sports_understanding\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Is the following sentence plausible? \\\"Bam Adebayo scored a reverse layup in the Western Conference Finals.\\\"\\nA: yes\\n\\nQ: Is the following sentence plausible? \\\"Santi Cazorla scored a touchdown.\\\"\\nA: no\\n\\nQ: Is the following sentence plausible? \\\"DeMar DeRozan was called for the goal tend.\\\"\\nA: yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Determine whether an artificially constructed sentence relating to sports is plausible or not.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_temporal_sequences\": {\n",
            "      \"task\": \"bbh_fewshot_temporal_sequences\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"temporal_sequences\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Today, Emily went to the museum. Between what times could they have gone?\\nWe know that:\\nEmily woke up at 1pm.\\nElizabeth saw Emily reading at the library from 2pm to 4pm.\\nJessica saw Emily watching a movie at the theater from 4pm to 5pm.\\nLeslie saw Emily waiting at the airport from 5pm to 6pm.\\nWilliam saw Emily buying clothes at the mall from 6pm to 7pm.\\nThe museum was closed after 7pm.\\nBetween what times could Emily have gone to the museum?\\nOptions:\\n(A) 1pm to 2pm\\n(B) 6pm to 7pm\\n(C) 5pm to 6pm\\n(D) 2pm to 4pm\\nA: (A)\\n\\nQ: Today, Elizabeth went to the amusement park. Between what times could they have gone?\\nWe know that:\\nElizabeth woke up at 7am.\\nDavid saw Elizabeth fixing their computer at the electronic store from 1pm to 2pm.\\nSarah saw Elizabeth playing tennis at the tennis court from 2pm to 3pm.\\nSusan saw Elizabeth walking towards the Statue of Liberty from 3pm to 6pm.\\nAndrew saw Elizabeth taking photos near the Eiffel Tower from 6pm to 9pm.\\nEmily saw Elizabeth getting a coffee at the cafe from 9pm to 10pm.\\nThe amusement park was closed after 10pm.\\nBetween what times could Elizabeth have gone to the amusement park?\\nOptions:\\n(A) 7am to 1pm\\n(B) 9pm to 10pm\\n(C) 1pm to 2pm\\n(D) 3pm to 6pm\\nA: (A)\\n\\nQ: Today, Tiffany went to the beach. Between what times could they have gone?\\nWe know that:\\nTiffany woke up at 5am.\\nBetty saw Tiffany getting a coffee at the cafe from 5am to 6am.\\nJessica saw Tiffany working at the office from 6am to 9am.\\nJohn saw Tiffany stretching at a yoga studio from 9am to 12pm.\\nSean saw Tiffany sitting on a rooftop from 12pm to 2pm.\\nSarah saw Tiffany playing tennis at the tennis court from 2pm to 3pm.\\nThe beach was closed after 4pm.\\nBetween what times could Tiffany have gone to the beach?\\nOptions:\\n(A) 9am to 12pm\\n(B) 12pm to 2pm\\n(C) 5am to 6am\\n(D) 3pm to 4pm\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Task description: Answer questions about which times certain events could have occurred.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_five_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_seven_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_seven_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_three_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_three_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_web_of_lies\": {\n",
            "      \"task\": \"bbh_fewshot_web_of_lies\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"web_of_lies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Question: Fidel tells the truth. Jerry says Fidel tells the truth. Vina says Jerry tells the truth. Millicent says Vina lies. Raymond says Millicent lies. Does Raymond tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian lies. Millie says Kristian lies. Maybelle says Millie tells the truth. Fidel says Maybelle lies. Leda says Fidel lies. Does Leda tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian tells the truth. Michaela says Kristian lies. Raymond says Michaela tells the truth. Osvaldo says Raymond tells the truth. Jamey says Osvaldo tells the truth. Does Jamey tell the truth?\\nA: No\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Evaluate a random boolean function expressed as a word problem.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_word_sorting\": {\n",
            "      \"task\": \"bbh_fewshot_word_sorting\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"word_sorting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Sort the following words alphabetically: List: oven costume counterpart\\nA: costume counterpart oven\\n\\nQ: Sort the following words alphabetically: List: hypochlorite ponderosa phone credulity\\nA: credulity hypochlorite phone ponderosa\\n\\nQ: Sort the following words alphabetically: List: newt arson parthia seismography mugho aspect census\\nA: arson aspect census mugho newt parthia seismography\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Sort a list of words.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"bbh_fewshot_boolean_expressions\": 1.0,\n",
            "    \"bbh_fewshot_causal_judgement\": 1.0,\n",
            "    \"bbh_fewshot_date_understanding\": 1.0,\n",
            "    \"bbh_fewshot_disambiguation_qa\": 1.0,\n",
            "    \"bbh_fewshot_dyck_languages\": 1.0,\n",
            "    \"bbh_fewshot_formal_fallacies\": 1.0,\n",
            "    \"bbh_fewshot_geometric_shapes\": 1.0,\n",
            "    \"bbh_fewshot_hyperbaton\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": 1.0,\n",
            "    \"bbh_fewshot_movie_recommendation\": 1.0,\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": 1.0,\n",
            "    \"bbh_fewshot_navigate\": 1.0,\n",
            "    \"bbh_fewshot_object_counting\": 1.0,\n",
            "    \"bbh_fewshot_penguins_in_a_table\": 1.0,\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": 1.0,\n",
            "    \"bbh_fewshot_ruin_names\": 1.0,\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": 1.0,\n",
            "    \"bbh_fewshot_snarks\": 1.0,\n",
            "    \"bbh_fewshot_sports_understanding\": 1.0,\n",
            "    \"bbh_fewshot_temporal_sequences\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": 1.0,\n",
            "    \"bbh_fewshot_web_of_lies\": 1.0,\n",
            "    \"bbh_fewshot_word_sorting\": 1.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"bbh_fewshot\": 3,\n",
            "    \"bbh_fewshot_boolean_expressions\": 3,\n",
            "    \"bbh_fewshot_causal_judgement\": 3,\n",
            "    \"bbh_fewshot_date_understanding\": 3,\n",
            "    \"bbh_fewshot_disambiguation_qa\": 3,\n",
            "    \"bbh_fewshot_dyck_languages\": 3,\n",
            "    \"bbh_fewshot_formal_fallacies\": 3,\n",
            "    \"bbh_fewshot_geometric_shapes\": 3,\n",
            "    \"bbh_fewshot_hyperbaton\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": 3,\n",
            "    \"bbh_fewshot_movie_recommendation\": 3,\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": 3,\n",
            "    \"bbh_fewshot_navigate\": 3,\n",
            "    \"bbh_fewshot_object_counting\": 3,\n",
            "    \"bbh_fewshot_penguins_in_a_table\": 3,\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": 3,\n",
            "    \"bbh_fewshot_ruin_names\": 3,\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": 3,\n",
            "    \"bbh_fewshot_snarks\": 3,\n",
            "    \"bbh_fewshot_sports_understanding\": 3,\n",
            "    \"bbh_fewshot_temporal_sequences\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": 3,\n",
            "    \"bbh_fewshot_web_of_lies\": 3,\n",
            "    \"bbh_fewshot_word_sorting\": 3\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True,do_sample=True\",\n",
            "    \"batch_size\": \"2\",\n",
            "    \"batch_sizes\": [],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_falcon_bbh\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712235247.1225529,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=tiiuae/falcon-7b-instruct,trust_remote_code=True,do_sample=True), gen_kwargs: (None), limit: None, num_fewshot: 3, batch_size: 2\n",
            "|                        Tasks                         |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n",
            "|------------------------------------------------------|-------|------|-----:|-----------|----:|---|-----:|\n",
            "|bbh_fewshot                                           |N/A    |none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_boolean_expressions                    |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_causal_judgement                       |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_date_understanding                     |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_disambiguation_qa                      |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_dyck_languages                         |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_formal_fallacies                       |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_geometric_shapes                       |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_hyperbaton                             |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_logical_deduction_five_objects         |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_logical_deduction_seven_objects        |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_logical_deduction_three_objects        |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_movie_recommendation                   |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_multistep_arithmetic_two               |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_navigate                               |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_object_counting                        |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_penguins_in_a_table                    |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_reasoning_about_colored_objects        |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_ruin_names                             |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_salient_translation_error_detection    |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_snarks                                 |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_sports_understanding                   |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_temporal_sequences                     |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_five_objects |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_seven_objects|      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_three_objects|      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_web_of_lies                            |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "| - bbh_fewshot_word_sorting                           |      1|none  |     3|exact_match|    0|±  |     0|\n",
            "\n",
            "|  Groups   |Version|Filter|n-shot|  Metric   |Value|   |Stderr|\n",
            "|-----------|-------|------|-----:|-----------|----:|---|-----:|\n",
            "|bbh_fewshot|N/A    |none  |     3|exact_match|    0|±  |     0|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                      bbh_fewshot/alias bbh_fewshot\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_boolean_expressions/alias  - bbh_fewshot_boole...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_causal_judgement/alias  - bbh_fewshot_causa...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_date_understanding/alias  - bbh_fewshot_date_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_disambiguation_qa/alias  - bbh_fewshot_disam...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_dyck_languages/alias  - bbh_fewshot_dyck_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_formal_fallacies/alias  - bbh_fewshot_forma...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_geometric_shapes/alias  - bbh_fewshot_geome...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_hyperbaton/alias  - bbh_fewshot_hyper...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_logical_deduction_five_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_seven_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_three_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_movie_recommendation/alias  - bbh_fewshot_movie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_multistep_arithmetic_two/alias  - bbh_fewshot_multi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                             bbh_fewshot_navigate/alias  - bbh_fewshot_navig...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      bbh_fewshot_object_counting/alias  - bbh_fewshot_objec...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_penguins_in_a_table/alias  - bbh_fewshot_pengu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_reasoning_about_colored_objects/alias  - bbh_fewshot_reaso...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_ruin_names/alias  - bbh_fewshot_ruin_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  bbh_fewshot_salient_translation_error_detection/alias  - bbh_fewshot_salie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                               bbh_fewshot_snarks/alias  - bbh_fewshot_snark...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_sports_understanding/alias  - bbh_fewshot_sport...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_temporal_sequences/alias  - bbh_fewshot_tempo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               bbh_fewshot_tracking_shuffled_objects_five_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_seven_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_three_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                          bbh_fewshot_web_of_lies/alias  - bbh_fewshot_web_o...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_word_sorting/alias  - bbh_fewshot_word_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mFalcon-7B-instruct-BBH\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/gpuiznyr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 29 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_125359-gpuiznyr/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Mistral-7B-Instruct"
      ],
      "metadata": {
        "id": "t_mFJrVCildu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# create directory to store results\n",
        "results_dir = f\"/content/drive/MyDrive/Evaluation/{eval_model}\"\n",
        "os.makedirs(results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "OruA8Al0pqWX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MMLU\n",
        "\n",
        "5-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n",
        "\n",
        "revision number is used in the code to ensure an newer version of falcon is not downloaded automatically"
      ],
      "metadata": {
        "id": "-NnfcdaTmclh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True \\\n",
        "    --tasks mmlu_stem,mmlu_social_sciences,mmlu_humanities,mmlu_other\\\n",
        "    --num_fewshot 5 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_mistral_mmlu \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Mistral-7B-Instruct-MMLU \\\n",
        "    --use_cache results_dir_mistral_mmlu \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "TLM7QdcEimL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b07c48-1d8e-43d4-8118-ec7000220cdd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 14:16:30.290687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 14:16:30.290740: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 14:16:30.292249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 14:16:31.491966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_141636-ddcp3fwd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMistral-7B-Instruct-MMLU\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/ddcp3fwd\u001b[0m\n",
            "2024-04-04:14:16:37,711 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:14:16:43,921 INFO     [__main__.py:335] Selected Tasks: ['mmlu_humanities', 'mmlu_other', 'mmlu_social_sciences', 'mmlu_stem']\n",
            "2024-04-04:14:16:43,922 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:14:16:43,922 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'mistralai/Mistral-7B-Instruct-v0.2', 'trust_remote_code': True}\n",
            "2024-04-04:14:16:43,927 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "config.json: 100% 596/596 [00:00<00:00, 1.88MB/s]\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 50.4MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 41.9M/4.94G [00:00<00:13, 356MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 83.9M/4.94G [00:00<00:13, 373MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 136M/4.94G [00:00<00:11, 415MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 189M/4.94G [00:00<00:10, 438MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 241M/4.94G [00:00<00:10, 453MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 294M/4.94G [00:00<00:10, 432MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 346M/4.94G [00:00<00:10, 431MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 398M/4.94G [00:00<00:10, 422MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 451M/4.94G [00:01<00:10, 427MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 503M/4.94G [00:01<00:09, 444MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 556M/4.94G [00:01<00:09, 456MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 608M/4.94G [00:01<00:09, 438MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 661M/4.94G [00:01<00:09, 442MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 713M/4.94G [00:01<00:09, 437MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 765M/4.94G [00:01<00:09, 450MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 818M/4.94G [00:01<00:08, 465MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 870M/4.94G [00:01<00:08, 472MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 923M/4.94G [00:02<00:10, 397MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 965M/4.94G [00:02<00:09, 402MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.94G [00:02<00:09, 404MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.06G/4.94G [00:02<00:09, 425MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.11G/4.94G [00:02<00:08, 439MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.16G/4.94G [00:02<00:08, 449MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.22G/4.94G [00:02<00:08, 455MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.94G [00:02<00:08, 410MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.32G/4.94G [00:03<00:09, 380MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.36G/4.94G [00:03<00:09, 384MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.94G [00:03<00:10, 343MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.45G/4.94G [00:03<00:12, 280MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.49G/4.94G [00:03<00:11, 290MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.52G/4.94G [00:06<01:26, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.57G/4.94G [00:06<00:57, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.60G/4.94G [00:07<00:46, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.64G/4.94G [00:07<00:37, 88.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.67G/4.94G [00:07<00:30, 108MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.72G/4.94G [00:07<00:21, 153MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.77G/4.94G [00:07<00:15, 202MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.82G/4.94G [00:07<00:12, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.87G/4.94G [00:07<00:11, 277MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.91G/4.94G [00:07<00:10, 277MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.94G [00:07<00:09, 301MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.99G/4.94G [00:08<00:09, 322MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.03G/4.94G [00:08<00:08, 337MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.94G [00:08<00:08, 350MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.12G/4.94G [00:08<00:09, 299MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.16G/4.94G [00:08<00:11, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.19G/4.94G [00:08<00:10, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.23G/4.94G [00:08<00:09, 279MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.29G/4.94G [00:09<00:08, 324MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.34G/4.94G [00:09<00:07, 358MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.38G/4.94G [00:09<00:07, 335MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.42G/4.94G [00:09<00:07, 344MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.46G/4.94G [00:09<00:07, 315MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.52G/4.94G [00:09<00:06, 348MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.56G/4.94G [00:09<00:06, 351MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.60G/4.94G [00:09<00:07, 312MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.64G/4.94G [00:10<00:07, 290MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.94G [00:10<00:07, 311MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.73G/4.94G [00:10<00:06, 331MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.94G [00:10<00:06, 325MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.82G/4.94G [00:10<00:06, 346MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.87G/4.94G [00:10<00:05, 373MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.93G/4.94G [00:10<00:05, 397MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.94G [00:10<00:04, 417MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.03G/4.94G [00:11<00:07, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.07G/4.94G [00:11<00:07, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.11G/4.94G [00:11<00:06, 279MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.16G/4.94G [00:11<00:07, 237MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.20G/4.94G [00:12<00:06, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.24G/4.94G [00:12<00:06, 273MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.28G/4.94G [00:12<00:05, 299MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.32G/4.94G [00:12<00:05, 322MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.37G/4.94G [00:12<00:04, 342MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.41G/4.94G [00:12<00:04, 352MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.45G/4.94G [00:12<00:04, 361MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.49G/4.94G [00:12<00:04, 355MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.54G/4.94G [00:12<00:03, 382MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.60G/4.94G [00:13<00:03, 414MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.65G/4.94G [00:13<00:03, 399MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.69G/4.94G [00:13<00:03, 380MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.73G/4.94G [00:13<00:03, 355MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.77G/4.94G [00:13<00:03, 366MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.94G [00:13<00:02, 390MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.87G/4.94G [00:13<00:02, 388MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.91G/4.94G [00:13<00:02, 379MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.95G/4.94G [00:14<00:02, 373MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.01G/4.94G [00:14<00:02, 384MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.05G/4.94G [00:14<00:02, 359MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.09G/4.94G [00:14<00:02, 361MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.14G/4.94G [00:14<00:02, 391MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.19G/4.94G [00:14<00:01, 408MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.24G/4.94G [00:14<00:01, 409MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.28G/4.94G [00:14<00:01, 405MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.33G/4.94G [00:14<00:01, 415MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.38G/4.94G [00:15<00:01, 425MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.44G/4.94G [00:15<00:01, 436MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.49G/4.94G [00:15<00:01, 445MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.54G/4.94G [00:15<00:00, 454MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.59G/4.94G [00:15<00:00, 458MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.65G/4.94G [00:15<00:00, 459MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.70G/4.94G [00:15<00:00, 424MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.75G/4.94G [00:15<00:00, 393MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.80G/4.94G [00:16<00:00, 408MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.84G/4.94G [00:16<00:00, 405MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.89G/4.94G [00:16<00:00, 391MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.94G/4.94G [00:16<00:00, 302MB/s]\n",
            "Downloading shards:  33% 1/3 [00:16<00:33, 16.77s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 41.9M/5.00G [00:00<00:13, 367MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 83.9M/5.00G [00:00<00:14, 343MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 136M/5.00G [00:00<00:12, 390MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 178M/5.00G [00:00<00:12, 389MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 220M/5.00G [00:00<00:13, 359MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 273M/5.00G [00:00<00:13, 362MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 315M/5.00G [00:00<00:12, 377MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 357M/5.00G [00:00<00:11, 389MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 409M/5.00G [00:01<00:11, 414MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 461M/5.00G [00:01<00:10, 421MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 514M/5.00G [00:01<00:10, 410MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 556M/5.00G [00:01<00:11, 403MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 598M/5.00G [00:01<00:10, 404MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 650M/5.00G [00:01<00:10, 411MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 703M/5.00G [00:01<00:10, 422MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 755M/5.00G [00:01<00:10, 415MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 807M/5.00G [00:02<00:10, 414MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 849M/5.00G [00:02<00:13, 308MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 891M/5.00G [00:02<00:12, 316MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 933M/5.00G [00:02<00:12, 314MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 975M/5.00G [00:02<00:11, 337MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.02G/5.00G [00:02<00:11, 334MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.06G/5.00G [00:02<00:11, 349MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.11G/5.00G [00:02<00:10, 368MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.15G/5.00G [00:03<00:10, 375MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.20G/5.00G [00:03<00:09, 381MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.24G/5.00G [00:03<00:09, 380MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/5.00G [00:03<00:10, 342MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.32G/5.00G [00:03<00:10, 354MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.36G/5.00G [00:03<00:10, 347MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/5.00G [00:03<00:10, 344MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.45G/5.00G [00:03<00:10, 339MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.50G/5.00G [00:04<00:09, 364MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/5.00G [00:04<00:09, 363MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.58G/5.00G [00:04<00:09, 346MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.63G/5.00G [00:04<00:10, 308MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.67G/5.00G [00:04<00:11, 293MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.72G/5.00G [00:04<00:09, 336MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.76G/5.00G [00:04<00:09, 342MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.81G/5.00G [00:05<00:08, 368MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.87G/5.00G [00:05<00:07, 399MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.92G/5.00G [00:05<00:07, 425MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.97G/5.00G [00:05<00:06, 446MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.02G/5.00G [00:05<00:06, 461MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/5.00G [00:05<00:06, 434MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.13G/5.00G [00:05<00:06, 423MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/5.00G [00:05<00:07, 380MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.22G/5.00G [00:05<00:07, 360MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.26G/5.00G [00:06<00:07, 354MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.31G/5.00G [00:06<00:07, 365MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/5.00G [00:06<00:07, 349MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.39G/5.00G [00:06<00:07, 332MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.43G/5.00G [00:06<00:07, 345MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.47G/5.00G [00:06<00:07, 321MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.52G/5.00G [00:06<00:07, 344MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.56G/5.00G [00:06<00:06, 360MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.60G/5.00G [00:07<00:06, 374MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.64G/5.00G [00:07<00:07, 328MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/5.00G [00:07<00:06, 356MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.74G/5.00G [00:07<00:06, 351MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.78G/5.00G [00:07<00:06, 329MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.82G/5.00G [00:07<00:06, 325MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.86G/5.00G [00:07<00:06, 338MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.92G/5.00G [00:07<00:05, 367MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.96G/5.00G [00:08<00:05, 367MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.00G/5.00G [00:08<00:05, 378MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.04G/5.00G [00:08<00:05, 346MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.08G/5.00G [00:08<00:05, 342MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.12G/5.00G [00:08<00:05, 347MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.17G/5.00G [00:08<00:05, 335MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.21G/5.00G [00:08<00:05, 340MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.26G/5.00G [00:08<00:04, 362MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.30G/5.00G [00:09<00:06, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.36G/5.00G [00:09<00:05, 298MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.40G/5.00G [00:09<00:05, 319MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.45G/5.00G [00:09<00:04, 347MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/5.00G [00:09<00:04, 313MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.53G/5.00G [00:09<00:04, 308MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.58G/5.00G [00:10<00:04, 287MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.61G/5.00G [00:10<00:05, 235MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.66G/5.00G [00:10<00:04, 284MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.70G/5.00G [00:10<00:04, 280MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.74G/5.00G [00:10<00:04, 307MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.79G/5.00G [00:10<00:03, 316MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.84G/5.00G [00:10<00:03, 357MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/5.00G [00:11<00:02, 379MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.93G/5.00G [00:11<00:03, 351MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.97G/5.00G [00:11<00:02, 363MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.02G/5.00G [00:11<00:02, 365MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.06G/5.00G [00:11<00:02, 373MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.10G/5.00G [00:11<00:02, 375MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.14G/5.00G [00:11<00:02, 355MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/5.00G [00:11<00:02, 355MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/5.00G [00:11<00:02, 370MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/5.00G [00:12<00:01, 395MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.32G/5.00G [00:12<00:01, 397MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.36G/5.00G [00:12<00:02, 300MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.40G/5.00G [00:12<00:01, 312MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.45G/5.00G [00:12<00:01, 329MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.50G/5.00G [00:12<00:01, 366MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.55G/5.00G [00:12<00:01, 400MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.60G/5.00G [00:12<00:00, 416MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.66G/5.00G [00:13<00:00, 405MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.71G/5.00G [00:13<00:00, 422MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.76G/5.00G [00:13<00:00, 435MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.81G/5.00G [00:13<00:00, 438MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.87G/5.00G [00:13<00:00, 436MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.92G/5.00G [00:13<00:00, 446MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:13<00:00, 359MB/s]\n",
            "Downloading shards:  67% 2/3 [00:30<00:15, 15.26s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 41.9M/4.54G [00:00<00:12, 356MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 83.9M/4.54G [00:00<00:12, 363MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 126M/4.54G [00:00<00:12, 360MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 168M/4.54G [00:00<00:11, 369MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 210M/4.54G [00:00<00:13, 330MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 252M/4.54G [00:00<00:12, 349MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 294M/4.54G [00:00<00:12, 354MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 336M/4.54G [00:00<00:11, 356MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 377M/4.54G [00:01<00:11, 360MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 419M/4.54G [00:01<00:11, 353MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 461M/4.54G [00:01<00:11, 354MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 503M/4.54G [00:01<00:11, 358MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 545M/4.54G [00:01<00:10, 366MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 587M/4.54G [00:01<00:10, 374MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 629M/4.54G [00:01<00:10, 372MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 671M/4.54G [00:01<00:10, 363MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 713M/4.54G [00:02<00:11, 340MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 755M/4.54G [00:02<00:10, 350MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 797M/4.54G [00:02<00:13, 272MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 839M/4.54G [00:02<00:12, 300MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 881M/4.54G [00:02<00:11, 315MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 923M/4.54G [00:02<00:11, 329MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 975M/4.54G [00:02<00:09, 365MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 1.02G/4.54G [00:02<00:09, 361MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.07G/4.54G [00:03<00:08, 390MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.11G/4.54G [00:03<00:08, 395MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.15G/4.54G [00:03<00:08, 400MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.20G/4.54G [00:03<00:09, 368MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.25G/4.54G [00:03<00:08, 376MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.29G/4.54G [00:03<00:09, 358MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.33G/4.54G [00:03<00:08, 365MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.37G/4.54G [00:03<00:08, 374MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.43G/4.54G [00:04<00:08, 369MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.47G/4.54G [00:04<00:08, 346MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.51G/4.54G [00:04<00:08, 362MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.55G/4.54G [00:04<00:08, 362MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.59G/4.54G [00:04<00:07, 371MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.64G/4.54G [00:04<00:08, 358MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.69G/4.54G [00:04<00:07, 380MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.74G/4.54G [00:04<00:06, 407MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.78G/4.54G [00:04<00:07, 383MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.82G/4.54G [00:05<00:07, 379MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.87G/4.54G [00:05<00:07, 379MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.91G/4.54G [00:05<00:08, 324MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.95G/4.54G [00:05<00:07, 325MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 1.99G/4.54G [00:05<00:07, 322MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 2.03G/4.54G [00:05<00:07, 324MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 2.08G/4.54G [00:05<00:07, 334MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 2.12G/4.54G [00:05<00:07, 344MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.16G/4.54G [00:06<00:06, 351MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.20G/4.54G [00:06<00:06, 340MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.24G/4.54G [00:06<00:06, 337MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.30G/4.54G [00:06<00:06, 370MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.35G/4.54G [00:06<00:05, 380MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.40G/4.54G [00:06<00:05, 407MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.44G/4.54G [00:06<00:05, 406MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.49G/4.54G [00:06<00:05, 406MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.53G/4.54G [00:07<00:04, 403MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.57G/4.54G [00:07<00:04, 405MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.61G/4.54G [00:07<00:04, 396MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.65G/4.54G [00:07<00:05, 362MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.69G/4.54G [00:07<00:05, 356MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.75G/4.54G [00:07<00:04, 377MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.79G/4.54G [00:07<00:04, 370MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.83G/4.54G [00:07<00:04, 348MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.88G/4.54G [00:07<00:04, 383MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.94G/4.54G [00:08<00:03, 409MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.99G/4.54G [00:08<00:03, 426MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 3.04G/4.54G [00:08<00:03, 441MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 3.09G/4.54G [00:08<00:04, 293MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 3.15G/4.54G [00:08<00:04, 317MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 3.19G/4.54G [00:08<00:04, 289MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 3.24G/4.54G [00:09<00:03, 326MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.29G/4.54G [00:09<00:03, 340MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.34G/4.54G [00:09<00:03, 347MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.39G/4.54G [00:09<00:03, 354MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.43G/4.54G [00:09<00:03, 347MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.47G/4.54G [00:09<00:02, 363MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.52G/4.54G [00:09<00:02, 389MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.57G/4.54G [00:09<00:02, 376MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.61G/4.54G [00:10<00:02, 379MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.66G/4.54G [00:10<00:02, 401MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.71G/4.54G [00:10<00:01, 419MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.76G/4.54G [00:10<00:01, 418MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.82G/4.54G [00:10<00:01, 422MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.87G/4.54G [00:10<00:01, 432MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.92G/4.54G [00:10<00:01, 427MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.97G/4.54G [00:10<00:01, 382MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 4.02G/4.54G [00:11<00:01, 389MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 4.06G/4.54G [00:11<00:01, 370MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 4.10G/4.54G [00:11<00:01, 363MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 4.14G/4.54G [00:11<00:01, 370MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 4.19G/4.54G [00:11<00:00, 399MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 4.25G/4.54G [00:11<00:00, 422MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 4.30G/4.54G [00:11<00:00, 407MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.34G/4.54G [00:11<00:00, 339MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.38G/4.54G [00:12<00:00, 354MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.44G/4.54G [00:12<00:00, 378MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.48G/4.54G [00:12<00:00, 355MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.54G/4.54G [00:12<00:00, 360MB/s]\n",
            "Downloading shards: 100% 3/3 [00:43<00:00, 14.62s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:07<00:00,  2.38s/it]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 404kB/s]\n",
            "tokenizer_config.json: 100% 1.46k/1.46k [00:00<00:00, 5.33MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 373MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 6.69MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 235kB/s]\n",
            "2024-04-04:14:17:41,045 INFO     [evaluator.py:195] Using cache at results_dir_mistral_mmlu_rank0.db\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-04-04:14:20:59,477 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
            "2024-04-04:14:20:59,478 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
            "2024-04-04:14:20:59,479 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
            "2024-04-04:14:20:59,480 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_management from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
            "2024-04-04:14:20:59,481 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_virology from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
            "2024-04-04:14:20:59,482 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
            "2024-04-04:14:20:59,483 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
            "2024-04-04:14:20:59,483 WARNING  [evaluator.py:239] Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
            "2024-04-04:14:20:59,998 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:14:21:00,018 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_mistral_mmlu_rank0.db' where possible...\n",
            "Checking cached requests: 100% 56168/56168 [00:08<00:00, 6374.27it/s]\n",
            "2024-04-04:14:21:08,831 INFO     [model.py:249] Cached requests: 0, Requests remaining: 56168\n",
            "Running loglikelihood requests:   0% 0/56168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 8\n",
            "Running loglikelihood requests:  25% 13953/56168 [07:47<10:32, 66.79it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Running loglikelihood requests:  25% 13984/56168 [07:58<10:31, 66.79it/s]Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 56168/56168 [15:16<00:00, 61.31it/s] \n",
            "{\n",
            "  \"results\": {\n",
            "    \"mmlu_stem\": {\n",
            "      \"alias\": \"stem\",\n",
            "      \"acc,none\": 0.49444973041547735,\n",
            "      \"acc_stderr,none\": 0.008648865440653956\n",
            "    },\n",
            "    \"mmlu_abstract_algebra\": {\n",
            "      \"alias\": \" - abstract_algebra\",\n",
            "      \"acc,none\": 0.31,\n",
            "      \"acc_stderr,none\": 0.04648231987117316\n",
            "    },\n",
            "    \"mmlu_anatomy\": {\n",
            "      \"alias\": \" - anatomy\",\n",
            "      \"acc,none\": 0.5777777777777777,\n",
            "      \"acc_stderr,none\": 0.04266763404099582\n",
            "    },\n",
            "    \"mmlu_astronomy\": {\n",
            "      \"alias\": \" - astronomy\",\n",
            "      \"acc,none\": 0.631578947368421,\n",
            "      \"acc_stderr,none\": 0.039255233810529325\n",
            "    },\n",
            "    \"mmlu_college_biology\": {\n",
            "      \"alias\": \" - college_biology\",\n",
            "      \"acc,none\": 0.6805555555555556,\n",
            "      \"acc_stderr,none\": 0.038990736873573344\n",
            "    },\n",
            "    \"mmlu_college_chemistry\": {\n",
            "      \"alias\": \" - college_chemistry\",\n",
            "      \"acc,none\": 0.42,\n",
            "      \"acc_stderr,none\": 0.049604496374885836\n",
            "    },\n",
            "    \"mmlu_college_computer_science\": {\n",
            "      \"alias\": \" - college_computer_science\",\n",
            "      \"acc,none\": 0.5,\n",
            "      \"acc_stderr,none\": 0.050251890762960605\n",
            "    },\n",
            "    \"mmlu_college_mathematics\": {\n",
            "      \"alias\": \" - college_mathematics\",\n",
            "      \"acc,none\": 0.39,\n",
            "      \"acc_stderr,none\": 0.04902071300001975\n",
            "    },\n",
            "    \"mmlu_college_physics\": {\n",
            "      \"alias\": \" - college_physics\",\n",
            "      \"acc,none\": 0.4215686274509804,\n",
            "      \"acc_stderr,none\": 0.049135952012744975\n",
            "    },\n",
            "    \"mmlu_computer_security\": {\n",
            "      \"alias\": \" - computer_security\",\n",
            "      \"acc,none\": 0.69,\n",
            "      \"acc_stderr,none\": 0.04648231987117316\n",
            "    },\n",
            "    \"mmlu_conceptual_physics\": {\n",
            "      \"alias\": \" - conceptual_physics\",\n",
            "      \"acc,none\": 0.5404255319148936,\n",
            "      \"acc_stderr,none\": 0.03257901482099834\n",
            "    },\n",
            "    \"mmlu_electrical_engineering\": {\n",
            "      \"alias\": \" - electrical_engineering\",\n",
            "      \"acc,none\": 0.6137931034482759,\n",
            "      \"acc_stderr,none\": 0.04057324734419036\n",
            "    },\n",
            "    \"mmlu_elementary_mathematics\": {\n",
            "      \"alias\": \" - elementary_mathematics\",\n",
            "      \"acc,none\": 0.37566137566137564,\n",
            "      \"acc_stderr,none\": 0.024942368931159788\n",
            "    },\n",
            "    \"mmlu_high_school_biology\": {\n",
            "      \"alias\": \" - high_school_biology\",\n",
            "      \"acc,none\": 0.6645161290322581,\n",
            "      \"acc_stderr,none\": 0.026860206444724352\n",
            "    },\n",
            "    \"mmlu_high_school_chemistry\": {\n",
            "      \"alias\": \" - high_school_chemistry\",\n",
            "      \"acc,none\": 0.4975369458128079,\n",
            "      \"acc_stderr,none\": 0.03517945038691063\n",
            "    },\n",
            "    \"mmlu_high_school_computer_science\": {\n",
            "      \"alias\": \" - high_school_computer_science\",\n",
            "      \"acc,none\": 0.63,\n",
            "      \"acc_stderr,none\": 0.04852365870939099\n",
            "    },\n",
            "    \"mmlu_high_school_mathematics\": {\n",
            "      \"alias\": \" - high_school_mathematics\",\n",
            "      \"acc,none\": 0.31851851851851853,\n",
            "      \"acc_stderr,none\": 0.02840653309060846\n",
            "    },\n",
            "    \"mmlu_high_school_physics\": {\n",
            "      \"alias\": \" - high_school_physics\",\n",
            "      \"acc,none\": 0.3576158940397351,\n",
            "      \"acc_stderr,none\": 0.03913453431177258\n",
            "    },\n",
            "    \"mmlu_high_school_statistics\": {\n",
            "      \"alias\": \" - high_school_statistics\",\n",
            "      \"acc,none\": 0.4444444444444444,\n",
            "      \"acc_stderr,none\": 0.03388857118502325\n",
            "    },\n",
            "    \"mmlu_machine_learning\": {\n",
            "      \"alias\": \" - machine_learning\",\n",
            "      \"acc,none\": 0.4375,\n",
            "      \"acc_stderr,none\": 0.04708567521880525\n",
            "    },\n",
            "    \"mmlu_social_sciences\": {\n",
            "      \"alias\": \"social_sciences\",\n",
            "      \"acc,none\": 0.6909327266818329,\n",
            "      \"acc_stderr,none\": 0.008138746539699652\n",
            "    },\n",
            "    \"mmlu_econometrics\": {\n",
            "      \"alias\": \" - econometrics\",\n",
            "      \"acc,none\": 0.41228070175438597,\n",
            "      \"acc_stderr,none\": 0.046306532033665956\n",
            "    },\n",
            "    \"mmlu_high_school_geography\": {\n",
            "      \"alias\": \" - high_school_geography\",\n",
            "      \"acc,none\": 0.7575757575757576,\n",
            "      \"acc_stderr,none\": 0.030532892233932036\n",
            "    },\n",
            "    \"mmlu_high_school_government_and_politics\": {\n",
            "      \"alias\": \" - high_school_government_and_politics\",\n",
            "      \"acc,none\": 0.8497409326424871,\n",
            "      \"acc_stderr,none\": 0.025787723180723886\n",
            "    },\n",
            "    \"mmlu_high_school_macroeconomics\": {\n",
            "      \"alias\": \" - high_school_macroeconomics\",\n",
            "      \"acc,none\": 0.5641025641025641,\n",
            "      \"acc_stderr,none\": 0.025141801511177495\n",
            "    },\n",
            "    \"mmlu_high_school_microeconomics\": {\n",
            "      \"alias\": \" - high_school_microeconomics\",\n",
            "      \"acc,none\": 0.6428571428571429,\n",
            "      \"acc_stderr,none\": 0.031124619309328177\n",
            "    },\n",
            "    \"mmlu_high_school_psychology\": {\n",
            "      \"alias\": \" - high_school_psychology\",\n",
            "      \"acc,none\": 0.7944954128440367,\n",
            "      \"acc_stderr,none\": 0.01732435232501601\n",
            "    },\n",
            "    \"mmlu_human_sexuality\": {\n",
            "      \"alias\": \" - human_sexuality\",\n",
            "      \"acc,none\": 0.7404580152671756,\n",
            "      \"acc_stderr,none\": 0.03844876139785271\n",
            "    },\n",
            "    \"mmlu_professional_psychology\": {\n",
            "      \"alias\": \" - professional_psychology\",\n",
            "      \"acc,none\": 0.6258169934640523,\n",
            "      \"acc_stderr,none\": 0.01957695312208884\n",
            "    },\n",
            "    \"mmlu_public_relations\": {\n",
            "      \"alias\": \" - public_relations\",\n",
            "      \"acc,none\": 0.7181818181818181,\n",
            "      \"acc_stderr,none\": 0.04309118709946458\n",
            "    },\n",
            "    \"mmlu_security_studies\": {\n",
            "      \"alias\": \" - security_studies\",\n",
            "      \"acc,none\": 0.710204081632653,\n",
            "      \"acc_stderr,none\": 0.029043088683304328\n",
            "    },\n",
            "    \"mmlu_sociology\": {\n",
            "      \"alias\": \" - sociology\",\n",
            "      \"acc,none\": 0.7114427860696517,\n",
            "      \"acc_stderr,none\": 0.03203841040213322\n",
            "    },\n",
            "    \"mmlu_us_foreign_policy\": {\n",
            "      \"alias\": \" - us_foreign_policy\",\n",
            "      \"acc,none\": 0.83,\n",
            "      \"acc_stderr,none\": 0.0377525168068637\n",
            "    },\n",
            "    \"mmlu_other\": {\n",
            "      \"alias\": \"other\",\n",
            "      \"acc,none\": 0.6578693273253943,\n",
            "      \"acc_stderr,none\": 0.008205287302550301\n",
            "    },\n",
            "    \"mmlu_business_ethics\": {\n",
            "      \"alias\": \" - business_ethics\",\n",
            "      \"acc,none\": 0.56,\n",
            "      \"acc_stderr,none\": 0.04988876515698589\n",
            "    },\n",
            "    \"mmlu_clinical_knowledge\": {\n",
            "      \"alias\": \" - clinical_knowledge\",\n",
            "      \"acc,none\": 0.6679245283018868,\n",
            "      \"acc_stderr,none\": 0.028985455652334395\n",
            "    },\n",
            "    \"mmlu_college_medicine\": {\n",
            "      \"alias\": \" - college_medicine\",\n",
            "      \"acc,none\": 0.5895953757225434,\n",
            "      \"acc_stderr,none\": 0.03750757044895537\n",
            "    },\n",
            "    \"mmlu_global_facts\": {\n",
            "      \"alias\": \" - global_facts\",\n",
            "      \"acc,none\": 0.35,\n",
            "      \"acc_stderr,none\": 0.0479372485441102\n",
            "    },\n",
            "    \"mmlu_human_aging\": {\n",
            "      \"alias\": \" - human_aging\",\n",
            "      \"acc,none\": 0.6143497757847534,\n",
            "      \"acc_stderr,none\": 0.03266842214289202\n",
            "    },\n",
            "    \"mmlu_management\": {\n",
            "      \"alias\": \" - management\",\n",
            "      \"acc,none\": 0.7475728155339806,\n",
            "      \"acc_stderr,none\": 0.04301250399690878\n",
            "    },\n",
            "    \"mmlu_marketing\": {\n",
            "      \"alias\": \" - marketing\",\n",
            "      \"acc,none\": 0.8632478632478633,\n",
            "      \"acc_stderr,none\": 0.022509033937077805\n",
            "    },\n",
            "    \"mmlu_medical_genetics\": {\n",
            "      \"alias\": \" - medical_genetics\",\n",
            "      \"acc,none\": 0.67,\n",
            "      \"acc_stderr,none\": 0.04725815626252607\n",
            "    },\n",
            "    \"mmlu_miscellaneous\": {\n",
            "      \"alias\": \" - miscellaneous\",\n",
            "      \"acc,none\": 0.7803320561941252,\n",
            "      \"acc_stderr,none\": 0.014805384478371162\n",
            "    },\n",
            "    \"mmlu_nutrition\": {\n",
            "      \"alias\": \" - nutrition\",\n",
            "      \"acc,none\": 0.673202614379085,\n",
            "      \"acc_stderr,none\": 0.02685729466328142\n",
            "    },\n",
            "    \"mmlu_professional_accounting\": {\n",
            "      \"alias\": \" - professional_accounting\",\n",
            "      \"acc,none\": 0.44680851063829785,\n",
            "      \"acc_stderr,none\": 0.029658235097666907\n",
            "    },\n",
            "    \"mmlu_professional_medicine\": {\n",
            "      \"alias\": \" - professional_medicine\",\n",
            "      \"acc,none\": 0.6139705882352942,\n",
            "      \"acc_stderr,none\": 0.029573269134411124\n",
            "    },\n",
            "    \"mmlu_virology\": {\n",
            "      \"alias\": \" - virology\",\n",
            "      \"acc,none\": 0.4879518072289157,\n",
            "      \"acc_stderr,none\": 0.038913644958358196\n",
            "    },\n",
            "    \"mmlu_humanities\": {\n",
            "      \"alias\": \"humanities\",\n",
            "      \"acc,none\": 0.5511158342189161,\n",
            "      \"acc_stderr,none\": 0.006821687854667083\n",
            "    },\n",
            "    \"mmlu_formal_logic\": {\n",
            "      \"alias\": \" - formal_logic\",\n",
            "      \"acc,none\": 0.4126984126984127,\n",
            "      \"acc_stderr,none\": 0.04403438954768177\n",
            "    },\n",
            "    \"mmlu_high_school_european_history\": {\n",
            "      \"alias\": \" - high_school_european_history\",\n",
            "      \"acc,none\": 0.7333333333333333,\n",
            "      \"acc_stderr,none\": 0.03453131801885417\n",
            "    },\n",
            "    \"mmlu_high_school_us_history\": {\n",
            "      \"alias\": \" - high_school_us_history\",\n",
            "      \"acc,none\": 0.7647058823529411,\n",
            "      \"acc_stderr,none\": 0.029771775228145652\n",
            "    },\n",
            "    \"mmlu_high_school_world_history\": {\n",
            "      \"alias\": \" - high_school_world_history\",\n",
            "      \"acc,none\": 0.7552742616033755,\n",
            "      \"acc_stderr,none\": 0.027985699387036416\n",
            "    },\n",
            "    \"mmlu_international_law\": {\n",
            "      \"alias\": \" - international_law\",\n",
            "      \"acc,none\": 0.8016528925619835,\n",
            "      \"acc_stderr,none\": 0.036401182719909476\n",
            "    },\n",
            "    \"mmlu_jurisprudence\": {\n",
            "      \"alias\": \" - jurisprudence\",\n",
            "      \"acc,none\": 0.75,\n",
            "      \"acc_stderr,none\": 0.04186091791394607\n",
            "    },\n",
            "    \"mmlu_logical_fallacies\": {\n",
            "      \"alias\": \" - logical_fallacies\",\n",
            "      \"acc,none\": 0.7239263803680982,\n",
            "      \"acc_stderr,none\": 0.035123852837050475\n",
            "    },\n",
            "    \"mmlu_moral_disputes\": {\n",
            "      \"alias\": \" - moral_disputes\",\n",
            "      \"acc,none\": 0.6763005780346821,\n",
            "      \"acc_stderr,none\": 0.025190181327608408\n",
            "    },\n",
            "    \"mmlu_moral_scenarios\": {\n",
            "      \"alias\": \" - moral_scenarios\",\n",
            "      \"acc,none\": 0.3452513966480447,\n",
            "      \"acc_stderr,none\": 0.01590143260893035\n",
            "    },\n",
            "    \"mmlu_philosophy\": {\n",
            "      \"alias\": \" - philosophy\",\n",
            "      \"acc,none\": 0.6913183279742765,\n",
            "      \"acc_stderr,none\": 0.02623696588115326\n",
            "    },\n",
            "    \"mmlu_prehistory\": {\n",
            "      \"alias\": \" - prehistory\",\n",
            "      \"acc,none\": 0.691358024691358,\n",
            "      \"acc_stderr,none\": 0.025702640260603756\n",
            "    },\n",
            "    \"mmlu_professional_law\": {\n",
            "      \"alias\": \" - professional_law\",\n",
            "      \"acc,none\": 0.4315514993481095,\n",
            "      \"acc_stderr,none\": 0.01265000799946388\n",
            "    },\n",
            "    \"mmlu_world_religions\": {\n",
            "      \"alias\": \" - world_religions\",\n",
            "      \"acc,none\": 0.847953216374269,\n",
            "      \"acc_stderr,none\": 0.027539122889061445\n",
            "    }\n",
            "  },\n",
            "  \"groups\": {\n",
            "    \"mmlu_stem\": {\n",
            "      \"alias\": \"stem\",\n",
            "      \"acc,none\": 0.49444973041547735,\n",
            "      \"acc_stderr,none\": 0.008648865440653956\n",
            "    },\n",
            "    \"mmlu_social_sciences\": {\n",
            "      \"alias\": \"social_sciences\",\n",
            "      \"acc,none\": 0.6909327266818329,\n",
            "      \"acc_stderr,none\": 0.008138746539699652\n",
            "    },\n",
            "    \"mmlu_other\": {\n",
            "      \"alias\": \"other\",\n",
            "      \"acc,none\": 0.6578693273253943,\n",
            "      \"acc_stderr,none\": 0.008205287302550301\n",
            "    },\n",
            "    \"mmlu_humanities\": {\n",
            "      \"alias\": \"humanities\",\n",
            "      \"acc,none\": 0.5511158342189161,\n",
            "      \"acc_stderr,none\": 0.006821687854667083\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"mmlu_humanities\": [\n",
            "      \"mmlu_high_school_world_history\",\n",
            "      \"mmlu_international_law\",\n",
            "      \"mmlu_high_school_european_history\",\n",
            "      \"mmlu_moral_disputes\",\n",
            "      \"mmlu_prehistory\",\n",
            "      \"mmlu_high_school_us_history\",\n",
            "      \"mmlu_professional_law\",\n",
            "      \"mmlu_moral_scenarios\",\n",
            "      \"mmlu_world_religions\",\n",
            "      \"mmlu_jurisprudence\",\n",
            "      \"mmlu_logical_fallacies\",\n",
            "      \"mmlu_formal_logic\",\n",
            "      \"mmlu_philosophy\"\n",
            "    ],\n",
            "    \"mmlu_other\": [\n",
            "      \"mmlu_professional_accounting\",\n",
            "      \"mmlu_medical_genetics\",\n",
            "      \"mmlu_college_medicine\",\n",
            "      \"mmlu_clinical_knowledge\",\n",
            "      \"mmlu_management\",\n",
            "      \"mmlu_human_aging\",\n",
            "      \"mmlu_business_ethics\",\n",
            "      \"mmlu_global_facts\",\n",
            "      \"mmlu_miscellaneous\",\n",
            "      \"mmlu_nutrition\",\n",
            "      \"mmlu_professional_medicine\",\n",
            "      \"mmlu_marketing\",\n",
            "      \"mmlu_virology\"\n",
            "    ],\n",
            "    \"mmlu_social_sciences\": [\n",
            "      \"mmlu_human_sexuality\",\n",
            "      \"mmlu_high_school_geography\",\n",
            "      \"mmlu_econometrics\",\n",
            "      \"mmlu_high_school_macroeconomics\",\n",
            "      \"mmlu_professional_psychology\",\n",
            "      \"mmlu_high_school_government_and_politics\",\n",
            "      \"mmlu_high_school_psychology\",\n",
            "      \"mmlu_security_studies\",\n",
            "      \"mmlu_us_foreign_policy\",\n",
            "      \"mmlu_high_school_microeconomics\",\n",
            "      \"mmlu_public_relations\",\n",
            "      \"mmlu_sociology\"\n",
            "    ],\n",
            "    \"mmlu_stem\": [\n",
            "      \"mmlu_conceptual_physics\",\n",
            "      \"mmlu_high_school_biology\",\n",
            "      \"mmlu_anatomy\",\n",
            "      \"mmlu_college_chemistry\",\n",
            "      \"mmlu_elementary_mathematics\",\n",
            "      \"mmlu_college_biology\",\n",
            "      \"mmlu_electrical_engineering\",\n",
            "      \"mmlu_machine_learning\",\n",
            "      \"mmlu_high_school_statistics\",\n",
            "      \"mmlu_computer_security\",\n",
            "      \"mmlu_abstract_algebra\",\n",
            "      \"mmlu_high_school_computer_science\",\n",
            "      \"mmlu_college_physics\",\n",
            "      \"mmlu_college_computer_science\",\n",
            "      \"mmlu_astronomy\",\n",
            "      \"mmlu_high_school_physics\",\n",
            "      \"mmlu_college_mathematics\",\n",
            "      \"mmlu_high_school_mathematics\",\n",
            "      \"mmlu_high_school_chemistry\"\n",
            "    ]\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"mmlu_abstract_algebra\": {\n",
            "      \"task\": \"mmlu_abstract_algebra\",\n",
            "      \"task_alias\": \"abstract_algebra\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"abstract_algebra\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about abstract algebra.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_anatomy\": {\n",
            "      \"task\": \"mmlu_anatomy\",\n",
            "      \"task_alias\": \"anatomy\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"anatomy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about anatomy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_astronomy\": {\n",
            "      \"task\": \"mmlu_astronomy\",\n",
            "      \"task_alias\": \"astronomy\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"astronomy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about astronomy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_business_ethics\": {\n",
            "      \"task\": \"mmlu_business_ethics\",\n",
            "      \"task_alias\": \"business_ethics\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"business_ethics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about business ethics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_clinical_knowledge\": {\n",
            "      \"task\": \"mmlu_clinical_knowledge\",\n",
            "      \"task_alias\": \"clinical_knowledge\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"clinical_knowledge\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about clinical knowledge.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_biology\": {\n",
            "      \"task\": \"mmlu_college_biology\",\n",
            "      \"task_alias\": \"college_biology\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_biology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college biology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_chemistry\": {\n",
            "      \"task\": \"mmlu_college_chemistry\",\n",
            "      \"task_alias\": \"college_chemistry\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_chemistry\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college chemistry.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_computer_science\": {\n",
            "      \"task\": \"mmlu_college_computer_science\",\n",
            "      \"task_alias\": \"college_computer_science\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_computer_science\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college computer science.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_mathematics\": {\n",
            "      \"task\": \"mmlu_college_mathematics\",\n",
            "      \"task_alias\": \"college_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_medicine\": {\n",
            "      \"task\": \"mmlu_college_medicine\",\n",
            "      \"task_alias\": \"college_medicine\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_medicine\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college medicine.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_college_physics\": {\n",
            "      \"task\": \"mmlu_college_physics\",\n",
            "      \"task_alias\": \"college_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"college_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about college physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_computer_security\": {\n",
            "      \"task\": \"mmlu_computer_security\",\n",
            "      \"task_alias\": \"computer_security\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"computer_security\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about computer security.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_conceptual_physics\": {\n",
            "      \"task\": \"mmlu_conceptual_physics\",\n",
            "      \"task_alias\": \"conceptual_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"conceptual_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about conceptual physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_econometrics\": {\n",
            "      \"task\": \"mmlu_econometrics\",\n",
            "      \"task_alias\": \"econometrics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"econometrics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about econometrics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_electrical_engineering\": {\n",
            "      \"task\": \"mmlu_electrical_engineering\",\n",
            "      \"task_alias\": \"electrical_engineering\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"electrical_engineering\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about electrical engineering.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_elementary_mathematics\": {\n",
            "      \"task\": \"mmlu_elementary_mathematics\",\n",
            "      \"task_alias\": \"elementary_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"elementary_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about elementary mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_formal_logic\": {\n",
            "      \"task\": \"mmlu_formal_logic\",\n",
            "      \"task_alias\": \"formal_logic\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"formal_logic\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about formal logic.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_global_facts\": {\n",
            "      \"task\": \"mmlu_global_facts\",\n",
            "      \"task_alias\": \"global_facts\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"global_facts\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about global facts.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_biology\": {\n",
            "      \"task\": \"mmlu_high_school_biology\",\n",
            "      \"task_alias\": \"high_school_biology\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_biology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school biology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_chemistry\": {\n",
            "      \"task\": \"mmlu_high_school_chemistry\",\n",
            "      \"task_alias\": \"high_school_chemistry\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_chemistry\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school chemistry.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_computer_science\": {\n",
            "      \"task\": \"mmlu_high_school_computer_science\",\n",
            "      \"task_alias\": \"high_school_computer_science\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_computer_science\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school computer science.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_european_history\": {\n",
            "      \"task\": \"mmlu_high_school_european_history\",\n",
            "      \"task_alias\": \"high_school_european_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_european_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school european history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_geography\": {\n",
            "      \"task\": \"mmlu_high_school_geography\",\n",
            "      \"task_alias\": \"high_school_geography\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_geography\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school geography.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_government_and_politics\": {\n",
            "      \"task\": \"mmlu_high_school_government_and_politics\",\n",
            "      \"task_alias\": \"high_school_government_and_politics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_government_and_politics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school government and politics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_macroeconomics\": {\n",
            "      \"task\": \"mmlu_high_school_macroeconomics\",\n",
            "      \"task_alias\": \"high_school_macroeconomics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_macroeconomics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school macroeconomics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_mathematics\": {\n",
            "      \"task\": \"mmlu_high_school_mathematics\",\n",
            "      \"task_alias\": \"high_school_mathematics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_mathematics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school mathematics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_microeconomics\": {\n",
            "      \"task\": \"mmlu_high_school_microeconomics\",\n",
            "      \"task_alias\": \"high_school_microeconomics\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_microeconomics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school microeconomics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_physics\": {\n",
            "      \"task\": \"mmlu_high_school_physics\",\n",
            "      \"task_alias\": \"high_school_physics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_physics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school physics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_psychology\": {\n",
            "      \"task\": \"mmlu_high_school_psychology\",\n",
            "      \"task_alias\": \"high_school_psychology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_psychology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school psychology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_statistics\": {\n",
            "      \"task\": \"mmlu_high_school_statistics\",\n",
            "      \"task_alias\": \"high_school_statistics\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_statistics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school statistics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_us_history\": {\n",
            "      \"task\": \"mmlu_high_school_us_history\",\n",
            "      \"task_alias\": \"high_school_us_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_us_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school us history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_high_school_world_history\": {\n",
            "      \"task\": \"mmlu_high_school_world_history\",\n",
            "      \"task_alias\": \"high_school_world_history\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"high_school_world_history\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about high school world history.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_human_aging\": {\n",
            "      \"task\": \"mmlu_human_aging\",\n",
            "      \"task_alias\": \"human_aging\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"human_aging\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about human aging.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_human_sexuality\": {\n",
            "      \"task\": \"mmlu_human_sexuality\",\n",
            "      \"task_alias\": \"human_sexuality\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"human_sexuality\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about human sexuality.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_international_law\": {\n",
            "      \"task\": \"mmlu_international_law\",\n",
            "      \"task_alias\": \"international_law\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"international_law\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about international law.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_jurisprudence\": {\n",
            "      \"task\": \"mmlu_jurisprudence\",\n",
            "      \"task_alias\": \"jurisprudence\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"jurisprudence\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about jurisprudence.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_logical_fallacies\": {\n",
            "      \"task\": \"mmlu_logical_fallacies\",\n",
            "      \"task_alias\": \"logical_fallacies\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"logical_fallacies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about logical fallacies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_machine_learning\": {\n",
            "      \"task\": \"mmlu_machine_learning\",\n",
            "      \"task_alias\": \"machine_learning\",\n",
            "      \"group\": \"mmlu_stem\",\n",
            "      \"group_alias\": \"stem\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"machine_learning\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about machine learning.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_management\": {\n",
            "      \"task\": \"mmlu_management\",\n",
            "      \"task_alias\": \"management\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"management\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about management.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_marketing\": {\n",
            "      \"task\": \"mmlu_marketing\",\n",
            "      \"task_alias\": \"marketing\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"marketing\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about marketing.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_medical_genetics\": {\n",
            "      \"task\": \"mmlu_medical_genetics\",\n",
            "      \"task_alias\": \"medical_genetics\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"medical_genetics\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about medical genetics.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_miscellaneous\": {\n",
            "      \"task\": \"mmlu_miscellaneous\",\n",
            "      \"task_alias\": \"miscellaneous\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"miscellaneous\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about miscellaneous.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_moral_disputes\": {\n",
            "      \"task\": \"mmlu_moral_disputes\",\n",
            "      \"task_alias\": \"moral_disputes\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"moral_disputes\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about moral disputes.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_moral_scenarios\": {\n",
            "      \"task\": \"mmlu_moral_scenarios\",\n",
            "      \"task_alias\": \"moral_scenarios\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"moral_scenarios\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about moral scenarios.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_nutrition\": {\n",
            "      \"task\": \"mmlu_nutrition\",\n",
            "      \"task_alias\": \"nutrition\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"nutrition\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about nutrition.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_philosophy\": {\n",
            "      \"task\": \"mmlu_philosophy\",\n",
            "      \"task_alias\": \"philosophy\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"philosophy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about philosophy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_prehistory\": {\n",
            "      \"task\": \"mmlu_prehistory\",\n",
            "      \"task_alias\": \"prehistory\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"prehistory\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about prehistory.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_accounting\": {\n",
            "      \"task\": \"mmlu_professional_accounting\",\n",
            "      \"task_alias\": \"professional_accounting\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_accounting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional accounting.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_law\": {\n",
            "      \"task\": \"mmlu_professional_law\",\n",
            "      \"task_alias\": \"professional_law\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_law\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional law.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_medicine\": {\n",
            "      \"task\": \"mmlu_professional_medicine\",\n",
            "      \"task_alias\": \"professional_medicine\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_medicine\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional medicine.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_professional_psychology\": {\n",
            "      \"task\": \"mmlu_professional_psychology\",\n",
            "      \"task_alias\": \"professional_psychology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"professional_psychology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about professional psychology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_public_relations\": {\n",
            "      \"task\": \"mmlu_public_relations\",\n",
            "      \"task_alias\": \"public_relations\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"public_relations\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about public relations.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_security_studies\": {\n",
            "      \"task\": \"mmlu_security_studies\",\n",
            "      \"task_alias\": \"security_studies\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"security_studies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about security studies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_sociology\": {\n",
            "      \"task\": \"mmlu_sociology\",\n",
            "      \"task_alias\": \"sociology\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"sociology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about sociology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_us_foreign_policy\": {\n",
            "      \"task\": \"mmlu_us_foreign_policy\",\n",
            "      \"task_alias\": \"us_foreign_policy\",\n",
            "      \"group\": \"mmlu_social_sciences\",\n",
            "      \"group_alias\": \"social_sciences\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"us_foreign_policy\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about us foreign policy.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_virology\": {\n",
            "      \"task\": \"mmlu_virology\",\n",
            "      \"task_alias\": \"virology\",\n",
            "      \"group\": \"mmlu_other\",\n",
            "      \"group_alias\": \"other\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"virology\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about virology.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"mmlu_world_religions\": {\n",
            "      \"task\": \"mmlu_world_religions\",\n",
            "      \"task_alias\": \"world_religions\",\n",
            "      \"group\": \"mmlu_humanities\",\n",
            "      \"group_alias\": \"humanities\",\n",
            "      \"dataset_path\": \"hails/mmlu_no_train\",\n",
            "      \"dataset_name\": \"world_religions\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"fewshot_split\": \"dev\",\n",
            "      \"doc_to_text\": \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\",\n",
            "      \"doc_to_target\": \"answer\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"A\",\n",
            "        \"B\",\n",
            "        \"C\",\n",
            "        \"D\"\n",
            "      ],\n",
            "      \"description\": \"The following are multiple choice questions (with answers) about world religions.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"fewshot_config\": {\n",
            "        \"sampler\": \"first_n\"\n",
            "      },\n",
            "      \"num_fewshot\": 5,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 0.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"mmlu_abstract_algebra\": 0.0,\n",
            "    \"mmlu_anatomy\": 0.0,\n",
            "    \"mmlu_astronomy\": 0.0,\n",
            "    \"mmlu_business_ethics\": 0.0,\n",
            "    \"mmlu_clinical_knowledge\": 0.0,\n",
            "    \"mmlu_college_biology\": 0.0,\n",
            "    \"mmlu_college_chemistry\": 0.0,\n",
            "    \"mmlu_college_computer_science\": 0.0,\n",
            "    \"mmlu_college_mathematics\": 0.0,\n",
            "    \"mmlu_college_medicine\": 0.0,\n",
            "    \"mmlu_college_physics\": 0.0,\n",
            "    \"mmlu_computer_security\": 0.0,\n",
            "    \"mmlu_conceptual_physics\": 0.0,\n",
            "    \"mmlu_econometrics\": 0.0,\n",
            "    \"mmlu_electrical_engineering\": 0.0,\n",
            "    \"mmlu_elementary_mathematics\": 0.0,\n",
            "    \"mmlu_formal_logic\": 0.0,\n",
            "    \"mmlu_global_facts\": 0.0,\n",
            "    \"mmlu_high_school_biology\": 0.0,\n",
            "    \"mmlu_high_school_chemistry\": 0.0,\n",
            "    \"mmlu_high_school_computer_science\": 0.0,\n",
            "    \"mmlu_high_school_european_history\": 0.0,\n",
            "    \"mmlu_high_school_geography\": 0.0,\n",
            "    \"mmlu_high_school_government_and_politics\": 0.0,\n",
            "    \"mmlu_high_school_macroeconomics\": 0.0,\n",
            "    \"mmlu_high_school_mathematics\": 0.0,\n",
            "    \"mmlu_high_school_microeconomics\": 0.0,\n",
            "    \"mmlu_high_school_physics\": 0.0,\n",
            "    \"mmlu_high_school_psychology\": 0.0,\n",
            "    \"mmlu_high_school_statistics\": 0.0,\n",
            "    \"mmlu_high_school_us_history\": 0.0,\n",
            "    \"mmlu_high_school_world_history\": 0.0,\n",
            "    \"mmlu_human_aging\": 0.0,\n",
            "    \"mmlu_human_sexuality\": 0.0,\n",
            "    \"mmlu_international_law\": 0.0,\n",
            "    \"mmlu_jurisprudence\": 0.0,\n",
            "    \"mmlu_logical_fallacies\": 0.0,\n",
            "    \"mmlu_machine_learning\": 0.0,\n",
            "    \"mmlu_management\": 0.0,\n",
            "    \"mmlu_marketing\": 0.0,\n",
            "    \"mmlu_medical_genetics\": 0.0,\n",
            "    \"mmlu_miscellaneous\": 0.0,\n",
            "    \"mmlu_moral_disputes\": 0.0,\n",
            "    \"mmlu_moral_scenarios\": 0.0,\n",
            "    \"mmlu_nutrition\": 0.0,\n",
            "    \"mmlu_philosophy\": 0.0,\n",
            "    \"mmlu_prehistory\": 0.0,\n",
            "    \"mmlu_professional_accounting\": 0.0,\n",
            "    \"mmlu_professional_law\": 0.0,\n",
            "    \"mmlu_professional_medicine\": 0.0,\n",
            "    \"mmlu_professional_psychology\": 0.0,\n",
            "    \"mmlu_public_relations\": 0.0,\n",
            "    \"mmlu_security_studies\": 0.0,\n",
            "    \"mmlu_sociology\": 0.0,\n",
            "    \"mmlu_us_foreign_policy\": 0.0,\n",
            "    \"mmlu_virology\": 0.0,\n",
            "    \"mmlu_world_religions\": 0.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"mmlu_abstract_algebra\": 5,\n",
            "    \"mmlu_anatomy\": 5,\n",
            "    \"mmlu_astronomy\": 5,\n",
            "    \"mmlu_business_ethics\": 5,\n",
            "    \"mmlu_clinical_knowledge\": 5,\n",
            "    \"mmlu_college_biology\": 5,\n",
            "    \"mmlu_college_chemistry\": 5,\n",
            "    \"mmlu_college_computer_science\": 5,\n",
            "    \"mmlu_college_mathematics\": 5,\n",
            "    \"mmlu_college_medicine\": 5,\n",
            "    \"mmlu_college_physics\": 5,\n",
            "    \"mmlu_computer_security\": 5,\n",
            "    \"mmlu_conceptual_physics\": 5,\n",
            "    \"mmlu_econometrics\": 5,\n",
            "    \"mmlu_electrical_engineering\": 5,\n",
            "    \"mmlu_elementary_mathematics\": 5,\n",
            "    \"mmlu_formal_logic\": 5,\n",
            "    \"mmlu_global_facts\": 5,\n",
            "    \"mmlu_high_school_biology\": 5,\n",
            "    \"mmlu_high_school_chemistry\": 5,\n",
            "    \"mmlu_high_school_computer_science\": 5,\n",
            "    \"mmlu_high_school_european_history\": 5,\n",
            "    \"mmlu_high_school_geography\": 5,\n",
            "    \"mmlu_high_school_government_and_politics\": 5,\n",
            "    \"mmlu_high_school_macroeconomics\": 5,\n",
            "    \"mmlu_high_school_mathematics\": 5,\n",
            "    \"mmlu_high_school_microeconomics\": 5,\n",
            "    \"mmlu_high_school_physics\": 5,\n",
            "    \"mmlu_high_school_psychology\": 5,\n",
            "    \"mmlu_high_school_statistics\": 5,\n",
            "    \"mmlu_high_school_us_history\": 5,\n",
            "    \"mmlu_high_school_world_history\": 5,\n",
            "    \"mmlu_human_aging\": 5,\n",
            "    \"mmlu_human_sexuality\": 5,\n",
            "    \"mmlu_humanities\": 5,\n",
            "    \"mmlu_international_law\": 5,\n",
            "    \"mmlu_jurisprudence\": 5,\n",
            "    \"mmlu_logical_fallacies\": 5,\n",
            "    \"mmlu_machine_learning\": 5,\n",
            "    \"mmlu_management\": 5,\n",
            "    \"mmlu_marketing\": 5,\n",
            "    \"mmlu_medical_genetics\": 5,\n",
            "    \"mmlu_miscellaneous\": 5,\n",
            "    \"mmlu_moral_disputes\": 5,\n",
            "    \"mmlu_moral_scenarios\": 5,\n",
            "    \"mmlu_nutrition\": 5,\n",
            "    \"mmlu_other\": 5,\n",
            "    \"mmlu_philosophy\": 5,\n",
            "    \"mmlu_prehistory\": 5,\n",
            "    \"mmlu_professional_accounting\": 5,\n",
            "    \"mmlu_professional_law\": 5,\n",
            "    \"mmlu_professional_medicine\": 5,\n",
            "    \"mmlu_professional_psychology\": 5,\n",
            "    \"mmlu_public_relations\": 5,\n",
            "    \"mmlu_security_studies\": 5,\n",
            "    \"mmlu_social_sciences\": 5,\n",
            "    \"mmlu_sociology\": 5,\n",
            "    \"mmlu_stem\": 5,\n",
            "    \"mmlu_us_foreign_policy\": 5,\n",
            "    \"mmlu_virology\": 5,\n",
            "    \"mmlu_world_religions\": 5\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      8,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_mistral_mmlu\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712240203.9223118,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto:4 (8,64,64,64,64)\n",
            "|                Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|--------------------------------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem                                  |N/A    |none  |     5|acc   |0.4944|±  |0.0086|\n",
            "| - abstract_algebra                   |      0|none  |     5|acc   |0.3100|±  |0.0465|\n",
            "| - anatomy                            |      0|none  |     5|acc   |0.5778|±  |0.0427|\n",
            "| - astronomy                          |      0|none  |     5|acc   |0.6316|±  |0.0393|\n",
            "| - college_biology                    |      0|none  |     5|acc   |0.6806|±  |0.0390|\n",
            "| - college_chemistry                  |      0|none  |     5|acc   |0.4200|±  |0.0496|\n",
            "| - college_computer_science           |      0|none  |     5|acc   |0.5000|±  |0.0503|\n",
            "| - college_mathematics                |      0|none  |     5|acc   |0.3900|±  |0.0490|\n",
            "| - college_physics                    |      0|none  |     5|acc   |0.4216|±  |0.0491|\n",
            "| - computer_security                  |      0|none  |     5|acc   |0.6900|±  |0.0465|\n",
            "| - conceptual_physics                 |      0|none  |     5|acc   |0.5404|±  |0.0326|\n",
            "| - electrical_engineering             |      0|none  |     5|acc   |0.6138|±  |0.0406|\n",
            "| - elementary_mathematics             |      0|none  |     5|acc   |0.3757|±  |0.0249|\n",
            "| - high_school_biology                |      0|none  |     5|acc   |0.6645|±  |0.0269|\n",
            "| - high_school_chemistry              |      0|none  |     5|acc   |0.4975|±  |0.0352|\n",
            "| - high_school_computer_science       |      0|none  |     5|acc   |0.6300|±  |0.0485|\n",
            "| - high_school_mathematics            |      0|none  |     5|acc   |0.3185|±  |0.0284|\n",
            "| - high_school_physics                |      0|none  |     5|acc   |0.3576|±  |0.0391|\n",
            "| - high_school_statistics             |      0|none  |     5|acc   |0.4444|±  |0.0339|\n",
            "| - machine_learning                   |      0|none  |     5|acc   |0.4375|±  |0.0471|\n",
            "|social_sciences                       |N/A    |none  |     5|acc   |0.6909|±  |0.0081|\n",
            "| - econometrics                       |      0|none  |     5|acc   |0.4123|±  |0.0463|\n",
            "| - high_school_geography              |      0|none  |     5|acc   |0.7576|±  |0.0305|\n",
            "| - high_school_government_and_politics|      0|none  |     5|acc   |0.8497|±  |0.0258|\n",
            "| - high_school_macroeconomics         |      0|none  |     5|acc   |0.5641|±  |0.0251|\n",
            "| - high_school_microeconomics         |      0|none  |     5|acc   |0.6429|±  |0.0311|\n",
            "| - high_school_psychology             |      0|none  |     5|acc   |0.7945|±  |0.0173|\n",
            "| - human_sexuality                    |      0|none  |     5|acc   |0.7405|±  |0.0384|\n",
            "| - professional_psychology            |      0|none  |     5|acc   |0.6258|±  |0.0196|\n",
            "| - public_relations                   |      0|none  |     5|acc   |0.7182|±  |0.0431|\n",
            "| - security_studies                   |      0|none  |     5|acc   |0.7102|±  |0.0290|\n",
            "| - sociology                          |      0|none  |     5|acc   |0.7114|±  |0.0320|\n",
            "| - us_foreign_policy                  |      0|none  |     5|acc   |0.8300|±  |0.0378|\n",
            "|other                                 |N/A    |none  |     5|acc   |0.6579|±  |0.0082|\n",
            "| - business_ethics                    |      0|none  |     5|acc   |0.5600|±  |0.0499|\n",
            "| - clinical_knowledge                 |      0|none  |     5|acc   |0.6679|±  |0.0290|\n",
            "| - college_medicine                   |      0|none  |     5|acc   |0.5896|±  |0.0375|\n",
            "| - global_facts                       |      0|none  |     5|acc   |0.3500|±  |0.0479|\n",
            "| - human_aging                        |      0|none  |     5|acc   |0.6143|±  |0.0327|\n",
            "| - management                         |      0|none  |     5|acc   |0.7476|±  |0.0430|\n",
            "| - marketing                          |      0|none  |     5|acc   |0.8632|±  |0.0225|\n",
            "| - medical_genetics                   |      0|none  |     5|acc   |0.6700|±  |0.0473|\n",
            "| - miscellaneous                      |      0|none  |     5|acc   |0.7803|±  |0.0148|\n",
            "| - nutrition                          |      0|none  |     5|acc   |0.6732|±  |0.0269|\n",
            "| - professional_accounting            |      0|none  |     5|acc   |0.4468|±  |0.0297|\n",
            "| - professional_medicine              |      0|none  |     5|acc   |0.6140|±  |0.0296|\n",
            "| - virology                           |      0|none  |     5|acc   |0.4880|±  |0.0389|\n",
            "|humanities                            |N/A    |none  |     5|acc   |0.5511|±  |0.0068|\n",
            "| - formal_logic                       |      0|none  |     5|acc   |0.4127|±  |0.0440|\n",
            "| - high_school_european_history       |      0|none  |     5|acc   |0.7333|±  |0.0345|\n",
            "| - high_school_us_history             |      0|none  |     5|acc   |0.7647|±  |0.0298|\n",
            "| - high_school_world_history          |      0|none  |     5|acc   |0.7553|±  |0.0280|\n",
            "| - international_law                  |      0|none  |     5|acc   |0.8017|±  |0.0364|\n",
            "| - jurisprudence                      |      0|none  |     5|acc   |0.7500|±  |0.0419|\n",
            "| - logical_fallacies                  |      0|none  |     5|acc   |0.7239|±  |0.0351|\n",
            "| - moral_disputes                     |      0|none  |     5|acc   |0.6763|±  |0.0252|\n",
            "| - moral_scenarios                    |      0|none  |     5|acc   |0.3453|±  |0.0159|\n",
            "| - philosophy                         |      0|none  |     5|acc   |0.6913|±  |0.0262|\n",
            "| - prehistory                         |      0|none  |     5|acc   |0.6914|±  |0.0257|\n",
            "| - professional_law                   |      0|none  |     5|acc   |0.4316|±  |0.0127|\n",
            "| - world_religions                    |      0|none  |     5|acc   |0.8480|±  |0.0275|\n",
            "\n",
            "|    Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|---------------|-------|------|-----:|------|-----:|---|-----:|\n",
            "|stem           |N/A    |none  |     5|acc   |0.4944|±  |0.0086|\n",
            "|social_sciences|N/A    |none  |     5|acc   |0.6909|±  |0.0081|\n",
            "|other          |N/A    |none  |     5|acc   |0.6579|±  |0.0082|\n",
            "|humanities     |N/A    |none  |     5|acc   |0.5511|±  |0.0068|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_abstract_algebra/acc 0.31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_abstract_algebra/acc_stderr 0.04648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_abstract_algebra/alias  - abstract_algebra\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_anatomy/acc 0.57778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_anatomy/acc_stderr 0.04267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_anatomy/alias  - anatomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_astronomy/acc 0.63158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_astronomy/acc_stderr 0.03926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_astronomy/alias  - astronomy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_business_ethics/acc 0.56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_business_ethics/acc_stderr 0.04989\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_business_ethics/alias  - business_ethics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_clinical_knowledge/acc 0.66792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_clinical_knowledge/acc_stderr 0.02899\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_clinical_knowledge/alias  - clinical_knowledg...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_biology/acc 0.68056\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_biology/acc_stderr 0.03899\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_biology/alias  - college_biology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_chemistry/acc 0.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_chemistry/acc_stderr 0.0496\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_chemistry/alias  - college_chemistry...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_college_computer_science/acc 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            mmlu_college_computer_science/acc_stderr 0.05025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_computer_science/alias  - college_computer_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_college_mathematics/acc 0.39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_college_mathematics/acc_stderr 0.04902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_college_mathematics/alias  - college_mathemati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_college_medicine/acc 0.5896\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_college_medicine/acc_stderr 0.03751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_college_medicine/alias  - college_medicine\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_college_physics/acc 0.42157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_college_physics/acc_stderr 0.04914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_college_physics/alias  - college_physics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_computer_security/acc 0.69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_computer_security/acc_stderr 0.04648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_computer_security/alias  - computer_security...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_conceptual_physics/acc 0.54043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_conceptual_physics/acc_stderr 0.03258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_conceptual_physics/alias  - conceptual_physic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_econometrics/acc 0.41228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_econometrics/acc_stderr 0.04631\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_econometrics/alias  - econometrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_electrical_engineering/acc 0.61379\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_electrical_engineering/acc_stderr 0.04057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_electrical_engineering/alias  - electrical_engine...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_elementary_mathematics/acc 0.37566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_elementary_mathematics/acc_stderr 0.02494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_elementary_mathematics/alias  - elementary_mathem...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_formal_logic/acc 0.4127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_formal_logic/acc_stderr 0.04403\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_formal_logic/alias  - formal_logic\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_global_facts/acc 0.35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_global_facts/acc_stderr 0.04794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_global_facts/alias  - global_facts\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_biology/acc 0.66452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_biology/acc_stderr 0.02686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_biology/alias  - high_school_biolo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_chemistry/acc 0.49754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_chemistry/acc_stderr 0.03518\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_chemistry/alias  - high_school_chemi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_computer_science/acc 0.63\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_computer_science/acc_stderr 0.04852\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_computer_science/alias  - high_school_compu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_european_history/acc 0.73333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_european_history/acc_stderr 0.03453\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_european_history/alias  - high_school_europ...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_geography/acc 0.75758\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_geography/acc_stderr 0.03053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_geography/alias  - high_school_geogr...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        mmlu_high_school_government_and_politics/acc 0.84974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: mmlu_high_school_government_and_politics/acc_stderr 0.02579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      mmlu_high_school_government_and_politics/alias  - high_school_gover...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_macroeconomics/acc 0.5641\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_macroeconomics/acc_stderr 0.02514\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_macroeconomics/alias  - high_school_macro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_high_school_mathematics/acc 0.31852\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_high_school_mathematics/acc_stderr 0.02841\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_mathematics/alias  - high_school_mathe...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_microeconomics/acc 0.64286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          mmlu_high_school_microeconomics/acc_stderr 0.03112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_high_school_microeconomics/alias  - high_school_micro...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_high_school_physics/acc 0.35762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 mmlu_high_school_physics/acc_stderr 0.03913\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_high_school_physics/alias  - high_school_physi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_psychology/acc 0.7945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_psychology/acc_stderr 0.01732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_psychology/alias  - high_school_psych...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_statistics/acc 0.44444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_statistics/acc_stderr 0.03389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_statistics/alias  - high_school_stati...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_high_school_us_history/acc 0.76471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mmlu_high_school_us_history/acc_stderr 0.02977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_high_school_us_history/alias  - high_school_us_hi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_high_school_world_history/acc 0.75527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           mmlu_high_school_world_history/acc_stderr 0.02799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                mmlu_high_school_world_history/alias  - high_school_world...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_human_aging/acc 0.61435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_human_aging/acc_stderr 0.03267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_human_aging/alias  - human_aging\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_human_sexuality/acc 0.74046\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_human_sexuality/acc_stderr 0.03845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_human_sexuality/alias  - human_sexuality\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_humanities/acc 0.55112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_humanities/acc_stderr 0.00682\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_humanities/alias humanities\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_international_law/acc 0.80165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_international_law/acc_stderr 0.0364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_international_law/alias  - international_law...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_jurisprudence/acc 0.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_jurisprudence/acc_stderr 0.04186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_jurisprudence/alias  - jurisprudence\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_logical_fallacies/acc 0.72393\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_logical_fallacies/acc_stderr 0.03512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_logical_fallacies/alias  - logical_fallacies...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_machine_learning/acc 0.4375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_machine_learning/acc_stderr 0.04709\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_machine_learning/alias  - machine_learning\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_management/acc 0.74757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_management/acc_stderr 0.04301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_management/alias  - management\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_marketing/acc 0.86325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_marketing/acc_stderr 0.02251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_marketing/alias  - marketing\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_medical_genetics/acc 0.67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_medical_genetics/acc_stderr 0.04726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_medical_genetics/alias  - medical_genetics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              mmlu_miscellaneous/acc 0.78033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       mmlu_miscellaneous/acc_stderr 0.01481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_miscellaneous/alias  - miscellaneous\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             mmlu_moral_disputes/acc 0.6763\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_moral_disputes/acc_stderr 0.02519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_moral_disputes/alias  - moral_disputes\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_moral_scenarios/acc 0.34525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_moral_scenarios/acc_stderr 0.0159\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_moral_scenarios/alias  - moral_scenarios\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_nutrition/acc 0.6732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_nutrition/acc_stderr 0.02686\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_nutrition/alias  - nutrition\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      mmlu_other/acc 0.65787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_other/acc_stderr 0.00821\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    mmlu_other/alias other\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_philosophy/acc 0.69132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_philosophy/acc_stderr 0.02624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_philosophy/alias  - philosophy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_prehistory/acc 0.69136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_prehistory/acc_stderr 0.0257\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               mmlu_prehistory/alias  - prehistory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_accounting/acc 0.44681\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_accounting/acc_stderr 0.02966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_accounting/alias  - professional_acco...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_professional_law/acc 0.43155\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_law/acc_stderr 0.01265\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_professional_law/alias  - professional_law\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mmlu_professional_medicine/acc 0.61397\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               mmlu_professional_medicine/acc_stderr 0.02957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_medicine/alias  - professional_medi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_professional_psychology/acc 0.62582\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mmlu_professional_psychology/acc_stderr 0.01958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  mmlu_professional_psychology/alias  - professional_psyc...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_public_relations/acc 0.71818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_public_relations/acc_stderr 0.04309\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_public_relations/alias  - public_relations\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_security_studies/acc 0.7102\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    mmlu_security_studies/acc_stderr 0.02904\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         mmlu_security_studies/alias  - security_studies\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_social_sciences/acc 0.69093\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_social_sciences/acc_stderr 0.00814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_social_sciences/alias social_sciences\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  mmlu_sociology/acc 0.71144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           mmlu_sociology/acc_stderr 0.03204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_sociology/alias  - sociology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       mmlu_stem/acc 0.49445\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                mmlu_stem/acc_stderr 0.00865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     mmlu_stem/alias stem\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_us_foreign_policy/acc 0.83\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   mmlu_us_foreign_policy/acc_stderr 0.03775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        mmlu_us_foreign_policy/alias  - us_foreign_policy...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   mmlu_virology/acc 0.48795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_virology/acc_stderr 0.03891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 mmlu_virology/alias  - virology\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            mmlu_world_religions/acc 0.84795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mmlu_world_religions/acc_stderr 0.02754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          mmlu_world_religions/alias  - world_religions\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMistral-7B-Instruct-MMLU\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/ddcp3fwd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 64 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_141636-ddcp3fwd/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HellaSwag\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "jco7RqKimdT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True \\\n",
        "    --tasks hellaswag \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_mistral_hellaswag \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Mistral-7B-Instruct-hellaswag \\\n",
        "    --use_cache results_dir_mistral_hellaswag \\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "T67z_0snmdoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5fada9-15ee-4c5c-d0ed-762c587c0882"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 14:44:36.365134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 14:44:36.365184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 14:44:36.366676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 14:44:37.572669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_144442-57hckogd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMistral-7B-Instruct-hellaswag\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/57hckogd\u001b[0m\n",
            "2024-04-04:14:44:43,961 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:14:44:50,118 INFO     [__main__.py:335] Selected Tasks: ['hellaswag']\n",
            "2024-04-04:14:44:50,119 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:14:44:50,119 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'mistralai/Mistral-7B-Instruct-v0.2', 'trust_remote_code': True}\n",
            "2024-04-04:14:44:50,123 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 3/3 [00:05<00:00,  1.95s/it]\n",
            "2024-04-04:14:44:58,027 INFO     [evaluator.py:195] Using cache at results_dir_mistral_hellaswag_rank0.db\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "2024-04-04:14:45:09,034 WARNING  [evaluator.py:239] Overwriting default num_fewshot of hellaswag from None to 0\n",
            "2024-04-04:14:45:09,540 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:14:45:09,553 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_mistral_hellaswag_rank0.db' where possible...\n",
            "Checking cached requests: 100% 40168/40168 [00:05<00:00, 7609.94it/s]\n",
            "2024-04-04:14:45:14,832 INFO     [model.py:249] Cached requests: 0, Requests remaining: 40168\n",
            "Running loglikelihood requests:   0% 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests:  25% 9929/40168 [01:42<04:35, 109.65it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 40168/40168 [04:53<00:00, 136.88it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"hellaswag\": {\n",
            "      \"acc,none\": 0.6601274646484764,\n",
            "      \"acc_stderr,none\": 0.004726976607130818,\n",
            "      \"acc_norm,none\": 0.8370842461661023,\n",
            "      \"acc_norm_stderr,none\": 0.003685340687255453,\n",
            "      \"alias\": \"hellaswag\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"hellaswag\": []\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"hellaswag\": {\n",
            "      \"task\": \"hellaswag\",\n",
            "      \"group\": [\n",
            "        \"multiple_choice\"\n",
            "      ],\n",
            "      \"dataset_path\": \"hellaswag\",\n",
            "      \"training_split\": \"train\",\n",
            "      \"validation_split\": \"validation\",\n",
            "      \"process_docs\": \"def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\\n    def _process_doc(doc):\\n        ctx = doc[\\\"ctx_a\\\"] + \\\" \\\" + doc[\\\"ctx_b\\\"].capitalize()\\n        out_doc = {\\n            \\\"query\\\": preprocess(doc[\\\"activity_label\\\"] + \\\": \\\" + ctx),\\n            \\\"choices\\\": [preprocess(ending) for ending in doc[\\\"endings\\\"]],\\n            \\\"gold\\\": int(doc[\\\"label\\\"]),\\n        }\\n        return out_doc\\n\\n    return dataset.map(_process_doc)\\n\",\n",
            "      \"doc_to_text\": \"{{query}}\",\n",
            "      \"doc_to_target\": \"{{label}}\",\n",
            "      \"doc_to_choice\": \"choices\",\n",
            "      \"description\": \"\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        },\n",
            "        {\n",
            "          \"metric\": \"acc_norm\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"hellaswag\": 1.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"hellaswag\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_mistral_hellaswag\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712241890.119346,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4 (64,64,64,64,64)\n",
            "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
            "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
            "|hellaswag|      1|none  |     0|acc     |0.6601|±  |0.0047|\n",
            "|         |       |none  |     0|acc_norm|0.8371|±  |0.0037|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             hellaswag/acc 0.66013\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        hellaswag/acc_norm 0.83708\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: hellaswag/acc_norm_stderr 0.00369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      hellaswag/acc_stderr 0.00473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           hellaswag/alias hellaswag\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMistral-7B-Instruct-hellaswag\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/57hckogd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_144442-57hckogd/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BoolQ\n",
        "\n",
        "0-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "MVl-3_xlNpsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True \\\n",
        "    --tasks boolq \\\n",
        "    --num_fewshot 0 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_mistral_boolq \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Mistral-7B-Instruct-boolq \\\n",
        "    --use_cache results_dir_mistral_boolq\\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "yAADoPGGNwhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b6f3e6-97bb-4124-8e18-23a397616dd3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 14:51:21.626203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 14:51:21.626265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 14:51:21.627751: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 14:51:22.820026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_145127-k4cf672l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMistral-7B-Instruct-boolq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/k4cf672l\u001b[0m\n",
            "2024-04-04:14:51:29,094 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:14:51:35,286 INFO     [__main__.py:335] Selected Tasks: ['boolq']\n",
            "2024-04-04:14:51:35,287 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:14:51:35,287 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'mistralai/Mistral-7B-Instruct-v0.2', 'trust_remote_code': True}\n",
            "2024-04-04:14:51:35,291 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 3/3 [00:05<00:00,  1.70s/it]\n",
            "2024-04-04:14:51:42,912 INFO     [evaluator.py:195] Using cache at results_dir_mistral_boolq_rank0.db\n",
            "2024-04-04:14:51:42,916 WARNING  [task.py:763] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-04-04:14:51:42,916 WARNING  [task.py:775] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "2024-04-04:14:51:47,993 WARNING  [evaluator.py:239] Overwriting default num_fewshot of boolq from None to 0\n",
            "2024-04-04:14:51:48,020 INFO     [evaluator.py:379] Running loglikelihood requests\n",
            "2024-04-04:14:51:48,022 INFO     [model.py:225] Loading 'loglikelihood' responses from cache 'results_dir_mistral_boolq_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6540/6540 [00:00<00:00, 7448.71it/s]\n",
            "2024-04-04:14:51:48,901 INFO     [model.py:249] Cached requests: 0, Requests remaining: 6540\n",
            "Running loglikelihood requests:   0% 0/6540 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 32\n",
            "Running loglikelihood requests:  24% 1537/6540 [00:42<00:42, 116.95it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 6540/6540 [01:14<00:00, 87.32it/s] \n",
            "{\n",
            "  \"results\": {\n",
            "    \"boolq\": {\n",
            "      \"acc,none\": 0.8538226299694189,\n",
            "      \"acc_stderr,none\": 0.006178975060597755,\n",
            "      \"alias\": \"boolq\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"boolq\": []\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"boolq\": {\n",
            "      \"task\": \"boolq\",\n",
            "      \"group\": [\n",
            "        \"super-glue-lm-eval-v1\"\n",
            "      ],\n",
            "      \"dataset_path\": \"super_glue\",\n",
            "      \"dataset_name\": \"boolq\",\n",
            "      \"training_split\": \"train\",\n",
            "      \"validation_split\": \"validation\",\n",
            "      \"doc_to_text\": \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\",\n",
            "      \"doc_to_target\": \"label\",\n",
            "      \"doc_to_choice\": [\n",
            "        \"no\",\n",
            "        \"yes\"\n",
            "      ],\n",
            "      \"description\": \"\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"acc\"\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"multiple_choice\",\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": true,\n",
            "      \"doc_to_decontamination_query\": \"passage\",\n",
            "      \"metadata\": {\n",
            "        \"version\": 2.0\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"boolq\": 2.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"boolq\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [\n",
            "      32,\n",
            "      64,\n",
            "      64,\n",
            "      64,\n",
            "      64\n",
            "    ],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_mistral_boolq\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712242295.2868748,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: auto:4 (32,64,64,64,64)\n",
            "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
            "|boolq|      2|none  |     0|acc   |0.8538|±  |0.0062|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        boolq/acc 0.85382\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: boolq/acc_stderr 0.00618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      boolq/alias boolq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMistral-7B-Instruct-boolq\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/k4cf672l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_145127-k4cf672l/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BBH\n",
        "\n",
        "3-Shot is used when running this evaluation to match the values used in the document [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)"
      ],
      "metadata": {
        "id": "ZxvNnKGCOB2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True \\\n",
        "    --tasks bbh_fewshot \\\n",
        "    --num_fewshot 3 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --verbosity INFO \\\n",
        "    --output_path results_dir_mistral_bbh \\\n",
        "    --log_samples \\\n",
        "    --wandb_args project=quantized_model_evaluation,name=Mistral-7B-Instruct-bbh\\\n",
        "    --use_cache results_dir_mistral_bbh\\\n",
        "    --cache_requests true \\\n",
        "    --show_config"
      ],
      "metadata": {
        "id": "mFSJExiLOG9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97932a4a-add1-47bb-ae6f-c301ffbca591"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 15:06:50.514777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 15:06:50.514836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 15:06:50.516605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 15:06:51.744017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mted-whooley\u001b[0m (\u001b[33matu-twhool02\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/lm-evaluation-harness/wandb/run-20240404_150656-az7txizj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMistral-7B-Instruct-bbh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/az7txizj\u001b[0m\n",
            "2024-04-04:15:06:57,972 INFO     [__main__.py:251] Verbosity set to INFO\n",
            "2024-04-04:15:07:04,094 INFO     [__main__.py:335] Selected Tasks: ['bbh_fewshot']\n",
            "2024-04-04:15:07:04,095 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
            "2024-04-04:15:07:04,095 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'mistralai/Mistral-7B-Instruct-v0.2', 'trust_remote_code': True}\n",
            "2024-04-04:15:07:04,099 INFO     [huggingface.py:163] Using device 'cuda:0'\n",
            "Loading checkpoint shards: 100% 3/3 [00:05<00:00,  1.68s/it]\n",
            "2024-04-04:15:07:11,218 INFO     [evaluator.py:195] Using cache at results_dir_mistral_bbh_rank0.db\n",
            "2024-04-04:15:07:15,080 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:15,080 WARNING  [task.py:322] [Task: bbh_fewshot_disambiguation_qa] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:18,333 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:18,334 WARNING  [task.py:322] [Task: bbh_fewshot_hyperbaton] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:20,897 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:20,897 WARNING  [task.py:322] [Task: bbh_fewshot_dyck_languages] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:23,514 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:23,515 WARNING  [task.py:322] [Task: bbh_fewshot_formal_fallacies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:26,284 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:26,285 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:29,141 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:29,141 WARNING  [task.py:322] [Task: bbh_fewshot_movie_recommendation] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:31,948 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:31,949 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:35,063 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:35,063 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_seven_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:38,039 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:38,039 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:40,839 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:40,839 WARNING  [task.py:322] [Task: bbh_fewshot_word_sorting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:43,909 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:43,910 WARNING  [task.py:322] [Task: bbh_fewshot_snarks] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:47,147 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:47,147 WARNING  [task.py:322] [Task: bbh_fewshot_ruin_names] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:49,964 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:49,965 WARNING  [task.py:322] [Task: bbh_fewshot_causal_judgement] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:52,737 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:52,738 WARNING  [task.py:322] [Task: bbh_fewshot_date_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:55,771 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:55,771 WARNING  [task.py:322] [Task: bbh_fewshot_salient_translation_error_detection] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:58,531 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:07:58,531 WARNING  [task.py:322] [Task: bbh_fewshot_object_counting] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:01,437 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:01,437 WARNING  [task.py:322] [Task: bbh_fewshot_temporal_sequences] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:04,373 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:04,373 WARNING  [task.py:322] [Task: bbh_fewshot_navigate] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:07,686 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:07,687 WARNING  [task.py:322] [Task: bbh_fewshot_reasoning_about_colored_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:10,528 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:10,529 WARNING  [task.py:322] [Task: bbh_fewshot_logical_deduction_three_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:13,341 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:13,342 WARNING  [task.py:322] [Task: bbh_fewshot_geometric_shapes] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:16,227 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:16,228 WARNING  [task.py:322] [Task: bbh_fewshot_web_of_lies] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:19,032 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:19,032 WARNING  [task.py:322] [Task: bbh_fewshot_penguins_in_a_table] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:21,570 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:21,570 WARNING  [task.py:322] [Task: bbh_fewshot_sports_understanding] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:24,304 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:24,305 WARNING  [task.py:322] [Task: bbh_fewshot_multistep_arithmetic_two] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:27,104 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:27,104 WARNING  [task.py:322] [Task: bbh_fewshot_tracking_shuffled_objects_five_objects] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:29,907 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:29,907 WARNING  [task.py:322] [Task: bbh_fewshot_boolean_expressions] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
            "2024-04-04:15:08:29,914 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_boolean_expressions in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_multistep_arithmetic_two in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_sports_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_penguins_in_a_table in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_web_of_lies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_geometric_shapes in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_reasoning_about_colored_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_navigate in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,915 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_temporal_sequences in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_object_counting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_salient_translation_error_detection in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_date_understanding in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_causal_judgement in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_ruin_names in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_snarks in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_word_sorting in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_five_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_logical_deduction_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_three_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_movie_recommendation in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_tracking_shuffled_objects_seven_objects in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,916 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_formal_fallacies in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,917 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_dyck_languages in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,917 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_hyperbaton in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,917 INFO     [evaluator.py:235] num_fewshot has been set to 0 for bbh_fewshot_disambiguation_qa in its config. Manual configuration will be ignored.\n",
            "2024-04-04:15:08:29,960 INFO     [evaluator.py:379] Running generate_until requests\n",
            "2024-04-04:15:08:29,962 INFO     [model.py:225] Loading 'generate_until' responses from cache 'results_dir_mistral_bbh_rank0.db' where possible...\n",
            "Checking cached requests: 100% 6511/6511 [00:01<00:00, 5034.16it/s]\n",
            "2024-04-04:15:08:31,256 INFO     [model.py:249] Cached requests: 0, Requests remaining: 6511\n",
            "Running generate_until requests:   0% 0/6511 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size\n",
            "Determined Largest batch size: 1\n",
            "Running generate_until requests: 100% 6511/6511 [29:09<00:00,  3.72it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"bbh_fewshot\": {\n",
            "      \"exact_match,none\": 0.4186760866226386,\n",
            "      \"exact_match_stderr,none\": 0.005688254650988727,\n",
            "      \"alias\": \"bbh_fewshot\"\n",
            "    },\n",
            "    \"bbh_fewshot_boolean_expressions\": {\n",
            "      \"exact_match,none\": 0.776,\n",
            "      \"exact_match_stderr,none\": 0.026421361687347905,\n",
            "      \"alias\": \" - bbh_fewshot_boolean_expressions\"\n",
            "    },\n",
            "    \"bbh_fewshot_causal_judgement\": {\n",
            "      \"exact_match,none\": 0.5828877005347594,\n",
            "      \"exact_match_stderr,none\": 0.03615450931140827,\n",
            "      \"alias\": \" - bbh_fewshot_causal_judgement\"\n",
            "    },\n",
            "    \"bbh_fewshot_date_understanding\": {\n",
            "      \"exact_match,none\": 0.356,\n",
            "      \"exact_match_stderr,none\": 0.030343680657153215,\n",
            "      \"alias\": \" - bbh_fewshot_date_understanding\"\n",
            "    },\n",
            "    \"bbh_fewshot_disambiguation_qa\": {\n",
            "      \"exact_match,none\": 0.608,\n",
            "      \"exact_match_stderr,none\": 0.030938207620401195,\n",
            "      \"alias\": \" - bbh_fewshot_disambiguation_qa\"\n",
            "    },\n",
            "    \"bbh_fewshot_dyck_languages\": {\n",
            "      \"exact_match,none\": 0.156,\n",
            "      \"exact_match_stderr,none\": 0.022995023034068755,\n",
            "      \"alias\": \" - bbh_fewshot_dyck_languages\"\n",
            "    },\n",
            "    \"bbh_fewshot_formal_fallacies\": {\n",
            "      \"exact_match,none\": 0.496,\n",
            "      \"exact_match_stderr,none\": 0.03168519855119917,\n",
            "      \"alias\": \" - bbh_fewshot_formal_fallacies\"\n",
            "    },\n",
            "    \"bbh_fewshot_geometric_shapes\": {\n",
            "      \"exact_match,none\": 0.384,\n",
            "      \"exact_match_stderr,none\": 0.030821679117375378,\n",
            "      \"alias\": \" - bbh_fewshot_geometric_shapes\"\n",
            "    },\n",
            "    \"bbh_fewshot_hyperbaton\": {\n",
            "      \"exact_match,none\": 0.652,\n",
            "      \"exact_match_stderr,none\": 0.03018656846451169,\n",
            "      \"alias\": \" - bbh_fewshot_hyperbaton\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": {\n",
            "      \"exact_match,none\": 0.344,\n",
            "      \"exact_match_stderr,none\": 0.03010450339231639,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_five_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": {\n",
            "      \"exact_match,none\": 0.256,\n",
            "      \"exact_match_stderr,none\": 0.027657108718204912,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_seven_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": {\n",
            "      \"exact_match,none\": 0.488,\n",
            "      \"exact_match_stderr,none\": 0.03167708558254709,\n",
            "      \"alias\": \" - bbh_fewshot_logical_deduction_three_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_movie_recommendation\": {\n",
            "      \"exact_match,none\": 0.508,\n",
            "      \"exact_match_stderr,none\": 0.0316821564314138,\n",
            "      \"alias\": \" - bbh_fewshot_movie_recommendation\"\n",
            "    },\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": {\n",
            "      \"exact_match,none\": 0.008,\n",
            "      \"exact_match_stderr,none\": 0.005645483676690166,\n",
            "      \"alias\": \" - bbh_fewshot_multistep_arithmetic_two\"\n",
            "    },\n",
            "    \"bbh_fewshot_navigate\": {\n",
            "      \"exact_match,none\": 0.552,\n",
            "      \"exact_match_stderr,none\": 0.031514387611153515,\n",
            "      \"alias\": \" - bbh_fewshot_navigate\"\n",
            "    },\n",
            "    \"bbh_fewshot_object_counting\": {\n",
            "      \"exact_match,none\": 0.412,\n",
            "      \"exact_match_stderr,none\": 0.0311915960260229,\n",
            "      \"alias\": \" - bbh_fewshot_object_counting\"\n",
            "    },\n",
            "    \"bbh_fewshot_penguins_in_a_table\": {\n",
            "      \"exact_match,none\": 0.410958904109589,\n",
            "      \"exact_match_stderr,none\": 0.04085902451640227,\n",
            "      \"alias\": \" - bbh_fewshot_penguins_in_a_table\"\n",
            "    },\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": {\n",
            "      \"exact_match,none\": 0.432,\n",
            "      \"exact_match_stderr,none\": 0.03139181076542941,\n",
            "      \"alias\": \" - bbh_fewshot_reasoning_about_colored_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_ruin_names\": {\n",
            "      \"exact_match,none\": 0.452,\n",
            "      \"exact_match_stderr,none\": 0.031539864492556614,\n",
            "      \"alias\": \" - bbh_fewshot_ruin_names\"\n",
            "    },\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": {\n",
            "      \"exact_match,none\": 0.396,\n",
            "      \"exact_match_stderr,none\": 0.030993197854577846,\n",
            "      \"alias\": \" - bbh_fewshot_salient_translation_error_detection\"\n",
            "    },\n",
            "    \"bbh_fewshot_snarks\": {\n",
            "      \"exact_match,none\": 0.5561797752808989,\n",
            "      \"exact_match_stderr,none\": 0.03734431584194243,\n",
            "      \"alias\": \" - bbh_fewshot_snarks\"\n",
            "    },\n",
            "    \"bbh_fewshot_sports_understanding\": {\n",
            "      \"exact_match,none\": 0.796,\n",
            "      \"exact_match_stderr,none\": 0.02553712157454817,\n",
            "      \"alias\": \" - bbh_fewshot_sports_understanding\"\n",
            "    },\n",
            "    \"bbh_fewshot_temporal_sequences\": {\n",
            "      \"exact_match,none\": 0.284,\n",
            "      \"exact_match_stderr,none\": 0.028576958730437398,\n",
            "      \"alias\": \" - bbh_fewshot_temporal_sequences\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": {\n",
            "      \"exact_match,none\": 0.232,\n",
            "      \"exact_match_stderr,none\": 0.02675007037486517,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_five_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": {\n",
            "      \"exact_match,none\": 0.18,\n",
            "      \"exact_match_stderr,none\": 0.024346890650293516,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_seven_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": {\n",
            "      \"exact_match,none\": 0.34,\n",
            "      \"exact_match_stderr,none\": 0.030020073605457904,\n",
            "      \"alias\": \" - bbh_fewshot_tracking_shuffled_objects_three_objects\"\n",
            "    },\n",
            "    \"bbh_fewshot_web_of_lies\": {\n",
            "      \"exact_match,none\": 0.512,\n",
            "      \"exact_match_stderr,none\": 0.03167708558254709,\n",
            "      \"alias\": \" - bbh_fewshot_web_of_lies\"\n",
            "    },\n",
            "    \"bbh_fewshot_word_sorting\": {\n",
            "      \"exact_match,none\": 0.212,\n",
            "      \"exact_match_stderr,none\": 0.025901884690541156,\n",
            "      \"alias\": \" - bbh_fewshot_word_sorting\"\n",
            "    }\n",
            "  },\n",
            "  \"groups\": {\n",
            "    \"bbh_fewshot\": {\n",
            "      \"exact_match,none\": 0.4186760866226386,\n",
            "      \"exact_match_stderr,none\": 0.005688254650988727,\n",
            "      \"alias\": \"bbh_fewshot\"\n",
            "    }\n",
            "  },\n",
            "  \"group_subtasks\": {\n",
            "    \"bbh_fewshot\": [\n",
            "      \"bbh_fewshot_boolean_expressions\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"bbh_fewshot_multistep_arithmetic_two\",\n",
            "      \"bbh_fewshot_sports_understanding\",\n",
            "      \"bbh_fewshot_penguins_in_a_table\",\n",
            "      \"bbh_fewshot_web_of_lies\",\n",
            "      \"bbh_fewshot_geometric_shapes\",\n",
            "      \"bbh_fewshot_logical_deduction_three_objects\",\n",
            "      \"bbh_fewshot_reasoning_about_colored_objects\",\n",
            "      \"bbh_fewshot_navigate\",\n",
            "      \"bbh_fewshot_temporal_sequences\",\n",
            "      \"bbh_fewshot_object_counting\",\n",
            "      \"bbh_fewshot_salient_translation_error_detection\",\n",
            "      \"bbh_fewshot_date_understanding\",\n",
            "      \"bbh_fewshot_causal_judgement\",\n",
            "      \"bbh_fewshot_ruin_names\",\n",
            "      \"bbh_fewshot_snarks\",\n",
            "      \"bbh_fewshot_word_sorting\",\n",
            "      \"bbh_fewshot_logical_deduction_five_objects\",\n",
            "      \"bbh_fewshot_logical_deduction_seven_objects\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_three_objects\",\n",
            "      \"bbh_fewshot_movie_recommendation\",\n",
            "      \"bbh_fewshot_tracking_shuffled_objects_seven_objects\",\n",
            "      \"bbh_fewshot_formal_fallacies\",\n",
            "      \"bbh_fewshot_dyck_languages\",\n",
            "      \"bbh_fewshot_hyperbaton\",\n",
            "      \"bbh_fewshot_disambiguation_qa\"\n",
            "    ]\n",
            "  },\n",
            "  \"configs\": {\n",
            "    \"bbh_fewshot_boolean_expressions\": {\n",
            "      \"task\": \"bbh_fewshot_boolean_expressions\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"boolean_expressions\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: not ( ( not not True ) ) is\\nA: False\\n\\nQ: True and False and not True and True is\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\nA: True\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Evaluate the result of a random Boolean expression.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_causal_judgement\": {\n",
            "      \"task\": \"bbh_fewshot_causal_judgement\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"causal_judgement\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: How would a typical person answer each of the following questions about causation?\\nFrank T., had an ongoing dispute with his neighbor over a stretch of land and one day decided to shoot his neighbor in the body. Frank T. had no experience with guns, his hand slipped on the barrel of the gun, and the shot went wild. Nonetheless, the bullet bounced off a large boulder several feet away and hit the neighbor's body, causing significant injury. Did Frank T. intentionally shoot his neighbor in the body?\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nSuzy and Billy are working on a project that is very important for our nation's security. The boss tells them both: \\\"Be sure that you are here at exactly 9 am. It is absolutely essential that you arrive at that time.\\\" Both Billy and Suzy arrive at 9 am. As it happens, there was a motion detector installed in the room where they arrived. The motion detector was set up to be triggered if at least one person appeared in the room at the same time. So the motion detector went off. Did Billy cause the motion detector to go off?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: How would a typical person answer each of the following questions about causation?\\nGeorge and his sister Lena reunite at their parents' house for Thanksgiving. Whereas George just got into medical school, Lena is unhappy in her marriage and recently lost her job. Over the course of the day, George and Lena get into a number of heated arguments. Later in the afternoon they play a game of darts. They split the first two games, and the third game is close until the end. Who will win comes down to George's last shot. If he hits a high point region, he wins; if he hits a low point region, Lena wins. George thinks of the difficult time Lena is having, and he really wants to let her win. He aims the dart at the low point region. He sets up his shot and the dart lands in the low point region. After his shot, Lena wins the game and is very happy. Did George hit the low point region intentionally?\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer questions about causal attribution.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_date_understanding\": {\n",
            "      \"task\": \"bbh_fewshot_date_understanding\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"date_understanding\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Today is Christmas Eve of 1937. What is the date 10 days ago in MM/DD/YYYY?\\nOptions:\\n(A) 12/14/2026\\n(B) 12/14/1950\\n(C) 12/14/2007\\n(D) 12/14/1937\\n(E) 07/14/1938\\n(F) 12/14/1988\\nA: (D)\\n\\nQ: Tomorrow is 11/12/2019. What is the date one year ago from today in MM/DD/YYYY?\\nOptions:\\n(A) 09/04/2018\\n(B) 11/11/2018\\n(C) 08/25/2018\\n(D) 11/02/2018\\n(E) 11/04/2018\\nA: (B)\\n\\nQ: Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date tomorrow in MM/DD/YYYY?\\nOptions:\\n(A) 01/11/1961\\n(B) 01/03/1963\\n(C) 01/18/1961\\n(D) 10/14/1960\\n(E) 01/03/1982\\n(F) 12/03/1960\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Infer the date from context.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_disambiguation_qa\": {\n",
            "      \"task\": \"bbh_fewshot_disambiguation_qa\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"disambiguation_qa\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The chief told the counselor that they took the day off.\\nOptions:\\n(A) The chief took the day off\\n(B) The counselor took the day off\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: The manager sent a message to the secretary, but he didn't reply yet.\\nOptions:\\n(A) The secretary didn't reply yet\\n(B) The manager didn't reply yet\\n(C) Ambiguous\\nA: (A)\\n\\nQ: In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\\nSentence: Bailey will plan to meet the director at his office\\nOptions:\\n(A) It will be Bailey's office\\n(B) It will be the director's office\\n(C) Ambiguous\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Clarify the meaning of sentences with ambiguous pronouns.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_dyck_languages\": {\n",
            "      \"task\": \"bbh_fewshot_dyck_languages\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"dyck_languages\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ { [\\nA: ] } ]\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < > ( ( [ [ ( { } ) [ < > ] ]\\nA: ] ) )\\n\\nQ: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ < [ { < [ ] < { } > > } ] > { { ( ) } { < [ < > ] > }\\nA: } ] >\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Correctly close a Dyck-n word.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_formal_fallacies\": {\n",
            "      \"task\": \"bbh_fewshot_formal_fallacies\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"formal_fallacies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: \\\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \\\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: Whoever is not a great-grandfather of Clyde is a stepbrother of Brian. Being an ancestor of Dana is sufficient for not being a great-grandfather of Clyde. We may conclude: Everyone who is an ancestor of Dana is a stepbrother of Brian, too.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: valid\\n\\nQ: \\\"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: Every infrequent user of Paul Mitchell shampoo is either a rare consumer of Nioxin shampoo or a loyal buyer of Caress soap, or both. No regular consumer of Lush soap is a rare consumer of Nioxin shampoo and, in the same time, a loyal buyer of Caress soap. It follows that whoever is an infrequent user of Paul Mitchell shampoo is not a regular consumer of Lush soap.\\\"\\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\\nOptions:\\n- valid\\n- invalid\\nA: invalid\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Distinguish deductively valid arguments from formal fallacies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_geometric_shapes\": {\n",
            "      \"task\": \"bbh_fewshot_geometric_shapes\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"geometric_shapes\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: This SVG path element <path d=\\\"M 31.00,73.00 L 32.00,59.00 L 44.00,50.00 L 49.00,41.00 L 64.00,37.00 L 71.00,55.00 L 64.00,76.00 L 52.00,61.00 L 31.00,73.00\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (F)\\n\\nQ: This SVG path element <path d=\\\"M 14.19,26.04 L 51.43,39.21 L 58.44,36.69 L 56.63,30.17 L 48.53,26.66 L 14.19,26.04\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (G)\\n\\nQ: This SVG path element <path d=\\\"M 41.00,43.00 L 37.00,34.00 L 41.00,33.00 L 45.00,34.00 L 41.00,43.00\\\"/> draws a\\nOptions:\\n(A) circle\\n(B) heptagon\\n(C) hexagon\\n(D) kite\\n(E) line\\n(F) octagon\\n(G) pentagon\\n(H) rectangle\\n(I) sector\\n(J) triangle\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Name geometric shapes from their SVG paths.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_hyperbaton\": {\n",
            "      \"task\": \"bbh_fewshot_hyperbaton\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"hyperbaton\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which sentence has the correct adjective order:\\nOptions:\\n(A) rubber terrible ship\\n(B) terrible rubber ship\\nA: (B)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) repulsive small Brazilian exercise ship\\n(B) Brazilian repulsive exercise small ship\\nA: (A)\\n\\nQ: Which sentence has the correct adjective order:\\nOptions:\\n(A) blue gold wonderful square shoe\\n(B) wonderful square blue gold shoe\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Order adjectives correctly in English sentences.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_five_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_five_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_seven_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_seven_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": {\n",
            "      \"task\": \"bbh_fewshot_logical_deduction_three_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"logical_deduction_three_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Amy, Eli, and Eve. Eve finished above Amy. Eli finished below Amy.\\nOptions:\\n(A) Amy finished last\\n(B) Eli finished last\\n(C) Eve finished last\\nA: (B)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost.\\nOptions:\\n(A) The white book is the leftmost\\n(B) The green book is the leftmost\\n(C) The orange book is the leftmost\\nA: (A)\\n\\nQ: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a red book, a gray book, and a white book. The white book is to the left of the gray book. The red book is the second from the left.\\nOptions:\\n(A) The red book is the leftmost\\n(B) The gray book is the leftmost\\n(C) The white book is the leftmost\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A logical deduction task which requires deducing the order of a sequence of objects.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_movie_recommendation\": {\n",
            "      \"task\": \"bbh_fewshot_movie_recommendation\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"movie_recommendation\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\\nOptions:\\n(A) Tetsuo\\n(B) the Ironman\\n(C) The Princess Bride\\n(D) The Barkley Marathons The Race That Eats Its Young\\n(E) Bug\\nA: (C)\\n\\nQ: Find a movie similar to Twister, The Silence of the Lambs, Independence Day, Braveheart:\\nOptions:\\n(A) They Shoot Horses\\n(B) Don't They\\n(C) Forrest Gump\\n(D) The Salton Sea\\n(E) Extreme Days\\nA: (C)\\n\\nQ: Find a movie similar to Minority Report, Total Recall, Inside Out, Forrest Gump:\\nOptions:\\n(A) Phenomena\\n(B) Lilting\\n(C) Catwoman\\n(D) Edge of Tomorrow\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Recommend movies similar to the given list of movies.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": {\n",
            "      \"task\": \"bbh_fewshot_multistep_arithmetic_two\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"multistep_arithmetic_two\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: ((-5 + 9 * -4 - 0) * (4 + -7 + 0 * -5)) =\\nA: 123\\n\\nQ: ((-9 * 7 * 7 * -9) + (4 * -9 - 8 - -4)) =\\nA: 3929\\n\\nQ: ((-3 + 5 * 8 * -4) - (9 - 8 * -7 + -9)) =\\nA: -219\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Solve multi-step arithmetic problems.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_navigate\": {\n",
            "      \"task\": \"bbh_fewshot_navigate\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"navigate\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: If you follow these instructions, do you return to the starting point? Turn left. Turn around. Turn left. Take 7 steps. Take 2 steps. Take 4 steps. Take 8 steps.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Turn around. Take 1 step. Take 6 steps. Turn around. Take 6 steps. Take 9 steps. Take 1 step.\\nOptions:\\n- Yes\\n- No\\nA: No\\n\\nQ: If you follow these instructions, do you return to the starting point? Always face forward. Take 2 steps right. Take 9 steps left. Take 7 steps right.\\nOptions:\\n- Yes\\n- No\\nA: Yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Given a series of navigation instructions, determine whether one would end up back at the starting point.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_object_counting\": {\n",
            "      \"task\": \"bbh_fewshot_object_counting\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"object_counting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange, and a violin. How many fruits do I have?\\nA: 6\\n\\nQ: I have an orange, a raspberry, two peaches, a blackberry, an apple, a grape, a nectarine, and three plums. How many fruits do I have?\\nA: 11\\n\\nQ: I have a lettuce head, a head of broccoli, an onion, a stalk of celery, two carrots, a garlic, and a yam. How many vegetables do I have?\\nA: 8\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Questions that involve enumerating objects and asking the model to count them.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_penguins_in_a_table\": {\n",
            "      \"task\": \"bbh_fewshot_penguins_in_a_table\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"penguins_in_a_table\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\\nJames, 12, 90, 12\\nHow many penguins are less than 8 years old?\\nOptions:\\n(A) 1\\n(B) 2\\n(C) 3\\n(D) 4\\n(E) 5\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the youngest penguin?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (B)\\n\\nQ: Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the name of the second penguin sorted by alphabetic order?\\nOptions:\\n(A) Louis\\n(B) Bernard\\n(C) Vincent\\n(D) Gwen\\n(E) James\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer questions about a table of penguins and their attributes.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": {\n",
            "      \"task\": \"bbh_fewshot_reasoning_about_colored_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"reasoning_about_colored_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (E)\\n\\nQ: On the table, you see a bunch of objects arranged in a row: a purple paperclip, a pink stress ball, a brown keychain, a green scrunchiephone charger, a mauve fidget spinner, and a burgundy pen. What is the color of the object directly to the right of the stress ball?\\nOptions:\\n(A) red\\n(B) orange\\n(C) yellow\\n(D) green\\n(E) blue\\n(F) brown\\n(G) magenta\\n(H) fuchsia\\n(I) mauve\\n(J) teal\\n(K) turquoise\\n(L) burgundy\\n(M) silver\\n(N) gold\\n(O) black\\n(P) grey\\n(Q) purple\\n(R) pink\\nA: (F)\\n\\nQ: On the nightstand, you see the following items arranged in a row: a teal plate, a burgundy keychain, a yellow scrunchiephone charger, an orange mug, a pink notebook, and a grey cup. How many non-orange items do you see to the left of the teal item?\\nOptions:\\n(A) zero\\n(B) one\\n(C) two\\n(D) three\\n(E) four\\n(F) five\\n(G) six\\nA: (A)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Answer extremely simple questions about the colors of objects on a surface.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_ruin_names\": {\n",
            "      \"task\": \"bbh_fewshot_ruin_names\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"ruin_names\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which of the following is a humorous edit of this artist or movie name: 'whitesnake'?\\nOptions:\\n(A) whitesnape\\n(B) whitesnapke\\n(C) whitesnuake\\n(D) mwhitesnake\\nA: (A)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'one of our dinosaurs is missing'?\\nOptions:\\n(A) ofne of our dinosaurs is missing\\n(B) one af our dinosaurs is missing\\n(C) one of our dinosaurs is pissing\\n(D) one of our dinosaur is missing\\nA: (C)\\n\\nQ: Which of the following is a humorous edit of this artist or movie name: 'counting crows'?\\nOptions:\\n(A) countingy crows\\n(B) counting cows\\n(C) courting crows\\n(D) coutnting crows\\nA: (B)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Select the humorous edit that 'ruins' the input movie or musical artist name.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": {\n",
            "      \"task\": \"bbh_fewshot_salient_translation_error_detection\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"salient_translation_error_detection\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In der Liste der Baudenkmale in Lenzen (Elbe) sind alle Baudenkmale der brandenburgischen Stadt Lenzen (Elbe) und ihrer Ortsteile aufgelistet.\\nTranslation: In the list of architectural monuments in Lenzen all architectural monuments of the Brandenburg city of Lenzen and its districts are listed.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (D)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Auf dieser Seite sind die Baudenkmäler der oberbayerischen Großen Kreisstadt Landsberg am Lech zusammengestellt.\\nTranslation: On this page are compiled the architectural monuments of the town of Landsberg am Lech.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (E)\\n\\nQ: The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Łeba ist eine Kleinstadt und ein Badeort im Powiat Lęborski der polnischen Woiwodschaft Pommern.\\nTranslation: Eba is not a small town and seaside resort in the Powiat Léborski county of the Pomeranian Voivodeship of Poland.\\nThe translation contains an error pertaining to\\nOptions:\\n(A) Modifiers or Adjectives\\n(B) Numerical Values\\n(C) Negation or Antonyms\\n(D) Named Entities\\n(E) Dropped Content\\n(F) Facts\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Detect the type of error in an English translation of a German source sentence.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_snarks\": {\n",
            "      \"task\": \"bbh_fewshot_snarks\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"snarks\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Which statement is sarcastic?\\nOptions:\\n(A) Yes, because having interests and actively researching them is a huge waste\\n(B) Yes, because having interests and actively researching them is a huge deal\\nA: (A)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) No one is going to disagree with you on this. Avoiding ad hominem attacks really help your case\\n(B) No one is going to disagree with you on this. Ad hominem attacks really help your case\\nA: (B)\\n\\nQ: Which statement is sarcastic?\\nOptions:\\n(A) Consistency in the league's punishments? What do you think this is supposed to be, politics?\\n(B) Consistency in the league's punishments? What do you think this is supposed to be, moral?\\nA: (A)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Determine which of two sentences is sarcastic.\\n\\nAccording to Cambridge University Dictionary, sarcasm is \\\"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\\\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_sports_understanding\": {\n",
            "      \"task\": \"bbh_fewshot_sports_understanding\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"sports_understanding\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Is the following sentence plausible? \\\"Bam Adebayo scored a reverse layup in the Western Conference Finals.\\\"\\nA: yes\\n\\nQ: Is the following sentence plausible? \\\"Santi Cazorla scored a touchdown.\\\"\\nA: no\\n\\nQ: Is the following sentence plausible? \\\"DeMar DeRozan was called for the goal tend.\\\"\\nA: yes\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Determine whether an artificially constructed sentence relating to sports is plausible or not.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_temporal_sequences\": {\n",
            "      \"task\": \"bbh_fewshot_temporal_sequences\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"temporal_sequences\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Today, Emily went to the museum. Between what times could they have gone?\\nWe know that:\\nEmily woke up at 1pm.\\nElizabeth saw Emily reading at the library from 2pm to 4pm.\\nJessica saw Emily watching a movie at the theater from 4pm to 5pm.\\nLeslie saw Emily waiting at the airport from 5pm to 6pm.\\nWilliam saw Emily buying clothes at the mall from 6pm to 7pm.\\nThe museum was closed after 7pm.\\nBetween what times could Emily have gone to the museum?\\nOptions:\\n(A) 1pm to 2pm\\n(B) 6pm to 7pm\\n(C) 5pm to 6pm\\n(D) 2pm to 4pm\\nA: (A)\\n\\nQ: Today, Elizabeth went to the amusement park. Between what times could they have gone?\\nWe know that:\\nElizabeth woke up at 7am.\\nDavid saw Elizabeth fixing their computer at the electronic store from 1pm to 2pm.\\nSarah saw Elizabeth playing tennis at the tennis court from 2pm to 3pm.\\nSusan saw Elizabeth walking towards the Statue of Liberty from 3pm to 6pm.\\nAndrew saw Elizabeth taking photos near the Eiffel Tower from 6pm to 9pm.\\nEmily saw Elizabeth getting a coffee at the cafe from 9pm to 10pm.\\nThe amusement park was closed after 10pm.\\nBetween what times could Elizabeth have gone to the amusement park?\\nOptions:\\n(A) 7am to 1pm\\n(B) 9pm to 10pm\\n(C) 1pm to 2pm\\n(D) 3pm to 6pm\\nA: (A)\\n\\nQ: Today, Tiffany went to the beach. Between what times could they have gone?\\nWe know that:\\nTiffany woke up at 5am.\\nBetty saw Tiffany getting a coffee at the cafe from 5am to 6am.\\nJessica saw Tiffany working at the office from 6am to 9am.\\nJohn saw Tiffany stretching at a yoga studio from 9am to 12pm.\\nSean saw Tiffany sitting on a rooftop from 12pm to 2pm.\\nSarah saw Tiffany playing tennis at the tennis court from 2pm to 3pm.\\nThe beach was closed after 4pm.\\nBetween what times could Tiffany have gone to the beach?\\nOptions:\\n(A) 9am to 12pm\\n(B) 12pm to 2pm\\n(C) 5am to 6am\\n(D) 3pm to 4pm\\nA: (D)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Task description: Answer questions about which times certain events could have occurred.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_five_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_five_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_seven_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_seven_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": {\n",
            "      \"task\": \"bbh_fewshot_tracking_shuffled_objects_three_objects\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"tracking_shuffled_objects_three_objects\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a yellow ball, Bob has a blue ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Claire and Alice swap balls. Then, Alice and Bob swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\\nOptions:\\n(A) yellow ball\\n(B) blue ball\\n(C) pink ball\\nA: (A)\\n\\nQ: Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a purple ball, and Claire has a pink ball.\\nAs the game progresses, pairs of players trade balls. First, Bob and Alice swap balls. Then, Bob and Claire swap balls. Finally, Bob and Alice swap balls. At the end of the game, Alice has the\\nOptions:\\n(A) white ball\\n(B) purple ball\\n(C) pink ball\\nA: (C)\\n\\nQ: Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Rodrigo, and Claire is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Claire and Bob switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Lola\\n(B) Rodrigo\\n(C) Patrick\\nA: (C)\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_web_of_lies\": {\n",
            "      \"task\": \"bbh_fewshot_web_of_lies\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"web_of_lies\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Question: Fidel tells the truth. Jerry says Fidel tells the truth. Vina says Jerry tells the truth. Millicent says Vina lies. Raymond says Millicent lies. Does Raymond tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian lies. Millie says Kristian lies. Maybelle says Millie tells the truth. Fidel says Maybelle lies. Leda says Fidel lies. Does Leda tell the truth?\\nA: Yes\\n\\nQ: Question: Kristian tells the truth. Michaela says Kristian lies. Raymond says Michaela tells the truth. Osvaldo says Raymond tells the truth. Jamey says Osvaldo tells the truth. Does Jamey tell the truth?\\nA: No\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Evaluate a random boolean function expressed as a word problem.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    },\n",
            "    \"bbh_fewshot_word_sorting\": {\n",
            "      \"task\": \"bbh_fewshot_word_sorting\",\n",
            "      \"group\": \"bbh_fewshot\",\n",
            "      \"dataset_path\": \"lukaemon/bbh\",\n",
            "      \"dataset_name\": \"word_sorting\",\n",
            "      \"test_split\": \"test\",\n",
            "      \"doc_to_text\": \"Q: Sort the following words alphabetically: List: oven costume counterpart\\nA: costume counterpart oven\\n\\nQ: Sort the following words alphabetically: List: hypochlorite ponderosa phone credulity\\nA: credulity hypochlorite phone ponderosa\\n\\nQ: Sort the following words alphabetically: List: newt arson parthia seismography mugho aspect census\\nA: arson aspect census mugho newt parthia seismography\\n\\nQ: {{input}}\\nA:\",\n",
            "      \"doc_to_target\": \"{{target}}\",\n",
            "      \"description\": \"Sort a list of words.\\n\\n\",\n",
            "      \"target_delimiter\": \" \",\n",
            "      \"fewshot_delimiter\": \"\\n\\n\",\n",
            "      \"num_fewshot\": 0,\n",
            "      \"metric_list\": [\n",
            "        {\n",
            "          \"metric\": \"exact_match\",\n",
            "          \"aggregation\": \"mean\",\n",
            "          \"higher_is_better\": true\n",
            "        }\n",
            "      ],\n",
            "      \"output_type\": \"generate_until\",\n",
            "      \"generation_kwargs\": {\n",
            "        \"until\": [\n",
            "          \"</s>\",\n",
            "          \"Q\",\n",
            "          \"\\n\\n\"\n",
            "        ],\n",
            "        \"do_sample\": false,\n",
            "        \"temperature\": 0.0\n",
            "      },\n",
            "      \"repeats\": 1,\n",
            "      \"should_decontaminate\": false,\n",
            "      \"metadata\": {\n",
            "        \"version\": 1.0,\n",
            "        \"num_fewshot\": 3\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"bbh_fewshot_boolean_expressions\": 1.0,\n",
            "    \"bbh_fewshot_causal_judgement\": 1.0,\n",
            "    \"bbh_fewshot_date_understanding\": 1.0,\n",
            "    \"bbh_fewshot_disambiguation_qa\": 1.0,\n",
            "    \"bbh_fewshot_dyck_languages\": 1.0,\n",
            "    \"bbh_fewshot_formal_fallacies\": 1.0,\n",
            "    \"bbh_fewshot_geometric_shapes\": 1.0,\n",
            "    \"bbh_fewshot_hyperbaton\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": 1.0,\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": 1.0,\n",
            "    \"bbh_fewshot_movie_recommendation\": 1.0,\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": 1.0,\n",
            "    \"bbh_fewshot_navigate\": 1.0,\n",
            "    \"bbh_fewshot_object_counting\": 1.0,\n",
            "    \"bbh_fewshot_penguins_in_a_table\": 1.0,\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": 1.0,\n",
            "    \"bbh_fewshot_ruin_names\": 1.0,\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": 1.0,\n",
            "    \"bbh_fewshot_snarks\": 1.0,\n",
            "    \"bbh_fewshot_sports_understanding\": 1.0,\n",
            "    \"bbh_fewshot_temporal_sequences\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": 1.0,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": 1.0,\n",
            "    \"bbh_fewshot_web_of_lies\": 1.0,\n",
            "    \"bbh_fewshot_word_sorting\": 1.0\n",
            "  },\n",
            "  \"n-shot\": {\n",
            "    \"bbh_fewshot\": 3,\n",
            "    \"bbh_fewshot_boolean_expressions\": 3,\n",
            "    \"bbh_fewshot_causal_judgement\": 3,\n",
            "    \"bbh_fewshot_date_understanding\": 3,\n",
            "    \"bbh_fewshot_disambiguation_qa\": 3,\n",
            "    \"bbh_fewshot_dyck_languages\": 3,\n",
            "    \"bbh_fewshot_formal_fallacies\": 3,\n",
            "    \"bbh_fewshot_geometric_shapes\": 3,\n",
            "    \"bbh_fewshot_hyperbaton\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_five_objects\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_seven_objects\": 3,\n",
            "    \"bbh_fewshot_logical_deduction_three_objects\": 3,\n",
            "    \"bbh_fewshot_movie_recommendation\": 3,\n",
            "    \"bbh_fewshot_multistep_arithmetic_two\": 3,\n",
            "    \"bbh_fewshot_navigate\": 3,\n",
            "    \"bbh_fewshot_object_counting\": 3,\n",
            "    \"bbh_fewshot_penguins_in_a_table\": 3,\n",
            "    \"bbh_fewshot_reasoning_about_colored_objects\": 3,\n",
            "    \"bbh_fewshot_ruin_names\": 3,\n",
            "    \"bbh_fewshot_salient_translation_error_detection\": 3,\n",
            "    \"bbh_fewshot_snarks\": 3,\n",
            "    \"bbh_fewshot_sports_understanding\": 3,\n",
            "    \"bbh_fewshot_temporal_sequences\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_five_objects\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_seven_objects\": 3,\n",
            "    \"bbh_fewshot_tracking_shuffled_objects_three_objects\": 3,\n",
            "    \"bbh_fewshot_web_of_lies\": 3,\n",
            "    \"bbh_fewshot_word_sorting\": 3\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf\",\n",
            "    \"model_args\": \"pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True\",\n",
            "    \"batch_size\": \"auto:4\",\n",
            "    \"batch_sizes\": [],\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"use_cache\": \"results_dir_mistral_bbh\",\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"gen_kwargs\": null\n",
            "  },\n",
            "  \"git_hash\": \"9516087b\",\n",
            "  \"date\": 1712243224.095261,\n",
            "  \"pretty_env_info\": \"PyTorch version: 2.2.1+cu121\\nIs debug build: False\\nCUDA used to build PyTorch: 12.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: 14.0.0-1ubuntu1.1\\nCMake version: version 3.27.9\\nLibc version: glibc-2.35\\n\\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\\nIs CUDA available: True\\nCUDA runtime version: 12.2.140\\nCUDA_MODULE_LOADING set to: LAZY\\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\\nNvidia driver version: 535.104.05\\ncuDNN version: Probably one of the following:\\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\nIs XNNPACK available: True\\n\\nCPU:\\nArchitecture:                       x86_64\\nCPU op-mode(s):                     32-bit, 64-bit\\nAddress sizes:                      46 bits physical, 48 bits virtual\\nByte Order:                         Little Endian\\nCPU(s):                             12\\nOn-line CPU(s) list:                0-11\\nVendor ID:                          GenuineIntel\\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\\nCPU family:                         6\\nModel:                              85\\nThread(s) per core:                 2\\nCore(s) per socket:                 6\\nSocket(s):                          1\\nStepping:                           7\\nBogoMIPS:                           4400.33\\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\\nHypervisor vendor:                  KVM\\nVirtualization type:                full\\nL1d cache:                          192 KiB (6 instances)\\nL1i cache:                          192 KiB (6 instances)\\nL2 cache:                           6 MiB (6 instances)\\nL3 cache:                           38.5 MiB (1 instance)\\nNUMA node(s):                       1\\nNUMA node0 CPU(s):                  0-11\\nVulnerability Gather data sampling: Not affected\\nVulnerability Itlb multihit:        Not affected\\nVulnerability L1tf:                 Not affected\\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\\nVulnerability Meltdown:             Not affected\\nVulnerability Mmio stale data:      Vulnerable\\nVulnerability Retbleed:             Vulnerable\\nVulnerability Spec rstack overflow: Not affected\\nVulnerability Spec store bypass:    Vulnerable\\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\\nVulnerability Srbds:                Not affected\\nVulnerability Tsx async abort:      Vulnerable\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.25.2\\n[pip3] torch==2.2.1+cu121\\n[pip3] torchaudio==2.2.1+cu121\\n[pip3] torchdata==0.7.1\\n[pip3] torchsummary==1.5.1\\n[pip3] torchtext==0.17.1\\n[pip3] torchvision==0.17.1+cu121\\n[pip3] triton==2.2.0\\n[conda] Could not collect\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"upper_git_hash\": null\n",
            "}\n",
            "hf (pretrained=mistralai/Mistral-7B-Instruct-v0.2,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 3, batch_size: auto:4\n",
            "|                        Tasks                         |Version|Filter|n-shot|  Metric   |Value |   |Stderr|\n",
            "|------------------------------------------------------|-------|------|-----:|-----------|-----:|---|-----:|\n",
            "|bbh_fewshot                                           |N/A    |none  |     3|exact_match|0.4187|±  |0.0057|\n",
            "| - bbh_fewshot_boolean_expressions                    |      1|none  |     3|exact_match|0.7760|±  |0.0264|\n",
            "| - bbh_fewshot_causal_judgement                       |      1|none  |     3|exact_match|0.5829|±  |0.0362|\n",
            "| - bbh_fewshot_date_understanding                     |      1|none  |     3|exact_match|0.3560|±  |0.0303|\n",
            "| - bbh_fewshot_disambiguation_qa                      |      1|none  |     3|exact_match|0.6080|±  |0.0309|\n",
            "| - bbh_fewshot_dyck_languages                         |      1|none  |     3|exact_match|0.1560|±  |0.0230|\n",
            "| - bbh_fewshot_formal_fallacies                       |      1|none  |     3|exact_match|0.4960|±  |0.0317|\n",
            "| - bbh_fewshot_geometric_shapes                       |      1|none  |     3|exact_match|0.3840|±  |0.0308|\n",
            "| - bbh_fewshot_hyperbaton                             |      1|none  |     3|exact_match|0.6520|±  |0.0302|\n",
            "| - bbh_fewshot_logical_deduction_five_objects         |      1|none  |     3|exact_match|0.3440|±  |0.0301|\n",
            "| - bbh_fewshot_logical_deduction_seven_objects        |      1|none  |     3|exact_match|0.2560|±  |0.0277|\n",
            "| - bbh_fewshot_logical_deduction_three_objects        |      1|none  |     3|exact_match|0.4880|±  |0.0317|\n",
            "| - bbh_fewshot_movie_recommendation                   |      1|none  |     3|exact_match|0.5080|±  |0.0317|\n",
            "| - bbh_fewshot_multistep_arithmetic_two               |      1|none  |     3|exact_match|0.0080|±  |0.0056|\n",
            "| - bbh_fewshot_navigate                               |      1|none  |     3|exact_match|0.5520|±  |0.0315|\n",
            "| - bbh_fewshot_object_counting                        |      1|none  |     3|exact_match|0.4120|±  |0.0312|\n",
            "| - bbh_fewshot_penguins_in_a_table                    |      1|none  |     3|exact_match|0.4110|±  |0.0409|\n",
            "| - bbh_fewshot_reasoning_about_colored_objects        |      1|none  |     3|exact_match|0.4320|±  |0.0314|\n",
            "| - bbh_fewshot_ruin_names                             |      1|none  |     3|exact_match|0.4520|±  |0.0315|\n",
            "| - bbh_fewshot_salient_translation_error_detection    |      1|none  |     3|exact_match|0.3960|±  |0.0310|\n",
            "| - bbh_fewshot_snarks                                 |      1|none  |     3|exact_match|0.5562|±  |0.0373|\n",
            "| - bbh_fewshot_sports_understanding                   |      1|none  |     3|exact_match|0.7960|±  |0.0255|\n",
            "| - bbh_fewshot_temporal_sequences                     |      1|none  |     3|exact_match|0.2840|±  |0.0286|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_five_objects |      1|none  |     3|exact_match|0.2320|±  |0.0268|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_seven_objects|      1|none  |     3|exact_match|0.1800|±  |0.0243|\n",
            "| - bbh_fewshot_tracking_shuffled_objects_three_objects|      1|none  |     3|exact_match|0.3400|±  |0.0300|\n",
            "| - bbh_fewshot_web_of_lies                            |      1|none  |     3|exact_match|0.5120|±  |0.0317|\n",
            "| - bbh_fewshot_word_sorting                           |      1|none  |     3|exact_match|0.2120|±  |0.0259|\n",
            "\n",
            "|  Groups   |Version|Filter|n-shot|  Metric   |Value |   |Stderr|\n",
            "|-----------|-------|------|-----:|-----------|-----:|---|-----:|\n",
            "|bbh_fewshot|N/A    |none  |     3|exact_match|0.4187|±  |0.0057|\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                      bbh_fewshot/alias bbh_fewshot\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                bbh_fewshot/exact_match 0.41868\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot/exact_match_stderr 0.00569\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_boolean_expressions/alias  - bbh_fewshot_boole...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_boolean_expressions/exact_match 0.776\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_boolean_expressions/exact_match_stderr 0.02642\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_causal_judgement/alias  - bbh_fewshot_causa...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_causal_judgement/exact_match 0.58289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_causal_judgement/exact_match_stderr 0.03615\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_date_understanding/alias  - bbh_fewshot_date_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_date_understanding/exact_match 0.356\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_date_understanding/exact_match_stderr 0.03034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_disambiguation_qa/alias  - bbh_fewshot_disam...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_disambiguation_qa/exact_match 0.608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_disambiguation_qa/exact_match_stderr 0.03094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_dyck_languages/alias  - bbh_fewshot_dyck_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_dyck_languages/exact_match 0.156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          bbh_fewshot_dyck_languages/exact_match_stderr 0.023\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_formal_fallacies/alias  - bbh_fewshot_forma...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_formal_fallacies/exact_match 0.496\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_formal_fallacies/exact_match_stderr 0.03169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_geometric_shapes/alias  - bbh_fewshot_geome...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               bbh_fewshot_geometric_shapes/exact_match 0.384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        bbh_fewshot_geometric_shapes/exact_match_stderr 0.03082\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_hyperbaton/alias  - bbh_fewshot_hyper...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_hyperbaton/exact_match 0.652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_hyperbaton/exact_match_stderr 0.03019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_logical_deduction_five_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 bbh_fewshot_logical_deduction_five_objects/exact_match 0.344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          bbh_fewshot_logical_deduction_five_objects/exact_match_stderr 0.0301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_seven_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_seven_objects/exact_match 0.256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_seven_objects/exact_match_stderr 0.02766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_logical_deduction_three_objects/alias  - bbh_fewshot_logic...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_logical_deduction_three_objects/exact_match 0.488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_logical_deduction_three_objects/exact_match_stderr 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_movie_recommendation/alias  - bbh_fewshot_movie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_movie_recommendation/exact_match 0.508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_movie_recommendation/exact_match_stderr 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_multistep_arithmetic_two/alias  - bbh_fewshot_multi...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       bbh_fewshot_multistep_arithmetic_two/exact_match 0.008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_multistep_arithmetic_two/exact_match_stderr 0.00565\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                             bbh_fewshot_navigate/alias  - bbh_fewshot_navig...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       bbh_fewshot_navigate/exact_match 0.552\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_navigate/exact_match_stderr 0.03151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                      bbh_fewshot_object_counting/alias  - bbh_fewshot_objec...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                bbh_fewshot_object_counting/exact_match 0.412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         bbh_fewshot_object_counting/exact_match_stderr 0.03119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_penguins_in_a_table/alias  - bbh_fewshot_pengu...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_penguins_in_a_table/exact_match 0.41096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     bbh_fewshot_penguins_in_a_table/exact_match_stderr 0.04086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_reasoning_about_colored_objects/alias  - bbh_fewshot_reaso...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                bbh_fewshot_reasoning_about_colored_objects/exact_match 0.432\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_reasoning_about_colored_objects/exact_match_stderr 0.03139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                           bbh_fewshot_ruin_names/alias  - bbh_fewshot_ruin_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     bbh_fewshot_ruin_names/exact_match 0.452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                              bbh_fewshot_ruin_names/exact_match_stderr 0.03154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  bbh_fewshot_salient_translation_error_detection/alias  - bbh_fewshot_salie...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            bbh_fewshot_salient_translation_error_detection/exact_match 0.396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     bbh_fewshot_salient_translation_error_detection/exact_match_stderr 0.03099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                               bbh_fewshot_snarks/alias  - bbh_fewshot_snark...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_snarks/exact_match 0.55618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                  bbh_fewshot_snarks/exact_match_stderr 0.03734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                 bbh_fewshot_sports_understanding/alias  - bbh_fewshot_sport...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           bbh_fewshot_sports_understanding/exact_match 0.796\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    bbh_fewshot_sports_understanding/exact_match_stderr 0.02554\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_temporal_sequences/alias  - bbh_fewshot_tempo...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_temporal_sequences/exact_match 0.284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      bbh_fewshot_temporal_sequences/exact_match_stderr 0.02858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               bbh_fewshot_tracking_shuffled_objects_five_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match 0.232\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  bbh_fewshot_tracking_shuffled_objects_five_objects/exact_match_stderr 0.02675\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_seven_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match 0.18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_seven_objects/exact_match_stderr 0.02435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              bbh_fewshot_tracking_shuffled_objects_three_objects/alias  - bbh_fewshot_track...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match 0.34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: bbh_fewshot_tracking_shuffled_objects_three_objects/exact_match_stderr 0.03002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                          bbh_fewshot_web_of_lies/alias  - bbh_fewshot_web_o...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                    bbh_fewshot_web_of_lies/exact_match 0.512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             bbh_fewshot_web_of_lies/exact_match_stderr 0.03168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                         bbh_fewshot_word_sorting/alias  - bbh_fewshot_word_...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                   bbh_fewshot_word_sorting/exact_match 0.212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            bbh_fewshot_word_sorting/exact_match_stderr 0.0259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mMistral-7B-Instruct-bbh\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation/runs/az7txizj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/atu-twhool02/quantized_model_evaluation\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 3 media file(s), 31 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240404_150656-az7txizj/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGPh9ekv+oBqwO6yFpFcrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d4d5545110c47578ef83be88fa3f03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45c67899b62148e9a064209d51d1700e",
              "IPY_MODEL_4806b10419db44b792a304fb2ee6d0ba",
              "IPY_MODEL_ed82baa673714109a091e785f65a39f8"
            ],
            "layout": "IPY_MODEL_cb502d1982944f6cacc4b8b2bb56925a"
          }
        },
        "45c67899b62148e9a064209d51d1700e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_039cd8e2213947b7aaf29638d0ffd17b",
            "placeholder": "​",
            "style": "IPY_MODEL_4f1ccd07713349618d3ccc61e0205f42",
            "value": "Downloading builder script: 100%"
          }
        },
        "4806b10419db44b792a304fb2ee6d0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_791461c7dc2444829eabbf5f5a8b3f8b",
            "max": 5669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_663ae81cf4d9405580a5d77c7fa521e9",
            "value": 5669
          }
        },
        "ed82baa673714109a091e785f65a39f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3369f67dc815474194b95227d1f01592",
            "placeholder": "​",
            "style": "IPY_MODEL_cda46cef5e1849bbb73cc53812a6f5eb",
            "value": " 5.67k/5.67k [00:00&lt;00:00, 467kB/s]"
          }
        },
        "cb502d1982944f6cacc4b8b2bb56925a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "039cd8e2213947b7aaf29638d0ffd17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f1ccd07713349618d3ccc61e0205f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "791461c7dc2444829eabbf5f5a8b3f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663ae81cf4d9405580a5d77c7fa521e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3369f67dc815474194b95227d1f01592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda46cef5e1849bbb73cc53812a6f5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}